# ğŸ“š TÃ€I LIá»†U Há»I ÄÃP - TRAFFIC VOLUME FORECASTING PROJECT

## Má»¤C Lá»¤C

- [PHáº¦N 1: CÃ‚U Há»I LIÃŠN QUAN Äáº¾N CODE](#pháº§n-1-cÃ¢u-há»i-liÃªn-quan-Ä‘áº¿n-code)
  - [1.1 Hiá»ƒu dá»¯ liá»‡u](#11-hiá»ƒu-dá»¯-liá»‡u-Ä‘Æ°á»£c-sá»­-dá»¥ng-trong-project)
  - [1.2 Tiá»n xá»­ lÃ½ dá»¯ liá»‡u](#12-hiá»ƒu-cÃ¡ch-tiá»n-xá»­-lÃ½-dá»¯-liá»‡u)
  - [1.3 Kiáº¿n trÃºc Encoder-Decoder](#13-hiá»ƒu-thiáº¿t-káº¿-kiáº¿n-trÃºc-cá»§a-cÃ¡c-máº¡ng-Ä‘Æ°á»£c-sá»­-dá»¥ng)
  - [1.4 Optuna Hyperparameter Optimization](#14-optuna-hyperparameter-optimization)
  - [1.5 PhÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ Model](#15-hiá»ƒu-phÆ°Æ¡ng-phÃ¡p-Ä‘Ã¡nh-giÃ¡-cháº¥t-lÆ°á»£ng-cÃ¡c-model)
- [PHáº¦N 2: CÃ‚U Há»I LÃ THUYáº¾T](#pháº§n-2-cÃ¢u-há»i-lÃ½-thuyáº¿t)

---

## PHáº¦N 1: CÃ‚U Há»I LIÃŠN QUAN Äáº¾N CODE

---

### 1.1 HIá»‚U Dá»® LIá»†U ÄÆ¯á»¢C Sá»¬ Dá»¤NG TRONG PROJECT

#### 1.1.1 Giá»›i thiá»‡u Dataset

**Dataset:** Metro Interstate Traffic Volume  
**Nguá»“n:** UCI Machine Learning Repository  
**Má»¥c tiÃªu:** Dá»± bÃ¡o lÆ°u lÆ°á»£ng giao thÃ´ng (`traffic_volume`) cho 5 giá» tiáº¿p theo

**BÃ i toÃ¡n:** ÄÃ¢y lÃ  bÃ i toÃ¡n **Multi-step Time Series Forecasting** (Dá»± bÃ¡o chuá»—i thá»i gian nhiá»u bÆ°á»›c):
- **Input:** 24 giá» dá»¯ liá»‡u lá»‹ch sá»­ (24 timesteps Ã— 22 features)
- **Output:** 5 giÃ¡ trá»‹ traffic_volume cho 5 giá» tiáº¿p theo (t+1, t+2, t+3, t+4, t+5)
- **MÃ´ hÃ¬nh:** LSTM Encoder-Decoder (Seq2Seq)

#### 1.1.2 CÃ¡c thuá»™c tÃ­nh trong dá»¯ liá»‡u gá»‘c

| STT | Thuá»™c tÃ­nh | Kiá»ƒu dá»¯ liá»‡u | MÃ´ táº£ | Vai trÃ² |
|-----|------------|--------------|-------|---------|
| 1 | `date_time` | DateTime | Thá»i Ä‘iá»ƒm ghi nháº­n (theo giá») | Index thá»i gian |
| 2 | `holiday` | Categorical | TÃªn ngÃ y lá»… hoáº·c None | Feature |
| 3 | `temp` | Numerical | Nhiá»‡t Ä‘á»™ (Kelvin) | Feature |
| 4 | `rain_1h` | Numerical | LÆ°á»£ng mÆ°a trong 1 giá» (mm) | Feature |
| 5 | `snow_1h` | Numerical | LÆ°á»£ng tuyáº¿t trong 1 giá» (mm) | Feature |
| 6 | `clouds_all` | Numerical | Pháº§n trÄƒm mÃ¢y che phá»§ (%) | Feature |
| 7 | `weather_main` | Categorical | Thá»i tiáº¿t chÃ­nh (Clear, Rain, Clouds...) | Feature |
| 8 | `weather_description` | Categorical | MÃ´ táº£ chi tiáº¿t thá»i tiáº¿t | Feature |
| 9 | **`traffic_volume`** | **Numerical** | **LÆ°u lÆ°á»£ng xe/giá»** | **TARGET** |

#### 1.1.3 Features Ä‘Æ°á»£c táº¡o thÃªm (Feature Engineering)

Sau quÃ¡ trÃ¬nh Feature Engineering, cÃ¡c features má»›i Ä‘Æ°á»£c táº¡o:

**A. Temporal Features (tá»« date_time):**
| Feature | MÃ´ táº£ | VÃ­ dá»¥ |
|---------|-------|-------|
| `hour` | Giá» trong ngÃ y | 0-23 |
| `day_of_week` | NgÃ y trong tuáº§n | 0=Monday, 6=Sunday |
| `day_of_month` | NgÃ y trong thÃ¡ng | 1-31 |
| `month` | ThÃ¡ng | 1-12 |
| `year` | NÄƒm | 2012-2018 |
| `week_of_year` | Tuáº§n trong nÄƒm | 1-52 |
| `quarter` | QuÃ½ | 1-4 |
| `season` | MÃ¹a | 0=Spring, 1=Summer, 2=Fall, 3=Winter |
| `is_weekend` | Cuá»‘i tuáº§n? | 0 hoáº·c 1 |
| `is_rush_hour` | Giá» cao Ä‘iá»ƒm? (7-9h, 16-18h) | 0 hoáº·c 1 |

**B. Cyclical Features (mÃ£ hÃ³a tuáº§n hoÃ n):**
| Feature | CÃ´ng thá»©c | Má»¥c Ä‘Ã­ch |
|---------|-----------|----------|
| `hour_sin` | sin(2Ï€ Ã— hour/24) | MÃ£ hÃ³a giá» dáº¡ng vÃ²ng trÃ²n |
| `hour_cos` | cos(2Ï€ Ã— hour/24) | Giá» 23 gáº§n giá» 0 |
| `day_sin` | sin(2Ï€ Ã— day_of_week/7) | MÃ£ hÃ³a ngÃ y trong tuáº§n |
| `day_cos` | cos(2Ï€ Ã— day_of_week/7) | Chá»§ nháº­t gáº§n thá»© 2 |
| `month_sin` | sin(2Ï€ Ã— month/12) | MÃ£ hÃ³a thÃ¡ng |
| `month_cos` | cos(2Ï€ Ã— month/12) | ThÃ¡ng 12 gáº§n thÃ¡ng 1 |

**C. Weather Features:**
| Feature | MÃ´ táº£ |
|---------|-------|
| `temp_celsius` | Nhiá»‡t Ä‘á»™ (Â°C) = temp - 273.15 |
| `is_rainy` | CÃ³ mÆ°a? (rain_1h > 0) |
| `is_snowy` | CÃ³ tuyáº¿t? (snow_1h > 0) |

**D. Interaction Features:**
| Feature | MÃ´ táº£ |
|---------|-------|
| `rush_rain` | is_rush_hour Ã— is_rainy |

#### 1.1.4 Thuá»™c tÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng cho X_train vÃ  y_train

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INPUT SEQUENCE (X)                            â”‚
â”‚                        Shape: (n_samples, 24, 22)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  22 Features (táº¡i má»—i timestep):                                â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  1. traffic_volume (target cÅ©ng lÃ  feature cho input)           â”‚   â”‚
â”‚  â”‚  2. temp, temp_celsius                                          â”‚   â”‚
â”‚  â”‚  3. clouds_all, rain_1h, snow_1h                                â”‚   â”‚
â”‚  â”‚  4. hour, day_of_week, day_of_month, month, year                â”‚   â”‚
â”‚  â”‚  5. week_of_year, quarter, season                               â”‚   â”‚
â”‚  â”‚  6. is_weekend, is_rush_hour                                    â”‚   â”‚
â”‚  â”‚  7. hour_sin, hour_cos, day_sin, day_cos, month_sin, month_cos  â”‚   â”‚
â”‚  â”‚  8. is_rainy, is_snowy                                          â”‚   â”‚
â”‚  â”‚  9. rush_rain                                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  Sequence length: 24 timesteps (24 giá» lá»‹ch sá»­)                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          OUTPUT SEQUENCE (y)                            â”‚
â”‚                          Shape: (n_samples, 5)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Chá»‰ cÃ³ 1 thuá»™c tÃ­nh: traffic_volume                                   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  y[0] = traffic_volume táº¡i t+1                                  â”‚   â”‚
â”‚  â”‚  y[1] = traffic_volume táº¡i t+2                                  â”‚   â”‚
â”‚  â”‚  y[2] = traffic_volume táº¡i t+3                                  â”‚   â”‚
â”‚  â”‚  y[3] = traffic_volume táº¡i t+4                                  â”‚   â”‚
â”‚  â”‚  y[4] = traffic_volume táº¡i t+5                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  Dá»± bÃ¡o 5 giá» tiáº¿p theo                                                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**TÃ³m táº¯t:**
- **X_train:** Sá»­ dá»¥ng **22 features** (bao gá»“m cáº£ traffic_volume) trong **24 timesteps**
- **y_train:** Chá»‰ sá»­ dá»¥ng **traffic_volume** cho **5 timesteps tiáº¿p theo**

---

### 1.2 HIá»‚U CÃCH TIá»€N Xá»¬ LÃ Dá»® LIá»†U

#### 1.2.1 Pipeline Tiá»n Xá»­ LÃ½

```
Raw Data (48,204 rows)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Convert DateTime             â”‚
â”‚  - Parse date_time â†’ datetime object  â”‚
â”‚  - Sort by timestamp                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Handle Duplicates            â”‚
â”‚  - 7,629 duplicate timestamps found   â”‚
â”‚  - Aggregation strategy:              â”‚
â”‚    â€¢ temp, clouds_all â†’ mean          â”‚
â”‚    â€¢ rain_1h, snow_1h â†’ max           â”‚
â”‚    â€¢ traffic_volume â†’ mean            â”‚
â”‚    â€¢ weather_main â†’ combine unique    â”‚
â”‚    â€¢ holiday â†’ any non-null           â”‚
â”‚  - Result: 40,575 rows                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Handle Missing Values        â”‚
â”‚  - Numerical: interpolate (linear)    â”‚
â”‚  - Categorical: forward fill          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Handle Outliers (IQR)        â”‚
â”‚  - Column: traffic_volume             â”‚
â”‚  - Method: IQR vá»›i factor = 1.5       â”‚
â”‚  - Q1 = 25th percentile               â”‚
â”‚  - Q3 = 75th percentile               â”‚
â”‚  - IQR = Q3 - Q1                      â”‚
â”‚  - Lower = Q1 - 1.5 Ã— IQR             â”‚
â”‚  - Upper = Q3 + 1.5 Ã— IQR             â”‚
â”‚  - Action: Clip values to [Lower,Upper]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Check Time Continuity        â”‚
â”‚  - KHÃ”NG resample/interpolate target  â”‚
â”‚  - Giá»¯ nguyÃªn data tháº­t               â”‚
â”‚  - Xá»­ lÃ½ gaps trong Data Preparation  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
Cleaned Data (40,575 rows)
```

#### 1.2.2 Code Xá»­ LÃ½ Duplicates (Chi tiáº¿t)

```python
# Thay vÃ¬ chá»‰ drop duplicates, aggregation Ä‘á»ƒ giá»¯ thÃ´ng tin
df = df.groupby("date_time", as_index=False).agg({
    "temp": "mean",           # Trung bÃ¬nh nhiá»‡t Ä‘á»™
    "rain_1h": "max",         # Náº¿u cÃ³ mÆ°a á»Ÿ báº¥t ká»³ row nÃ o â†’ giá»¯
    "snow_1h": "max",         # TÆ°Æ¡ng tá»± cho tuyáº¿t
    "clouds_all": "mean",     # Trung bÃ¬nh % mÃ¢y
    "traffic_volume": "mean", # Trung bÃ¬nh lÆ°u lÆ°á»£ng
    "weather_main": lambda x: ",".join(sorted(set(x))),  # Gá»™p unique
    "holiday": lambda x: 0 if x.isna().all() else 1      # Binary
})
```

#### 1.2.3 Code Xá»­ LÃ½ Outliers (IQR Method)

```python
def handle_outliers_iqr(df, column, factor=1.5, method='clip'):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower = Q1 - factor * IQR
    upper = Q3 + factor * IQR
    
    # Clip values ngoÃ i khoáº£ng [lower, upper]
    df[column] = df[column].clip(lower=lower, upper=upper)
    
    return df
```

#### 1.2.4 Segment-Based Data Preparation (QUAN TRá»ŒNG!)

**Váº¥n Ä‘á»:** Dá»¯ liá»‡u cÃ³ 2,588 timestamp gaps (khÃ´ng liÃªn tá»¥c)

```
Timeline thá»±c táº¿:
... 10:00 â†’ 11:00 â†’ 12:00 â†’ [GAP] â†’ 15:00 â†’ 16:00 ...
                              â†‘
                    Thiáº¿u 13:00, 14:00
```

**Giáº£i phÃ¡p:** TÃ¡ch data thÃ nh cÃ¡c segments liÃªn tá»¥c

```python
def split_continuous_segments(df, date_col, target_col, min_length=48, freq_hours=1):
    """
    TÃ¡ch DataFrame thÃ nh cÃ¡c segment THá»°C Sá»° liÃªn tá»¥c:
    1. KhÃ´ng cÃ³ NaN trong target
    2. KhÃ´ng cÃ³ gap trong timestamps (má»—i row cÃ¡ch nhau Ä‘Ãºng freq_hours)
    """
    # TÃ­nh time difference giá»¯a cÃ¡c rows liÃªn tiáº¿p
    time_diff = df[date_col].diff()
    expected_diff = pd.Timedelta(hours=freq_hours)
    
    # PhÃ¡t hiá»‡n gap: time_diff > expected_diff
    is_gap = (time_diff > expected_diff + tolerance) | time_diff.isna()
    
    # Táº¡o segment ID: tÄƒng má»—i khi gáº·p gap
    segment_id = is_gap.cumsum()
    
    # Chá»‰ giá»¯ segments Ä‘á»§ dÃ i (>= min_length)
    # ...
```

**Káº¿t quáº£:**
```
Original rows:     40,575
Gaps detected:      2,588
Segments created:     113
Usable rows:       30,871 (76.1%)
```

#### 1.2.5 Scaling Data

```python
from sklearn.preprocessing import MinMaxScaler

# QUAN TRá»ŒNG: Chá»‰ fit scaler trÃªn TRAINING data
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit + Transform
X_val_scaled = scaler.transform(X_val)          # Chá»‰ Transform
X_test_scaled = scaler.transform(X_test)        # Chá»‰ Transform

# LÃ½ do: TrÃ¡nh data leakage tá»« validation/test vÃ o training
```

#### 1.2.6 Train/Validation/Test Split

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TIMELINE-BASED SPLIT                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â—„â”€â”€â”€â”€â”€â”€â”€ TRAIN (70%) â”€â”€â”€â”€â”€â”€â”€â–ºâ—„â”€ VAL (15%) â”€â–ºâ—„â”€ TEST (15%) â”€â–º  â”‚
â”‚                                                                  â”‚
â”‚  [Past data]                                  [Future data]      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âŒ KHÃ”NG shuffle vÃ¬:
   - Time series pháº£i giá»¯ thá»© tá»± thá»i gian
   - Train trÃªn quÃ¡ khá»©, predict tÆ°Æ¡ng lai
   - Shuffle â†’ data leakage tá»« future
```

---

### 1.3 HIá»‚U THIáº¾T Káº¾ KIáº¾N TRÃšC Cá»¦A CÃC Máº NG ÄÆ¯á»¢C Sá»¬ Dá»¤NG

#### 1.3.1 Táº¡i sao dÃ¹ng Encoder-Decoder cho bÃ i toÃ¡n nÃ y?

**BÃ i toÃ¡n:** Dá»± bÃ¡o chuá»—i thá»i gian nhiá»u bÆ°á»›c (Multi-step Forecasting)
- **Input:** Sequence cÃ³ Ä‘á»™ dÃ i 24 (24 giá» lá»‹ch sá»­)
- **Output:** Sequence cÃ³ Ä‘á»™ dÃ i 5 (5 giá» dá»± bÃ¡o)

**Váº¥n Ä‘á» vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng:**

| PhÆ°Æ¡ng phÃ¡p | MÃ´ táº£ | NhÆ°á»£c Ä‘iá»ƒm |
|-------------|-------|------------|
| **Direct Multi-Output** | 1 model â†’ 5 outputs cÃ¹ng lÃºc | KhÃ´ng capture dependencies giá»¯a cÃ¡c outputs |
| **Recursive (Iterated)** | Predict t+1, dÃ¹ng lÃ m input predict t+2,... | Error accumulation nghiÃªm trá»ng |
| **Direct (5 models)** | 5 models riÃªng biá»‡t | KhÃ´ng share knowledge, tá»‘n resources |

**Giáº£i phÃ¡p: Encoder-Decoder (Seq2Seq)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Táº I SAO ENCODER-DECODER?                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  1. ENCODER:                                                            â”‚
â”‚     - Äá»c TOÃ€N Bá»˜ 24 giá» lá»‹ch sá»­                                       â”‚
â”‚     - NÃ©n thÃ´ng tin thÃ nh "Context Vector"                             â”‚
â”‚     - Context chá»©a Ä‘á»§ thÃ´ng tin Ä‘á»ƒ dá»± bÃ¡o                              â”‚
â”‚                                                                         â”‚
â”‚  2. DECODER:                                                            â”‚
â”‚     - Sinh output TUáº¦N Tá»°: t+1 â†’ t+2 â†’ t+3 â†’ t+4 â†’ t+5                â”‚
â”‚     - Má»—i step nháº­n: context + output cá»§a step trÆ°á»›c                   â”‚
â”‚     - Capture Ä‘Æ°á»£c dependencies giá»¯a cÃ¡c predictions                   â”‚
â”‚                                                                         â”‚
â”‚  3. Lá»¢I ÃCH:                                                            â”‚
â”‚     - Flexible input/output length (24 â†’ 5)                            â”‚
â”‚     - Sequential generation (nhÆ° language model)                        â”‚
â”‚     - Teacher forcing giÃºp train á»•n Ä‘á»‹nh                                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.3.2 Kiáº¿n trÃºc tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ENCODER-DECODER ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  INPUT: (batch_size, 24, 22)                                           â”‚
â”‚  - batch_size: sá»‘ samples trong 1 batch                                â”‚
â”‚  - 24: sequence length (24 giá» lá»‹ch sá»­)                                â”‚
â”‚  - 22: sá»‘ features (traffic_volume + cÃ¡c features khÃ¡c)                â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                      ENCODER                                   â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚  Bidirectional LSTM                                      â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - input_size: 22 (sá»‘ features)                          â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - hidden_size: 64 (optimized by Optuna)                 â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - num_layers: 2 (stacked LSTM)                          â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - bidirectional: True (forward + backward)              â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - dropout: 0.248                                        â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                           â”‚                                    â”‚     â”‚
â”‚  â”‚                           â–¼                                    â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚  Linear Projection (vÃ¬ bidirectional)                   â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - 64*2 = 128 â†’ 64                                       â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                           â”‚                                    â”‚     â”‚
â”‚  â”‚                           â–¼                                    â”‚     â”‚
â”‚  â”‚  Output: (hidden_state, cell_state) = "Context Vector"        â”‚     â”‚
â”‚  â”‚  - hidden: (2, batch, 64) â†’ 2 layers, 64 dimensions          â”‚     â”‚
â”‚  â”‚  - cell: (2, batch, 64) â†’ memory cá»§a LSTM                    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â”‚ Context Vector truyá»n sang Decoder       â”‚
â”‚                              â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                      DECODER                                   â”‚     â”‚
â”‚  â”‚                                                                â”‚     â”‚
â”‚  â”‚  Láº·p 5 láº§n (5 output steps):                                  â”‚     â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚      â”‚  LSTM Cell                                           â”‚  â”‚     â”‚
â”‚  â”‚      â”‚  - input: previous_output (hoáº·c ground truth)        â”‚  â”‚     â”‚
â”‚  â”‚      â”‚  - hidden: tá»« encoder hoáº·c step trÆ°á»›c                â”‚  â”‚     â”‚
â”‚  â”‚      â”‚  - cell: tá»« encoder hoáº·c step trÆ°á»›c                  â”‚  â”‚     â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                           â”‚                                    â”‚     â”‚
â”‚  â”‚                           â–¼                                    â”‚     â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚      â”‚  Fully Connected: hidden_size â†’ 1                    â”‚  â”‚     â”‚
â”‚  â”‚      â”‚  Output: 1 giÃ¡ trá»‹ traffic_volume                    â”‚  â”‚     â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                           â”‚                                    â”‚     â”‚
â”‚  â”‚                           â–¼                                    â”‚     â”‚
â”‚  â”‚      predictions[t] = output value                            â”‚     â”‚
â”‚  â”‚                                                                â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                         â”‚
â”‚  OUTPUT: (batch_size, 5)                                               â”‚
â”‚  - 5 giÃ¡ trá»‹ traffic_volume cho t+1, t+2, t+3, t+4, t+5               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.3.3 ENCODER - Chi tiáº¿t hoáº¡t Ä‘á»™ng

##### A. LSTM Cell - ÄÆ¡n vá»‹ cÆ¡ báº£n

**LSTM (Long Short-Term Memory)** giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient cá»§a RNN thÃ´ng thÆ°á»ng báº±ng cÃ¡ch sá»­ dá»¥ng **3 cá»•ng** vÃ  **cell state**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LSTM CELL CHI TIáº¾T                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Input táº¡i timestep t:                                                  â”‚
â”‚  â€¢ x_t: input vector (22 features)                                     â”‚
â”‚  â€¢ h_{t-1}: hidden state tá»« timestep trÆ°á»›c                             â”‚
â”‚  â€¢ C_{t-1}: cell state tá»« timestep trÆ°á»›c                               â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  FORGET GATE (Cá»•ng quÃªn): Quyáº¿t Ä‘á»‹nh quÃªn thÃ´ng tin nÃ o        â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)                            â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â€¢ f_t â‰ˆ 0: QuÃªn thÃ´ng tin tá»« cell state cÅ©                    â”‚   â”‚
â”‚  â”‚  â€¢ f_t â‰ˆ 1: Giá»¯ thÃ´ng tin tá»« cell state cÅ©                     â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  VÃ­ dá»¥ Traffic: Khi chuyá»ƒn tá»« ngÃ y thÆ°á»ng â†’ ngÃ y lá»…,            â”‚   â”‚
â”‚  â”‚  forget gate "quÃªn" patterns ngÃ y thÆ°á»ng                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  INPUT GATE (Cá»•ng Ä‘áº§u vÃ o): Quyáº¿t Ä‘á»‹nh thÃªm thÃ´ng tin má»›i nÃ o  â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)     â† Lá»c thÃ´ng tin        â”‚   â”‚
â”‚  â”‚  CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)  â† ThÃ´ng tin má»›i        â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â€¢ i_t quyáº¿t Ä‘á»‹nh bao nhiÃªu cá»§a CÌƒ_t Ä‘Æ°á»£c thÃªm vÃ o              â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  VÃ­ dá»¥ Traffic: Khi cÃ³ mÆ°a lá»›n Ä‘á»™t ngá»™t,                        â”‚   â”‚
â”‚  â”‚  input gate thÃªm thÃ´ng tin "mÆ°a áº£nh hÆ°á»Ÿng traffic"              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  CELL STATE UPDATE: Cáº­p nháº­t bá»™ nhá»› dÃ i háº¡n                    â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t                               â”‚   â”‚
â”‚  â”‚        â†‘                   â†‘                                     â”‚   â”‚
â”‚  â”‚        QuÃªn bá»›t           ThÃªm má»›i                               â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  Cell state lÃ  "highway" cho gradient flow                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  OUTPUT GATE (Cá»•ng Ä‘áº§u ra): Quyáº¿t Ä‘á»‹nh output gÃ¬                â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)                            â”‚   â”‚
â”‚  â”‚  h_t = o_t âŠ™ tanh(C_t)                                          â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â€¢ Lá»c thÃ´ng tin tá»« cell state thÃ nh hidden state               â”‚   â”‚
â”‚  â”‚  â€¢ h_t Ä‘Æ°á»£c dÃ¹ng cho prediction vÃ  truyá»n sang timestep tiáº¿p    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  Output:                                                                â”‚
â”‚  â€¢ h_t: hidden state (cho layer tiáº¿p theo hoáº·c output)                 â”‚
â”‚  â€¢ C_t: cell state (truyá»n sang timestep tiáº¿p theo)                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### B. Bidirectional LSTM

Encoder sá»­ dá»¥ng **Bidirectional LSTM** - xá»­ lÃ½ sequence theo 2 hÆ°á»›ng:

```
Input Sequence: [x_1, x_2, x_3, ..., x_24]

Forward LSTM (â†’):
   x_1 â”€â”€â–º h_1^f â”€â”€â–º h_2^f â”€â”€â–º h_3^f â”€â”€â–º ... â”€â”€â–º h_24^f
           â”‚         â”‚         â”‚                  â”‚
   Há»c patterns tá»« quÃ¡ khá»© Ä‘áº¿n hiá»‡n táº¡i

Backward LSTM (â†):
   x_1 â—„â”€â”€ h_1^b â—„â”€â”€ h_2^b â—„â”€â”€ h_3^b â—„â”€â”€ ... â—„â”€â”€ h_24^b
           â”‚         â”‚         â”‚                  â”‚
   Há»c patterns tá»« tÆ°Æ¡ng lai vá» quÃ¡ khá»©

Combined output táº¡i má»—i timestep t:
   h_t = [h_t^f ; h_t^b]  (concatenate)
   
   â†’ Má»—i h_t chá»©a thÃ´ng tin tá»« Cáº¢ 2 HÆ¯á»šNG
```

**Táº¡i sao cáº§n Bidirectional?**

```
VÃ­ dá»¥: Traffic táº¡i 8:00 (giá» cao Ä‘iá»ƒm sÃ¡ng)

Forward only:  Chá»‰ tháº¥y 7:00, 6:00, 5:00... â†’ biáº¿t Ä‘ang Ä‘áº¿n rush hour
Backward only: Chá»‰ tháº¥y 9:00, 10:00... â†’ biáº¿t rush hour sáº¯p káº¿t thÃºc
Bidirectional: Tháº¥y Cáº¢ HAI â†’ hiá»ƒu Ä‘áº§y Ä‘á»§ context cá»§a rush hour
```

##### C. Code Encoder

```python
class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2, 
                 dropout=0.2, bidirectional=True):
        super(Encoder, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        self.num_directions = 2 if bidirectional else 1
        
        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,      # 22 features
            hidden_size=hidden_size,    # 64
            num_layers=num_layers,      # 2 stacked layers
            batch_first=True,           # (batch, seq, features)
            dropout=dropout,            # Dropout giá»¯a layers
            bidirectional=bidirectional
        )
        
        # Projection layers: 64*2 â†’ 64 (Ä‘á»ƒ match vá»›i Decoder)
        if bidirectional:
            self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)
            self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)
    
    def forward(self, x):
        """
        x: (batch, 24, 22) - 24 timesteps, 22 features
        
        Returns:
        - outputs: (batch, 24, 64*2) - all hidden states
        - (hidden, cell): final states, each (2, batch, 64)
        """
        # LSTM forward pass
        outputs, (hidden, cell) = self.lstm(x)
        # outputs: (batch, 24, 128) náº¿u bidirectional
        # hidden: (4, batch, 64) = 2 layers Ã— 2 directions
        
        if self.bidirectional:
            # Reshape: (num_layers * 2, batch, hidden) â†’ (num_layers, batch, hidden * 2)
            batch_size = hidden.shape[1]
            
            # TÃ¡ch forward vÃ  backward cho má»—i layer
            hidden = hidden.view(self.num_layers, 2, batch_size, self.hidden_size)
            cell = cell.view(self.num_layers, 2, batch_size, self.hidden_size)
            
            # Concatenate forward vÃ  backward
            hidden = torch.cat([hidden[:, 0, :, :], hidden[:, 1, :, :]], dim=2)
            cell = torch.cat([cell[:, 0, :, :], cell[:, 1, :, :]], dim=2)
            
            # Project 128 â†’ 64
            hidden = self.fc_hidden(hidden)  # (2, batch, 64)
            cell = self.fc_cell(cell)        # (2, batch, 64)
        
        return outputs, (hidden, cell)
```

#### 1.3.4 DECODER - Chi tiáº¿t hoáº¡t Ä‘á»™ng

##### A. CÃ¡ch Decoder sinh output

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECODER STEP-BY-STEP                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Nháº­n tá»« Encoder:                                                       â”‚
â”‚  â€¢ hidden_0 = encoder_hidden (context vá» toÃ n bá»™ input)                â”‚
â”‚  â€¢ cell_0 = encoder_cell                                               â”‚
â”‚                                                                         â”‚
â”‚  STEP t=0 (predict t+1):                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  input_0 = last_traffic_volume tá»« input sequence                 â”‚  â”‚
â”‚  â”‚           (giÃ¡ trá»‹ traffic cuá»‘i cÃ¹ng mÃ  model "biáº¿t")            â”‚  â”‚
â”‚  â”‚                          â†“                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚         LSTM Cell                                        â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  input: input_0, hidden_0, cell_0                        â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  output: hidden_1, cell_1                                â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                          â†“                                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚  Fully Connected: hidden_1 â†’ prediction_0                â”‚    â”‚  â”‚
â”‚  â”‚  â”‚  prediction_0 = traffic_volume táº¡i t+1                   â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  STEP t=1 (predict t+2):                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  if Teacher Forcing (random < 0.65):                              â”‚  â”‚
â”‚  â”‚      input_1 = ground_truth[t+1]  â† DÃ¹ng Ä‘Ã¡p Ã¡n Ä‘Ãºng             â”‚  â”‚
â”‚  â”‚  else:                                                            â”‚  â”‚
â”‚  â”‚      input_1 = prediction_0       â† DÃ¹ng prediction vá»«a sinh     â”‚  â”‚
â”‚  â”‚                          â†“                                        â”‚  â”‚
â”‚  â”‚  LSTM Cell(input_1, hidden_1, cell_1) â†’ hidden_2, cell_2          â”‚  â”‚
â”‚  â”‚                          â†“                                        â”‚  â”‚
â”‚  â”‚  FC(hidden_2) â†’ prediction_1 = traffic_volume táº¡i t+2            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â”‚  ... Láº·p tÆ°Æ¡ng tá»± cho t=2,3,4 ...                                      â”‚
â”‚                                                                         â”‚
â”‚  Final Output: [prediction_0, prediction_1, ..., prediction_4]         â”‚
â”‚               = [traffic_t+1, traffic_t+2, ..., traffic_t+5]           â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### B. Teacher Forcing - Ká»¹ thuáº­t huáº¥n luyá»‡n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TEACHER FORCING EXPLAINED                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Váº¤N Äá»€: Autoregressive Decoding                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  â€¢ Decoder dÃ¹ng output cá»§a step trÆ°á»›c lÃ m input cho step tiáº¿p          â”‚
â”‚  â€¢ Náº¿u step Ä‘áº§u sai â†’ error lan truyá»n (error accumulation)            â”‚
â”‚                                                                         â”‚
â”‚  KhÃ´ng cÃ³ Teacher Forcing:                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  True:    [1000, 1100, 1200, 1150, 1050]                               â”‚
â”‚  Step 0:  Predict 1000 â†’ OK                                            â”‚
â”‚  Step 1:  Input=1000, Predict 1080 (sai 20)                            â”‚
â”‚  Step 2:  Input=1080 (Ä‘Ã£ sai), Predict 1250 (sai 50)                   â”‚
â”‚  Step 3:  Input=1250 (sai hÆ¡n), Predict 1400 (sai 250!)                â”‚
â”‚  â†’ Error tÃ­ch lÅ©y ngÃ y cÃ ng lá»›n!                                       â”‚
â”‚                                                                         â”‚
â”‚  Vá»šI Teacher Forcing (ratio = 0.65):                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  True:    [1000, 1100, 1200, 1150, 1050]                               â”‚
â”‚  Step 0:  Predict 1000                                                 â”‚
â”‚  Step 1:  Random < 0.65 â†’ Input = TRUE 1100, Predict 1095              â”‚
â”‚           (Model há»c tá»« Ä‘Ã¡p Ã¡n Ä‘Ãºng, khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi error)     â”‚
â”‚  Step 2:  Random > 0.65 â†’ Input = predicted 1095, Predict 1185         â”‚
â”‚           (Model cÅ©ng há»c xá»­ lÃ½ khi input khÃ´ng hoÃ n háº£o)              â”‚
â”‚                                                                         â”‚
â”‚  â†’ CÃ¢n báº±ng giá»¯a:                                                       â”‚
â”‚    â€¢ Há»c nhanh (dÃ¹ng ground truth)                                     â”‚
â”‚    â€¢ Robust (quen vá»›i imperfect input)                                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Decay Teacher Forcing Ratio:**

```python
# Trong training, giáº£m dáº§n teacher forcing theo epoch
current_tf_ratio = teacher_forcing_ratio * (1 - epoch / num_epochs)

# Epoch 1:  tf_ratio = 0.65 * (1 - 0/100) = 0.65 (dÃ¹ng nhiá»u ground truth)
# Epoch 50: tf_ratio = 0.65 * (1 - 50/100) = 0.325
# Epoch 99: tf_ratio = 0.65 * (1 - 99/100) â‰ˆ 0.007 (gáº§n nhÆ° autoregressive)

# â†’ Model dáº§n quen vá»›i viá»‡c dÃ¹ng chÃ­nh predictions cá»§a mÃ¬nh
```

##### C. Code Decoder

```python
class Decoder(nn.Module):
    def __init__(self, output_size=1, hidden_size=64, num_layers=2, dropout=0.2):
        super(Decoder, self).__init__()
        
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # LSTM: input lÃ  1 giÃ¡ trá»‹ (previous traffic_volume)
        self.lstm = nn.LSTM(
            input_size=output_size,     # 1
            hidden_size=hidden_size,    # 64
            num_layers=num_layers,      # 2
            batch_first=True,
            dropout=dropout
        )
        
        # Fully Connected: hidden â†’ 1 output
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x, hidden, cell):
        """
        x: (batch, 1, 1) - previous output/ground truth
        hidden: (2, batch, 64) - tá»« encoder hoáº·c step trÆ°á»›c
        cell: (2, batch, 64)
        
        Returns:
        - prediction: (batch, 1, 1)
        - (hidden, cell): updated states
        """
        output, (hidden, cell) = self.lstm(x, (hidden, cell))
        # output: (batch, 1, 64)
        
        prediction = self.fc(output)  # (batch, 1, 1)
        
        return prediction, (hidden, cell)
```

#### 1.3.5 Seq2Seq - Káº¿t há»£p Encoder vÃ  Decoder

```python
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, output_seq_len=5, device=None):
        super(Seq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.output_seq_len = output_seq_len  # 5 steps
        self.device = device
    
    def forward(self, x, target=None, teacher_forcing_ratio=0.5):
        """
        x: (batch, 24, 22) - input sequence
        target: (batch, 5) - ground truth (for teacher forcing)
        teacher_forcing_ratio: probability of using ground truth
        
        Returns: (batch, 5) - 5 predictions
        """
        batch_size = x.shape[0]
        
        # 1. ENCODE: NÃ©n 24 timesteps thÃ nh context vector
        _, (hidden, cell) = self.encoder(x)
        # hidden, cell: (2, batch, 64) - context tá»« toÃ n bá»™ input
        
        # 2. DECODE: Sinh 5 outputs tuáº§n tá»±
        
        # Initial input: last traffic_volume tá»« input sequence
        # x[:, -1, 0] = traffic_volume táº¡i timestep cuá»‘i cÃ¹ng
        decoder_input = x[:, -1, 0].unsqueeze(1).unsqueeze(2)  # (batch, 1, 1)
        
        outputs = []
        
        for t in range(self.output_seq_len):  # 5 steps
            # Decode 1 step
            prediction, (hidden, cell) = self.decoder(decoder_input, hidden, cell)
            outputs.append(prediction.squeeze(2))  # (batch, 1)
            
            # Chá»n input cho step tiáº¿p theo
            if target is not None and torch.rand(1).item() < teacher_forcing_ratio:
                # Teacher Forcing: dÃ¹ng ground truth
                decoder_input = target[:, t].unsqueeze(1).unsqueeze(2)
            else:
                # Autoregressive: dÃ¹ng prediction vá»«a sinh
                decoder_input = prediction
        
        # Concatenate: [(batch, 1), ...] â†’ (batch, 5)
        outputs = torch.cat(outputs, dim=1)
        
        return outputs
```

#### 1.3.6 Flow hoÃ n chá»‰nh má»™t Forward Pass

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 COMPLETE FORWARD PASS EXAMPLE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  INPUT: batch_size=32, seq_len=24, features=22                         â”‚
â”‚  X shape: (32, 24, 22)                                                 â”‚
â”‚                                                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚  ENCODER                                                                â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                         â”‚
â”‚  1. Bidirectional LSTM:                                                â”‚
â”‚     X (32, 24, 22) â”€â”€â–º LSTM â”€â”€â–º outputs (32, 24, 128)                  â”‚
â”‚                              â””â”€â–º hidden (4, 32, 64)                    â”‚
â”‚                              â””â”€â–º cell (4, 32, 64)                      â”‚
â”‚                                                                         â”‚
â”‚  2. Reshape & Project:                                                 â”‚
â”‚     hidden (4, 32, 64) â”€â”€â–º (2, 32, 128) â”€â”€â–º fc â”€â”€â–º (2, 32, 64)        â”‚
â”‚     cell   (4, 32, 64) â”€â”€â–º (2, 32, 128) â”€â”€â–º fc â”€â”€â–º (2, 32, 64)        â”‚
â”‚                                                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚  DECODER (5 iterations)                                                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                         â”‚
â”‚  Initial: decoder_input = X[:, -1, 0] = (32, 1, 1)                     â”‚
â”‚                                                                         â”‚
â”‚  t=0: input(32,1,1) + hidden(2,32,64) â”€â”€â–º LSTM â”€â”€â–º hidden'(2,32,64)   â”‚
â”‚       â””â”€â–º FC â”€â”€â–º pred_0 (32, 1)                                        â”‚
â”‚                                                                         â”‚
â”‚  t=1: input(32,1,1) + hidden'(2,32,64) â”€â”€â–º LSTM â”€â”€â–º hidden''          â”‚
â”‚       â””â”€â–º FC â”€â”€â–º pred_1 (32, 1)                                        â”‚
â”‚                                                                         â”‚
â”‚  ... (repeat for t=2, 3, 4) ...                                        â”‚
â”‚                                                                         â”‚
â”‚  3. Concatenate:                                                        â”‚
â”‚     [pred_0, pred_1, pred_2, pred_3, pred_4] â”€â”€â–º (32, 5)               â”‚
â”‚                                                                         â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚  OUTPUT: (32, 5) = 32 samples Ã— 5 predictions                          â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1.4 OPTUNA HYPERPARAMETER OPTIMIZATION

#### 1.4.1 Optuna lÃ  gÃ¬?

**Optuna** lÃ  má»™t framework tá»± Ä‘á»™ng tá»‘i Æ°u hÃ³a hyperparameters (AutoML) vá»›i cÃ¡c Ä‘áº·c Ä‘iá»ƒm:

1. **Define-by-Run API:** Äá»‹nh nghÄ©a search space linh hoáº¡t trong code
2. **Efficient Sampling:** Sá»­ dá»¥ng thuáº­t toÃ¡n TPE (Tree-structured Parzen Estimator)
3. **Pruning:** Dá»«ng sá»›m cÃ¡c trials khÃ´ng há»©a háº¹n Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian
4. **Visualization:** Cung cáº¥p cÃ¡c cÃ´ng cá»¥ visualize káº¿t quáº£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPTUNA OPTIMIZATION FLOW                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚ Search Space â”‚  Äá»‹nh nghÄ©a khoáº£ng giÃ¡ trá»‹ cho má»—i hyperparameter    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚         â”‚                                                               â”‚
â”‚         â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚   Sampler    â”‚  TPE chá»n giÃ¡ trá»‹ hyperparameters thÃ´ng minh         â”‚
â”‚  â”‚    (TPE)     â”‚  dá»±a trÃªn káº¿t quáº£ cÃ¡c trials trÆ°á»›c                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚         â”‚                                                               â”‚
â”‚         â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚    Trial     â”‚  Huáº¥n luyá»‡n model vá»›i hyperparameters Ä‘Æ°á»£c chá»n      â”‚
â”‚  â”‚  (Training)  â”‚                                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚         â”‚                                                               â”‚
â”‚         â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚   Pruner     â”‚  Dá»«ng sá»›m náº¿u trial khÃ´ng tá»‘t (MedianPruner)         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚         â”‚                                                               â”‚
â”‚         â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚  Objective   â”‚  Tráº£ vá» validation loss Ä‘á»ƒ Optuna Ä‘Ã¡nh giÃ¡           â”‚
â”‚  â”‚    Value     â”‚                                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚         â”‚                                                               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Láº·p láº¡i cho n_trials                              â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.4.2 CÃ¡c Hyperparameters Ä‘Æ°á»£c tá»‘i Æ°u

Trong project nÃ y, Optuna tá»‘i Æ°u **7 hyperparameters**:

| Hyperparameter | Search Space | Ã nghÄ©a | TÃ¡c dá»¥ng |
|----------------|--------------|---------|----------|
| **hidden_size** | [64, 128, 256] | Sá»‘ neurons trong hidden layer cá»§a LSTM | CÃ ng lá»›n â†’ model cÃ ng phá»©c táº¡p, há»c Ä‘Æ°á»£c patterns phá»©c táº¡p hÆ¡n, nhÆ°ng dá»… overfit |
| **num_layers** | [1, 2, 3] | Sá»‘ LSTM layers xáº¿p chá»“ng | Nhiá»u layers â†’ há»c hierarchical features, nhÆ°ng khÃ³ train hÆ¡n |
| **dropout** | (0.1, 0.5) | Tá»· lá»‡ dropout giá»¯a cÃ¡c layers | Regularization, giáº£m overfitting |
| **learning_rate** | (1e-4, 1e-2) | Tá»‘c Ä‘á»™ há»c cá»§a optimizer | QuÃ¡ cao â†’ khÃ´ng há»™i tá»¥, quÃ¡ tháº¥p â†’ train cháº­m |
| **batch_size** | [32, 64, 128] | Sá»‘ samples trong 1 batch | áº¢nh hÆ°á»Ÿng Ä‘áº¿n gradient estimation vÃ  memory |
| **weight_decay** | (1e-6, 1e-3) | L2 regularization strength | Giáº£m overfitting báº±ng cÃ¡ch penalize large weights |
| **teacher_forcing_ratio** | (0.3, 0.7) | Tá»· lá»‡ dÃ¹ng ground truth khi train decoder | CÃ¢n báº±ng giá»¯a há»c nhanh vÃ  exposure bias |

#### 1.4.3 Chi tiáº¿t tá»«ng Hyperparameter

##### A. Hidden Size (64, 128, 256)

```
hidden_size = 64:        hidden_size = 256:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  64 neurons    â”‚       â”‚         256 neurons                â”‚
â”‚  Ãt parameters â”‚       â”‚         Nhiá»u parameters           â”‚
â”‚  Nhanh train   â”‚       â”‚         Cháº­m train                 â”‚
â”‚  CÃ³ thá»ƒ underfitï½¾      â”‚         CÃ³ thá»ƒ overfit             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **capacity** (kháº£ nÄƒng há»c) cá»§a model
- hidden_size = 64 Ä‘Æ°á»£c chá»n: Äá»§ Ä‘á»ƒ capture patterns mÃ  khÃ´ng overfit

##### B. Number of Layers (1, 2, 3)

```
1 Layer:           2 Layers:              3 Layers:
â”Œâ”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”
â”‚LSTM â”‚           â”‚LSTM â”‚ â† Layer 2      â”‚LSTM â”‚ â† Layer 3
â””â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”˜
                  â”Œâ”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”
                  â”‚LSTM â”‚ â† Layer 1      â”‚LSTM â”‚ â† Layer 2
                  â””â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”˜
                                         â”Œâ”€â”€â”€â”€â”€â”
                                         â”‚LSTM â”‚ â† Layer 1
                                         â””â”€â”€â”€â”€â”€â”˜
```

**TÃ¡c dá»¥ng:**
- Nhiá»u layers = há»c **hierarchical representations**
- Layer 1: Low-level patterns (trends ngáº¯n háº¡n)
- Layer 2+: High-level patterns (trends dÃ i háº¡n)
- num_layers = 2 Ä‘Æ°á»£c chá»n: Balance giá»¯a capacity vÃ  training difficulty

##### C. Dropout (0.1 - 0.5)

```python
# Dropout hoáº¡t Ä‘á»™ng trong training:
# Randomly "táº¯t" má»™t sá»‘ neurons vá»›i xÃ¡c suáº¥t p

Input:  [0.5, 0.3, 0.8, 0.2, 0.6]
              â†“ dropout=0.3
Output: [0.5, 0.0, 0.8, 0.0, 0.6]  # 30% neurons bá»‹ táº¯t
              â†“ scale by 1/(1-p)
Output: [0.71, 0.0, 1.14, 0.0, 0.86]  # Scale Ä‘á»ƒ giá»¯ expected value
```

**TÃ¡c dá»¥ng:**
- **Regularization:** Giáº£m overfitting báº±ng cÃ¡ch buá»™c model khÃ´ng phá»¥ thuá»™c vÃ o báº¥t ká»³ neuron nÃ o
- **Ensemble effect:** NhÆ° train nhiá»u sub-networks rá»“i average
- dropout â‰ˆ 0.25 Ä‘Æ°á»£c chá»n: Vá»«a Ä‘á»§ regularization

##### D. Learning Rate (1e-4 - 1e-2)

```
Learning Rate quÃ¡ cao (0.01):     Learning Rate quÃ¡ tháº¥p (0.0001):
        Loss                              Loss
          â”‚                                 â”‚
          â”‚  â•±â•²  â•±â•²                         â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          â”‚ â•±  â•²â•±  â•²                        â”‚          â•²
          â”‚â•±        â•²                       â”‚           â•²
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
            Epochs                              Epochs
      Oscillating, khÃ´ng há»™i tá»¥           Há»™i tá»¥ cháº­m, tá»‘n thá»i gian

Learning Rate phÃ¹ há»£p (~0.001):
        Loss
          â”‚
          â”‚â•²
          â”‚ â•²
          â”‚  â•²â”€â”€â”€â”€â”€â”€â”€â”€
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
              Epochs
      Há»™i tá»¥ nhanh vÃ  á»•n Ä‘á»‹nh
```

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **step size** khi update weights
- lr â‰ˆ 0.0015 Ä‘Æ°á»£c chá»n: Há»™i tá»¥ nhanh mÃ  á»•n Ä‘á»‹nh

##### E. Batch Size (32, 64, 128)

| Batch Size | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|------------|---------|------------|
| 32 (nhá») | Gradient noisy â†’ better generalization | Cháº­m (nhiá»u updates/epoch) |
| 128 (lá»›n) | Nhanh, stable gradients | CÃ³ thá»ƒ converge Ä‘áº¿n sharp minima |
| 64 (vá»«a) | Balance giá»¯a speed vÃ  generalization | - |

**TÃ¡c dá»¥ng:**
- batch_size = 64 Ä‘Æ°á»£c chá»n: CÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  cháº¥t lÆ°á»£ng

##### F. Weight Decay (1e-6 - 1e-3)

```python
# L2 Regularization: ThÃªm penalty vÃ o loss
# Loss_total = Loss_original + weight_decay * Î£(wÂ²)

# weight_decay = 0.0004:
# Penalize cÃ¡c weights lá»›n â†’ model Ä‘Æ¡n giáº£n hÆ¡n â†’ giáº£m overfit
```

**TÃ¡c dá»¥ng:**
- **L2 regularization** trong optimizer
- Giá»¯ weights nhá» â†’ model Ã­t overfit

##### G. Teacher Forcing Ratio (0.3 - 0.7)

```
Teacher Forcing Ratio = 0.5:

Training Step t=1:
  Input: last_known_value
  Output: prediction_1
  
Training Step t=2:
  if random() < 0.5:   # 50% chance
      Input = ground_truth[1]     # Teacher Forcing: dÃ¹ng Ä‘Ã¡p Ã¡n Ä‘Ãºng
  else:
      Input = prediction_1        # Autoregressive: dÃ¹ng prediction
  Output: prediction_2
```

**TÃ¡c dá»¥ng:**
- **Teacher Forcing = 1.0:** LuÃ´n dÃ¹ng ground truth â†’ train nhanh nhÆ°ng **exposure bias** (inference khÃ¡c training)
- **Teacher Forcing = 0.0:** LuÃ´n dÃ¹ng prediction â†’ train cháº­m, dá»… error accumulation
- **Teacher Forcing â‰ˆ 0.65:** CÃ¢n báº±ng giá»¯a train nhanh vÃ  giáº£m exposure bias

#### 1.4.4 Káº¿t quáº£ Optuna trong Project

**Best Hyperparameters tÃ¬m Ä‘Æ°á»£c:**

```json
{
    "hidden_size": 64,
    "num_layers": 2,
    "dropout": 0.248,
    "learning_rate": 0.00155,
    "batch_size": 64,
    "weight_decay": 0.000379,
    "teacher_forcing_ratio": 0.648
}
```

**PhÃ¢n tÃ­ch káº¿t quáº£:**

| Hyperparameter | GiÃ¡ trá»‹ | Nháº­n xÃ©t |
|----------------|---------|----------|
| hidden_size = 64 | Nhá» nháº¥t | Data khÃ´ng quÃ¡ phá»©c táº¡p, 64 Ä‘á»§ capacity |
| num_layers = 2 | Trung bÃ¬nh | Cáº§n hierarchy nhÆ°ng khÃ´ng quÃ¡ sÃ¢u |
| dropout â‰ˆ 0.25 | Trung bÃ¬nh | Regularization vá»«a pháº£i |
| lr â‰ˆ 0.0015 | Cao hÆ¡n default | Model cÃ³ thá»ƒ há»c nhanh |
| batch_size = 64 | Trung bÃ¬nh | Balance speed vÃ  generalization |
| weight_decay â‰ˆ 0.0004 | Nhá» | KhÃ´ng cáº§n quÃ¡ nhiá»u L2 reg |
| teacher_forcing â‰ˆ 0.65 | Cao | Æ¯u tiÃªn learning speed |

#### 1.4.5 Code Optuna trong Project

```python
class Seq2SeqObjective:
    """Objective function cho Optuna optimization"""
    
    def __call__(self, trial: optuna.Trial) -> float:
        # 1. Sample hyperparameters tá»« search space
        hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])
        num_layers = trial.suggest_categorical('num_layers', [1, 2, 3])
        dropout = trial.suggest_float('dropout', 0.1, 0.5)
        lr = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])
        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)
        teacher_forcing = trial.suggest_float('teacher_forcing_ratio', 0.3, 0.7)
        
        # 2. Build model vá»›i hyperparameters Ä‘Æ°á»£c chá»n
        model = build_model(
            input_size=self.input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            ...
        )
        
        # 3. Training loop vá»›i early stopping
        for epoch in range(self.n_epochs):
            train_loss = train_one_epoch(...)
            val_loss = validate(...)
            
            # 4. Report Ä‘á»ƒ Optuna cÃ³ thá»ƒ prune
            trial.report(val_loss, epoch)
            if trial.should_prune():
                raise optuna.TrialPruned()
        
        # 5. Return validation loss (minimize)
        return best_val_loss


# Run optimization
study = optuna.create_study(
    direction="minimize",  # Minimize validation loss
    pruner=optuna.pruners.MedianPruner()  # Prune trials tá»‡ hÆ¡n median
)
study.optimize(objective, n_trials=50, timeout=3600)
```

#### 1.4.6 Pruning - Dá»«ng sá»›m trials khÃ´ng tá»‘t

```
Trial 1: val_loss = [0.5, 0.4, 0.3, 0.25, 0.22] â†’ Complete âœ“
Trial 2: val_loss = [0.6, 0.55, 0.52, ...     ] â†’ Pruned âœ— (worse than median)
Trial 3: val_loss = [0.45, 0.35, 0.28, 0.23, 0.20] â†’ Complete âœ“ (Best!)
Trial 4: val_loss = [0.7, 0.65, ...           ] â†’ Pruned âœ—
...

MedianPruner: Náº¿u val_loss táº¡i epoch t > median cá»§a cÃ¡c trials khÃ¡c táº¡i epoch t â†’ Prune
```

**TÃ¡c dá»¥ng:**
- Tiáº¿t kiá»‡m thá»i gian báº±ng cÃ¡ch dá»«ng sá»›m cÃ¡c trials khÃ´ng há»©a háº¹n
- Táº­p trung resources vÃ o cÃ¡c trials cÃ³ tiá»m nÄƒng

---

### 1.5 HIá»‚U PHÆ¯Æ NG PHÃP ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG CÃC MODEL

#### 1.5.1 CÃ¡c Metrics Ä‘Æ°á»£c sá»­ dá»¥ng

**1. RÂ² (Coefficient of Determination)**

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum_{i}(y_i - \hat{y}_i)^2}{\sum_{i}(y_i - \bar{y})^2}$$

| GiÃ¡ trá»‹ | Ã nghÄ©a |
|---------|---------|
| RÂ² = 1 | Dá»± bÃ¡o hoÃ n háº£o |
| RÂ² = 0 | Model = dá»± bÃ¡o báº±ng mean |
| RÂ² < 0 | Model tá»‡ hÆ¡n dá»± bÃ¡o báº±ng mean |

**ÄÃ¡nh giÃ¡:**
- RÂ² > 0.9: Excellent
- RÂ² > 0.7: Good
- RÂ² > 0.5: Moderate
- RÂ² < 0.5: Poor

---

**2. NSE (Nash-Sutcliffe Efficiency)**

$$NSE = 1 - \frac{\sum_{i}(y_i - \hat{y}_i)^2}{\sum_{i}(y_i - \bar{y})^2}$$

| GiÃ¡ trá»‹ | Ã nghÄ©a |
|---------|---------|
| NSE > 0.75 | Very Good |
| NSE > 0.65 | Good |
| NSE > 0.50 | Satisfactory |
| NSE < 0.50 | Unsatisfactory |

*Note: NSE vÃ  RÂ² cÃ³ cÃ´ng thá»©c tÆ°Æ¡ng tá»±, nhÆ°ng NSE thÆ°á»ng dÃ¹ng trong hydrology vÃ  time series.*

---

**3. MAE (Mean Absolute Error)**

$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

- ÄÆ¡n vá»‹: Giá»‘ng vá»›i target (vehicles/hour)
- Dá»… hiá»ƒu: "Trung bÃ¬nh, model sai bao nhiÃªu?"
- KhÃ´ng pháº¡t náº·ng outliers

---

**4. RMSE (Root Mean Squared Error)**

$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$$

- ÄÆ¡n vá»‹: Giá»‘ng vá»›i target (vehicles/hour)
- Pháº¡t náº·ng cÃ¡c errors lá»›n (do bÃ¬nh phÆ°Æ¡ng)
- RMSE â‰¥ MAE (báº±ng khi táº¥t cáº£ errors báº±ng nhau)

---

#### 1.5.2 ÄÃ¡nh giÃ¡ Per-Step (Multi-Step Forecasting)

```python
def calculate_metrics_per_step(y_true, y_pred):
    """
    y_true, y_pred: shape (n_samples, 5)
    
    TÃ­nh metrics cho tá»«ng step: t+1, t+2, t+3, t+4, t+5
    """
    results = []
    for i in range(5):  # 5 steps
        metrics = {
            'Step': f't+{i+1}',
            'R2': r2_score(y_true[:, i], y_pred[:, i]),
            'NSE': calculate_nse(y_true[:, i], y_pred[:, i]),
            'MAE': mean_absolute_error(y_true[:, i], y_pred[:, i]),
            'RMSE': calculate_rmse(y_true[:, i], y_pred[:, i])
        }
        results.append(metrics)
    
    # Average across all steps
    avg_metrics = calculate_metrics(y_true.flatten(), y_pred.flatten())
    results.append({'Step': 'Average', **avg_metrics})
    
    return results
```

#### 1.5.3 Káº¿t quáº£ thá»±c táº¿ cá»§a Model

| Step | RÂ² | NSE | MAE | RMSE |
|------|-----|-----|-----|------|
| t+1 | 0.9841 | 0.9841 | 178.9 | 249.6 |
| t+2 | 0.9783 | 0.9783 | 200.4 | 291.7 |
| t+3 | 0.9720 | 0.9720 | 211.8 | 331.0 |
| t+4 | 0.9665 | 0.9665 | 220.7 | 362.3 |
| t+5 | 0.9615 | 0.9615 | 233.7 | 388.8 |
| **Average** | **0.9725** | **0.9725** | **209.1** | **328.4** |

**Nháº­n xÃ©t:**
- RÂ² > 0.96 cho táº¥t cáº£ steps â†’ Excellent
- Error tÄƒng dáº§n theo horizon (t+1 chÃ­nh xÃ¡c nháº¥t, t+5 kÃ©m nháº¥t) â†’ Expected behavior
- Model dá»± bÃ¡o tá»‘t cáº£ 5 steps

#### 1.5.4 Inverse Transform trÆ°á»›c khi Ä‘Ã¡nh giÃ¡

```python
# Predictions Ä‘ang á»Ÿ scaled space [0, 1]
# Cáº§n inverse transform vá» original space Ä‘á»ƒ metrics cÃ³ Ã½ nghÄ©a

def inverse_transform_predictions(y_scaled, scaler, target_idx):
    """
    y_scaled: (n_samples, 5) - scaled predictions
    scaler: fitted MinMaxScaler
    target_idx: index cá»§a traffic_volume trong feature list
    """
    n_samples, n_steps = y_scaled.shape
    n_features = scaler.n_features_in_
    
    y_original = np.zeros_like(y_scaled)
    
    for i in range(n_steps):
        # Táº¡o dummy array vá»›i Ä‘Ãºng sá»‘ features
        dummy = np.zeros((n_samples, n_features))
        dummy[:, target_idx] = y_scaled[:, i]
        
        # Inverse transform vÃ  láº¥y cá»™t target
        y_original[:, i] = scaler.inverse_transform(dummy)[:, target_idx]
    
    return y_original
```

---

## PHáº¦N 2: CÃ‚U Há»I LÃ THUYáº¾T

---

### CÃ¢u 1: Dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘á»ƒ tÃ­nh tráº¡ng thÃ¡i áº©n $h_t$ trong RNN

**Trong máº¡ng nÆ¡-ron há»“i tiáº¿p (RNN), dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘á»ƒ tÃ­nh tráº¡ng thÃ¡i áº©n $h_t$ táº¡i node thá»© $t$ gá»“m:**

1. **Input hiá»‡n táº¡i $x_t$:** Vector Ä‘áº·c trÆ°ng táº¡i thá»i Ä‘iá»ƒm $t$
2. **Tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ $h_{t-1}$:** ThÃ´ng tin tá»« cÃ¡c timesteps trÆ°á»›c

**CÃ´ng thá»©c:**

$$h_t = \tanh(W_{xh} \cdot x_t + W_{hh} \cdot h_{t-1} + b_h)$$

Trong Ä‘Ã³:
- $W_{xh}$: Ma tráº­n trá»ng sá»‘ tá»« input Ä‘áº¿n hidden
- $W_{hh}$: Ma tráº­n trá»ng sá»‘ tá»« hidden Ä‘áº¿n hidden (recurrent)
- $b_h$: Bias
- $\tanh$: HÃ m kÃ­ch hoáº¡t

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
x_t â”€â”€â”€â–ºâ”‚         â”‚
        â”‚  CELL   â”œâ”€â”€â”€â–º h_t
h_{t-1}â–ºâ”‚         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### CÃ¢u 2: CÃ¡c cá»•ng trong máº¡ng GRU

**Máº¡ng GRU (Gated Recurrent Unit) cÃ³ 2 cá»•ng:**

#### 1. Update Gate ($z_t$) - Cá»•ng cáº­p nháº­t

$$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **bao nhiÃªu thÃ´ng tin tá»« quÃ¡ khá»©** ($h_{t-1}$) Ä‘Æ°á»£c giá»¯ láº¡i
- Quyáº¿t Ä‘á»‹nh **bao nhiÃªu thÃ´ng tin má»›i** ($\tilde{h}_t$) Ä‘Æ°á»£c thÃªm vÃ o
- $z_t$ gáº§n 1: Giá»¯ nhiá»u thÃ´ng tin cÅ© (long-term memory)
- $z_t$ gáº§n 0: Cáº­p nháº­t nhiá»u thÃ´ng tin má»›i

#### 2. Reset Gate ($r_t$) - Cá»•ng reset

$$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **bao nhiÃªu thÃ´ng tin quÃ¡ khá»© cáº§n "quÃªn"** khi tÃ­nh candidate hidden state
- $r_t$ gáº§n 0: "QuÃªn" nhiá»u thÃ´ng tin cÅ©
- $r_t$ gáº§n 1: Giá»¯ thÃ´ng tin cÅ© Ä‘á»ƒ tÃ­nh state má»›i

#### CÃ´ng thá»©c tÃ­nh hidden state:

$$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)$$

$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GRU CELL                             â”‚
â”‚                                                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚    â”‚ Reset   â”‚                    â”‚ Update  â”‚               â”‚
â”‚    â”‚ Gate r_tâ”‚                    â”‚ Gate z_tâ”‚               â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                              â”‚                     â”‚
â”‚         â–¼                              â”‚                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚                     â”‚
â”‚    â”‚Candidateâ”‚                         â”‚                     â”‚
â”‚    â”‚  hÌƒ_t    â”‚                         â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â”‚                     â”‚
â”‚         â”‚                              â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º(Ã—)â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                      â”‚                                       â”‚
â”‚                      â–¼                                       â”‚
â”‚                    h_t                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### CÃ¢u 3: CÃ¡c cá»•ng trong máº¡ng LSTM

**Máº¡ng LSTM (Long Short-Term Memory) cÃ³ 3 cá»•ng:**

#### 1. Forget Gate ($f_t$) - Cá»•ng quÃªn

$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **thÃ´ng tin nÃ o tá»« cell state cÅ© $C_{t-1}$ cáº§n Ä‘Æ°á»£c loáº¡i bá»**
- $f_t$ gáº§n 0: QuÃªn thÃ´ng tin
- $f_t$ gáº§n 1: Giá»¯ thÃ´ng tin

#### 2. Input Gate ($i_t$) - Cá»•ng Ä‘áº§u vÃ o

$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$

$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

**TÃ¡c dá»¥ng:**
- $i_t$: Quyáº¿t Ä‘á»‹nh **thÃ´ng tin má»›i nÃ o sáº½ Ä‘Æ°á»£c lÆ°u vÃ o cell state**
- $\tilde{C}_t$: Candidate values cÃ³ thá»ƒ thÃªm vÃ o cell state
- Káº¿t há»£p: $i_t \odot \tilde{C}_t$ = thÃ´ng tin má»›i thá»±c sá»± Ä‘Æ°á»£c thÃªm

#### 3. Output Gate ($o_t$) - Cá»•ng Ä‘áº§u ra

$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **pháº§n nÃ o cá»§a cell state Ä‘Æ°á»£c xuáº¥t ra** lÃ m hidden state
- Lá»c thÃ´ng tin tá»« cell state Ä‘á»ƒ táº¡o output

#### CÃ´ng thá»©c cáº­p nháº­t Cell State vÃ  Hidden State:

$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

$$h_t = o_t \odot \tanh(C_t)$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            LSTM CELL                                 â”‚
â”‚                                                                      â”‚
â”‚  C_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º(Ã—)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º(+)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º C_t         â”‚
â”‚                     â–²               â–²                                â”‚
â”‚                     â”‚               â”‚                                â”‚
â”‚                 â”Œâ”€â”€â”€â”´â”€â”€â”€â”       â”Œâ”€â”€â”€â”´â”€â”€â”€â”                           â”‚
â”‚                 â”‚Forget â”‚       â”‚ Input â”‚                            â”‚
â”‚                 â”‚Gate f â”‚       â”‚Gate i â”‚                            â”‚
â”‚                 â”‚  t    â”‚       â”‚  t    â”‚                            â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜                           â”‚
â”‚                                     â”‚                                â”‚
â”‚                                 â”Œâ”€â”€â”€â”´â”€â”€â”€â”                           â”‚
â”‚                                 â”‚ CÌƒ_t   â”‚                           â”‚
â”‚                                 â”‚(tanh) â”‚                           â”‚
â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                      â”‚
â”‚  h_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–º h_t        â”‚
â”‚                                                    â”‚                 â”‚
â”‚                                                â”Œâ”€â”€â”€â”´â”€â”€â”€â”            â”‚
â”‚                                                â”‚Output â”‚            â”‚
â”‚                                                â”‚Gate o â”‚            â”‚
â”‚                                                â”‚  t    â”‚            â”‚
â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### CÃ¢u 4: TÃ­nh tráº¡ng thÃ¡i áº©n $h_t$ trong Bidirectional RNN

**Trong máº¡ng Bidirectional RNN, tráº¡ng thÃ¡i áº©n $h_t$ Ä‘Æ°á»£c tÃ­nh tá»« 2 hÆ°á»›ng:**

#### Forward Direction (Thuáº­n):
Xá»­ lÃ½ sequence tá»« $t=1$ Ä‘áº¿n $t=T$

$$\overrightarrow{h_t} = f(\overrightarrow{W_{xh}} \cdot x_t + \overrightarrow{W_{hh}} \cdot \overrightarrow{h_{t-1}} + \overrightarrow{b_h})$$

#### Backward Direction (Nghá»‹ch):
Xá»­ lÃ½ sequence tá»« $t=T$ Ä‘áº¿n $t=1$

$$\overleftarrow{h_t} = f(\overleftarrow{W_{xh}} \cdot x_t + \overleftarrow{W_{hh}} \cdot \overleftarrow{h_{t+1}} + \overleftarrow{b_h})$$

#### Káº¿t há»£p (Concatenation):

$$h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]$$

hoáº·c:

$$h_t = \overrightarrow{h_t} + \overleftarrow{h_t} \quad \text{(sum)}$$

$$h_t = (\overrightarrow{h_t} + \overleftarrow{h_t}) / 2 \quad \text{(average)}$$

```
Forward:    x_1 â”€â”€â–º h_1 â”€â”€â–º h_2 â”€â”€â–º h_3 â”€â”€â–º ... â”€â”€â–º h_T
                    â†“       â†“       â†“               â†“
Combine:           [;]     [;]     [;]             [;]
                    â†‘       â†‘       â†‘               â†‘
Backward:   x_1 â—„â”€â”€ h_1 â—„â”€â”€ h_2 â—„â”€â”€ h_3 â—„â”€â”€ ... â—„â”€â”€ h_T
```

**Æ¯u Ä‘iá»ƒm:**
- Hidden state táº¡i $t$ chá»©a thÃ´ng tin tá»« **cáº£ quÃ¡ khá»© vÃ  tÆ°Æ¡ng lai**
- Há»¯u Ã­ch cho cÃ¡c task cáº§n context Ä‘áº§y Ä‘á»§ (NER, POS tagging, machine translation encoder)

---

### CÃ¢u 5: Vai trÃ² cá»§a Encoder vÃ  Decoder trong Seq2Seq

#### ENCODER

**Vai trÃ²:**
1. **Äá»c vÃ  hiá»ƒu** toÃ n bá»™ input sequence
2. **NÃ©n thÃ´ng tin** thÃ nh má»™t vector ngá»¯ cáº£nh cá»‘ Ä‘á»‹nh (context vector)
3. **TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng** quan trá»ng tá»« input

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- Nháº­n input sequence: $X = (x_1, x_2, ..., x_T)$
- Xá»­ lÃ½ tuáº§n tá»± qua cÃ¡c RNN/LSTM cells
- Output: Hidden state cuá»‘i cÃ¹ng (hoáº·c táº¥t cáº£ hidden states náº¿u dÃ¹ng Attention)

```
INPUT: "I love machine learning"

        x_1      x_2       x_3          x_4
         â”‚        â”‚         â”‚            â”‚
         â–¼        â–¼         â–¼            â–¼
      â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
      â”‚ h_1 â”œâ”€â–ºâ”‚ h_2 â”œâ”€â”€â–ºâ”‚ h_3 â”œâ”€â”€â”€â”€â–ºâ”‚ h_4 â”œâ”€â”€â–º Context Vector
      â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜
        "I"    "love"  "machine"   "learning"
```

#### DECODER

**Vai trÃ²:**
1. **Nháº­n context vector** tá»« Encoder
2. **Sinh ra output sequence** tá»«ng pháº§n tá»­ má»™t
3. **Dá»‹ch/Chuyá»ƒn Ä‘á»•i** thÃ´ng tin tá»« context thÃ nh output mong muá»‘n

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- Khá»Ÿi táº¡o hidden state tá»« context vector cá»§a Encoder
- Sinh output tá»«ng bÆ°á»›c: $y_1, y_2, ..., y_{T'}$
- Má»—i bÆ°á»›c: Input = output cá»§a bÆ°á»›c trÆ°á»›c (hoáº·c ground truth náº¿u teacher forcing)

```
Context Vector
      â”‚
      â–¼
   â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
   â”‚ s_1 â”œâ”€â”€â”€â”€â–ºâ”‚ s_2 â”œâ”€â”€â”€â”€â–ºâ”‚ s_3 â”œâ”€â”€â”€â”€â–ºâ”‚ s_4 â”‚
   â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜
      â”‚           â”‚           â”‚           â”‚
      â–¼           â–¼           â–¼           â–¼
    "TÃ´i"      "yÃªu"       "há»c"       "mÃ¡y"

OUTPUT: "TÃ´i yÃªu há»c mÃ¡y"
```

**Tá»•ng káº¿t:**

| Component | Input | Output | Vai trÃ² |
|-----------|-------|--------|---------|
| **Encoder** | Source sequence $(x_1, ..., x_T)$ | Context vector | NÃ©n thÃ´ng tin input |
| **Decoder** | Context + previous outputs | Target sequence $(y_1, ..., y_{T'})$ | Sinh output tuáº§n tá»± |

---

### CÃ¢u 6: Key, Value, Query trong Attention (Encoder-Decoder)

**Trong cÆ¡ cháº¿ Attention giá»¯a Encoder-Decoder:**

#### Query (Q) - Truy váº¥n
- **Nguá»“n:** Hidden state cá»§a **Decoder** táº¡i bÆ°á»›c hiá»‡n táº¡i: $Q = s_t$
- **Ã nghÄ©a:** "TÃ´i (decoder) Ä‘ang á»Ÿ tráº¡ng thÃ¡i nÃ y, cáº§n thÃ´ng tin gÃ¬ tá»« encoder?"
- **Vai trÃ²:** Äáº¡i diá»‡n cho "cÃ¢u há»i" cáº§n tÃ¬m thÃ´ng tin liÃªn quan

#### Key (K) - KhÃ³a
- **Nguá»“n:** Táº¥t cáº£ hidden states cá»§a **Encoder**: $K = (h_1, h_2, ..., h_T)$
- **Ã nghÄ©a:** "ÄÃ¢y lÃ  cÃ¡c keys Ä‘á»ƒ so sÃ¡nh vá»›i query"
- **Vai trÃ²:** DÃ¹ng Ä‘á»ƒ tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá»›i Query

#### Value (V) - GiÃ¡ trá»‹
- **Nguá»“n:** Táº¥t cáº£ hidden states cá»§a **Encoder**: $V = (h_1, h_2, ..., h_T)$
- **Ã nghÄ©a:** "ÄÃ¢y lÃ  thÃ´ng tin thá»±c sá»± sáº½ Ä‘Æ°á»£c láº¥y"
- **Vai trÃ²:** ThÃ´ng tin Ä‘Æ°á»£c trÃ­ch xuáº¥t dá»±a trÃªn attention weights

**LÆ°u Ã½:** Trong Encoder-Decoder Attention cÆ¡ báº£n, $K = V$ (Ä‘á»u lÃ  encoder hidden states)

#### CÃ´ng thá»©c tÃ­nh Attention:

**1. TÃ­nh attention scores (alignment):**

$$e_{t,i} = score(s_t, h_i)$$

CÃ¡c hÃ m score phá»• biáº¿n:
- **Dot product:** $e_{t,i} = s_t^T \cdot h_i$
- **General:** $e_{t,i} = s_t^T \cdot W_a \cdot h_i$
- **Concat (Bahdanau):** $e_{t,i} = v_a^T \cdot \tanh(W_a \cdot [s_t; h_i])$

**2. Softmax Ä‘á»ƒ Ä‘Æ°á»£c attention weights:**

$$\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T}\exp(e_{t,j})}$$

**3. TÃ­nh context vector:**

$$c_t = \sum_{i=1}^{T} \alpha_{t,i} \cdot h_i$$

```
                    Encoder Hidden States (K, V)
                 h_1      h_2      h_3      h_4
                  â”‚        â”‚        â”‚        â”‚
                  â”‚        â”‚        â”‚        â”‚
   Query â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   (s_t)          â”‚        â”‚        â”‚        â”‚
                  â–¼        â–¼        â–¼        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
              â”‚Î±_{t,1}â”‚ â”‚Î±_{t,2}â”‚ â”‚Î±_{t,3}â”‚ â”‚Î±_{t,4}â”‚  Attention Weights
              â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜
                 â”‚        â”‚        â”‚        â”‚
                 â–¼        â–¼        â–¼        â–¼
              h_1Ã—Î±    h_2Ã—Î±    h_3Ã—Î±    h_4Ã—Î±
                 â”‚        â”‚        â”‚        â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    Context Vector (c_t)
```

---

### CÃ¢u 7: Key, Value, Query trong Self-Attention

**Trong Self-Attention, Q, K, V Ä‘á»u Ä‘Æ°á»£c táº¡o tá»« CÃ™NG Má»˜T input:**

Cho input sequence $X = (x_1, x_2, ..., x_n)$

#### Query, Key, Value Ä‘Æ°á»£c tÃ­nh:

$$Q = X \cdot W^Q$$
$$K = X \cdot W^K$$
$$V = X \cdot W^V$$

Trong Ä‘Ã³:
- $W^Q, W^K, W^V$ lÃ  cÃ¡c ma tráº­n trá»ng sá»‘ há»c Ä‘Æ°á»£c
- $X$: Input embeddings, shape $(n, d_{model})$
- $Q, K, V$: shape $(n, d_k)$ hoáº·c $(n, d_v)$

**Äiá»ƒm khÃ¡c biá»‡t vá»›i Encoder-Decoder Attention:**

| | Self-Attention | Encoder-Decoder Attention |
|--|---------------|---------------------------|
| **Q nguá»“n** | Input sequence | Decoder hidden state |
| **K nguá»“n** | Input sequence (same as Q) | Encoder hidden states |
| **V nguá»“n** | Input sequence (same as Q, K) | Encoder hidden states |
| **Má»¥c Ä‘Ã­ch** | Há»c má»‘i quan há»‡ giá»¯a cÃ¡c pháº§n tá»­ trong cÃ¹ng sequence | Há»c alignment giá»¯a input vÃ  output |

#### CÃ´ng thá»©c Self-Attention (Scaled Dot-Product):

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

```
Input: "The cat sat on the mat"
        â†“
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚Embeddingâ”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼            â–¼            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   W^Q  â”‚   â”‚   W^K  â”‚   â”‚   W^V  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
        â”‚            â”‚            â”‚
        â–¼            â–¼            â–¼
        Q            K            V
        â”‚            â”‚            â”‚
        â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”´            â”‚
        â–¼     â–¼                   â”‚
      Q Ã— K^T                     â”‚
        â”‚                         â”‚
        â–¼                         â”‚
   Ã· âˆšd_k                         â”‚
        â”‚                         â”‚
        â–¼                         â”‚
    Softmax                       â”‚
        â”‚                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–º Ã— V
                                  â”‚
                                  â–¼
                             Output (Attention)
```

---

### CÃ¢u 8: Ã tÆ°á»Ÿng cá»§a Multi-Head Attention

**Ã tÆ°á»Ÿng chÃ­nh:**

Thay vÃ¬ thá»±c hiá»‡n **má»™t phÃ©p attention** vá»›i $d_{model}$ dimensions, chia thÃ nh **nhiá»u "heads"** thá»±c hiá»‡n attention song song trÃªn cÃ¡c khÃ´ng gian con khÃ¡c nhau.

#### Táº¡i sao cáº§n Multi-Head?

1. **Capture nhiá»u loáº¡i relationships:**
   - Head 1: CÃ³ thá»ƒ há»c syntactic relationships
   - Head 2: CÃ³ thá»ƒ há»c semantic relationships
   - Head 3: CÃ³ thá»ƒ há»c positional relationships
   - ...

2. **Attention á»Ÿ nhiá»u positions khÃ¡c nhau:**
   - Má»™t head cÃ³ thá»ƒ focus vÃ o tá»« gáº§n
   - Head khÃ¡c cÃ³ thá»ƒ focus vÃ o tá»« xa

3. **Richer representation:**
   - Káº¿t há»£p nhiá»u perspectives

#### CÃ´ng thá»©c Multi-Head Attention:

**1. Táº¡o multiple heads:**

Vá»›i má»—i head $i$ (tá»« 1 Ä‘áº¿n $h$):

$$Q_i = Q \cdot W_i^Q, \quad K_i = K \cdot W_i^K, \quad V_i = V \cdot W_i^V$$

Trong Ä‘Ã³:
- $W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$
- $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$
- $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$
- ThÆ°á»ng: $d_k = d_v = d_{model} / h$

**2. TÃ­nh attention cho má»—i head:**

$$\text{head}_i = \text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i$$

**3. Concatenate táº¥t cáº£ heads:**

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) \cdot W^O$$

Trong Ä‘Ã³ $W^O \in \mathbb{R}^{hd_v \times d_{model}}$

```
                    Input (Q, K, V)
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼               â–¼               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Head 1  â”‚    â”‚ Head 2  â”‚ ... â”‚ Head h  â”‚
     â”‚ W_1^Q   â”‚    â”‚ W_2^Q   â”‚     â”‚ W_h^Q   â”‚
     â”‚ W_1^K   â”‚    â”‚ W_2^K   â”‚     â”‚ W_h^K   â”‚
     â”‚ W_1^V   â”‚    â”‚ W_2^V   â”‚     â”‚ W_h^V   â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚              â”‚               â”‚
          â–¼              â–¼               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚Attentionâ”‚    â”‚Attentionâ”‚     â”‚Attentionâ”‚
     â”‚ head_1  â”‚    â”‚ head_2  â”‚ ... â”‚ head_h  â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚              â”‚               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Concat     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   W^O      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  MultiHead Output
```

#### VÃ­ dá»¥ vá»›i Transformer (h=8 heads):

| Parameter | GiÃ¡ trá»‹ |
|-----------|---------|
| $d_{model}$ | 512 |
| $h$ (sá»‘ heads) | 8 |
| $d_k = d_v$ | 512/8 = 64 |

- Má»—i head cÃ³ dimension 64
- 8 heads cháº¡y song song
- Concat: 8 Ã— 64 = 512
- Project qua $W^O$: 512 â†’ 512

---

## TÃ“M Táº®T

### Pháº§n Code:
1. **Dá»¯ liá»‡u:** 9 thuá»™c tÃ­nh gá»‘c + 22 features sau engineering â†’ X_train (24 timesteps Ã— 22 features), y_train (5 timesteps Ã— 1 target)
2. **Tiá»n xá»­ lÃ½:** Handle duplicates (aggregation), outliers (IQR), segment splitting (2,588 gaps â†’ 113 segments)
3. **Kiáº¿n trÃºc:** Bidirectional LSTM Encoder + LSTM Decoder vá»›i Teacher Forcing
4. **ÄÃ¡nh giÃ¡:** RÂ², NSE, MAE, RMSE per step vÃ  average

### Pháº§n LÃ½ thuyáº¿t:
1. **RNN:** $h_t = f(x_t, h_{t-1})$
2. **GRU:** 2 cá»•ng (Update, Reset)
3. **LSTM:** 3 cá»•ng (Forget, Input, Output) + Cell State
4. **BiRNN:** Forward + Backward hidden states
5. **Encoder-Decoder:** Encoder nÃ©n input â†’ Context â†’ Decoder sinh output
6. **Attention (Enc-Dec):** Q=decoder state, K=V=encoder states
7. **Self-Attention:** Q, K, V tá»« cÃ¹ng input vá»›i learned projections
8. **Multi-Head:** h heads parallel attention â†’ concat â†’ project

---

*Document created for Traffic Volume Forecasting Project - Deep Learning Course*
