# ğŸ“š TÃ€I LIá»†U Há»I ÄÃP - TRAFFIC VOLUME FORECASTING PROJECT

## PHáº¦N 1: CÃ‚U Há»I LIÃŠN QUAN Äáº¾N CODE

---

### 1.1 HIá»‚U Dá»® LIá»†U ÄÆ¯á»¢C Sá»¬ Dá»¤NG TRONG PROJECT

#### 1.1.1 Giá»›i thiá»‡u Dataset

**Dataset:** Metro Interstate Traffic Volume  
**Nguá»“n:** UCI Machine Learning Repository  
**Má»¥c tiÃªu:** Dá»± bÃ¡o lÆ°u lÆ°á»£ng giao thÃ´ng (`traffic_volume`) cho 5 giá» tiáº¿p theo

#### 1.1.2 CÃ¡c thuá»™c tÃ­nh trong dá»¯ liá»‡u gá»‘c

| STT | Thuá»™c tÃ­nh | Kiá»ƒu dá»¯ liá»‡u | MÃ´ táº£ | Vai trÃ² |
|-----|------------|--------------|-------|---------|
| 1 | `date_time` | DateTime | Thá»i Ä‘iá»ƒm ghi nháº­n (theo giá») | Index thá»i gian |
| 2 | `holiday` | Categorical | TÃªn ngÃ y lá»… hoáº·c None | Feature |
| 3 | `temp` | Numerical | Nhiá»‡t Ä‘á»™ (Kelvin) | Feature |
| 4 | `rain_1h` | Numerical | LÆ°á»£ng mÆ°a trong 1 giá» (mm) | Feature |
| 5 | `snow_1h` | Numerical | LÆ°á»£ng tuyáº¿t trong 1 giá» (mm) | Feature |
| 6 | `clouds_all` | Numerical | Pháº§n trÄƒm mÃ¢y che phá»§ (%) | Feature |
| 7 | `weather_main` | Categorical | Thá»i tiáº¿t chÃ­nh (Clear, Rain, Clouds...) | Feature |
| 8 | `weather_description` | Categorical | MÃ´ táº£ chi tiáº¿t thá»i tiáº¿t | Feature |
| 9 | **`traffic_volume`** | **Numerical** | **LÆ°u lÆ°á»£ng xe/giá»** | **TARGET** |

#### 1.1.3 Features Ä‘Æ°á»£c táº¡o thÃªm (Feature Engineering)

Sau quÃ¡ trÃ¬nh Feature Engineering, cÃ¡c features má»›i Ä‘Æ°á»£c táº¡o:

**A. Temporal Features (tá»« date_time):**
| Feature | MÃ´ táº£ | VÃ­ dá»¥ |
|---------|-------|-------|
| `hour` | Giá» trong ngÃ y | 0-23 |
| `day_of_week` | NgÃ y trong tuáº§n | 0=Monday, 6=Sunday |
| `day_of_month` | NgÃ y trong thÃ¡ng | 1-31 |
| `month` | ThÃ¡ng | 1-12 |
| `year` | NÄƒm | 2012-2018 |
| `week_of_year` | Tuáº§n trong nÄƒm | 1-52 |
| `quarter` | QuÃ½ | 1-4 |
| `season` | MÃ¹a | 0=Spring, 1=Summer, 2=Fall, 3=Winter |
| `is_weekend` | Cuá»‘i tuáº§n? | 0 hoáº·c 1 |
| `is_rush_hour` | Giá» cao Ä‘iá»ƒm? (7-9h, 16-18h) | 0 hoáº·c 1 |

**B. Cyclical Features (mÃ£ hÃ³a tuáº§n hoÃ n):**
| Feature | CÃ´ng thá»©c | Má»¥c Ä‘Ã­ch |
|---------|-----------|----------|
| `hour_sin` | sin(2Ï€ Ã— hour/24) | MÃ£ hÃ³a giá» dáº¡ng vÃ²ng trÃ²n |
| `hour_cos` | cos(2Ï€ Ã— hour/24) | Giá» 23 gáº§n giá» 0 |
| `day_sin` | sin(2Ï€ Ã— day_of_week/7) | MÃ£ hÃ³a ngÃ y trong tuáº§n |
| `day_cos` | cos(2Ï€ Ã— day_of_week/7) | Chá»§ nháº­t gáº§n thá»© 2 |
| `month_sin` | sin(2Ï€ Ã— month/12) | MÃ£ hÃ³a thÃ¡ng |
| `month_cos` | cos(2Ï€ Ã— month/12) | ThÃ¡ng 12 gáº§n thÃ¡ng 1 |

**C. Weather Features:**
| Feature | MÃ´ táº£ |
|---------|-------|
| `temp_celsius` | Nhiá»‡t Ä‘á»™ (Â°C) = temp - 273.15 |
| `is_rainy` | CÃ³ mÆ°a? (rain_1h > 0) |
| `is_snowy` | CÃ³ tuyáº¿t? (snow_1h > 0) |

**D. Interaction Features:**
| Feature | MÃ´ táº£ |
|---------|-------|
| `rush_rain` | is_rush_hour Ã— is_rainy |

#### 1.1.4 Thuá»™c tÃ­nh Ä‘Æ°á»£c sá»­ dá»¥ng cho X_train vÃ  y_train

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INPUT SEQUENCE (X)                            â”‚
â”‚                        Shape: (n_samples, 24, 22)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  22 Features (táº¡i má»—i timestep):                                â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  1. traffic_volume (target cÅ©ng lÃ  feature cho input)           â”‚   â”‚
â”‚  â”‚  2. temp, temp_celsius                                          â”‚   â”‚
â”‚  â”‚  3. clouds_all, rain_1h, snow_1h                                â”‚   â”‚
â”‚  â”‚  4. hour, day_of_week, day_of_month, month, year                â”‚   â”‚
â”‚  â”‚  5. week_of_year, quarter, season                               â”‚   â”‚
â”‚  â”‚  6. is_weekend, is_rush_hour                                    â”‚   â”‚
â”‚  â”‚  7. hour_sin, hour_cos, day_sin, day_cos, month_sin, month_cos  â”‚   â”‚
â”‚  â”‚  8. is_rainy, is_snowy                                          â”‚   â”‚
â”‚  â”‚  9. rush_rain                                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  Sequence length: 24 timesteps (24 giá» lá»‹ch sá»­)                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          OUTPUT SEQUENCE (y)                            â”‚
â”‚                          Shape: (n_samples, 5)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Chá»‰ cÃ³ 1 thuá»™c tÃ­nh: traffic_volume                                   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  y[0] = traffic_volume táº¡i t+1                                  â”‚   â”‚
â”‚  â”‚  y[1] = traffic_volume táº¡i t+2                                  â”‚   â”‚
â”‚  â”‚  y[2] = traffic_volume táº¡i t+3                                  â”‚   â”‚
â”‚  â”‚  y[3] = traffic_volume táº¡i t+4                                  â”‚   â”‚
â”‚  â”‚  y[4] = traffic_volume táº¡i t+5                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  Dá»± bÃ¡o 5 giá» tiáº¿p theo                                                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**TÃ³m táº¯t:**
- **X_train:** Sá»­ dá»¥ng **22 features** (bao gá»“m cáº£ traffic_volume) trong **24 timesteps**
- **y_train:** Chá»‰ sá»­ dá»¥ng **traffic_volume** cho **5 timesteps tiáº¿p theo**

#### 1.1.5 LÃ½ do khÃ´ng dÃ¹ng Lag/Rolling/Diff Features

```
âŒ KHÃ”NG Sá»¬ Dá»¤NG:
   - traffic_lag_1h, traffic_lag_24h (lag features)
   - rolling_mean_24h, rolling_std_6h (rolling statistics)
   - diff_1h, pct_change (difference features)

âœ… LÃ DO:
   1. LSTM tá»± há»c temporal patterns tá»« sequence input
   2. Input Ä‘Ã£ cÃ³ 24 giá» lá»‹ch sá»­ â†’ model "tháº¥y" Ä‘Æ°á»£c lag information
   3. ThÃªm lag features thá»§ cÃ´ng â†’ data leakage khi táº¡o sequences
```

---

### 1.2 HIá»‚U CÃCH TIá»€N Xá»¬ LÃ Dá»® LIá»†U

#### 1.2.1 Pipeline Tiá»n Xá»­ LÃ½

```
Raw Data (48,204 rows)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Convert DateTime             â”‚
â”‚  - Parse date_time â†’ datetime object  â”‚
â”‚  - Sort by timestamp                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Handle Duplicates            â”‚
â”‚  - 7,629 duplicate timestamps found   â”‚
â”‚  - Aggregation strategy:              â”‚
â”‚    â€¢ temp, clouds_all â†’ mean          â”‚
â”‚    â€¢ rain_1h, snow_1h â†’ max           â”‚
â”‚    â€¢ traffic_volume â†’ mean            â”‚
â”‚    â€¢ weather_main â†’ combine unique    â”‚
â”‚    â€¢ holiday â†’ any non-null           â”‚
â”‚  - Result: 40,575 rows                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Handle Missing Values        â”‚
â”‚  - Numerical: interpolate (linear)    â”‚
â”‚  - Categorical: forward fill          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Handle Outliers (IQR)        â”‚
â”‚  - Column: traffic_volume             â”‚
â”‚  - Method: IQR vá»›i factor = 1.5       â”‚
â”‚  - Q1 = 25th percentile               â”‚
â”‚  - Q3 = 75th percentile               â”‚
â”‚  - IQR = Q3 - Q1                      â”‚
â”‚  - Lower = Q1 - 1.5 Ã— IQR             â”‚
â”‚  - Upper = Q3 + 1.5 Ã— IQR             â”‚
â”‚  - Action: Clip values to [Lower,Upper]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Check Time Continuity        â”‚
â”‚  - KHÃ”NG resample/interpolate target  â”‚
â”‚  - Giá»¯ nguyÃªn data tháº­t               â”‚
â”‚  - Xá»­ lÃ½ gaps trong Data Preparation  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
Cleaned Data (40,575 rows)
```

#### 1.2.2 Code Xá»­ LÃ½ Duplicates (Chi tiáº¿t)

```python
# Thay vÃ¬ chá»‰ drop duplicates, aggregation Ä‘á»ƒ giá»¯ thÃ´ng tin
df = df.groupby("date_time", as_index=False).agg({
    "temp": "mean",           # Trung bÃ¬nh nhiá»‡t Ä‘á»™
    "rain_1h": "max",         # Náº¿u cÃ³ mÆ°a á»Ÿ báº¥t ká»³ row nÃ o â†’ giá»¯
    "snow_1h": "max",         # TÆ°Æ¡ng tá»± cho tuyáº¿t
    "clouds_all": "mean",     # Trung bÃ¬nh % mÃ¢y
    "traffic_volume": "mean", # Trung bÃ¬nh lÆ°u lÆ°á»£ng
    "weather_main": lambda x: ",".join(sorted(set(x))),  # Gá»™p unique
    "holiday": lambda x: 0 if x.isna().all() else 1      # Binary
})
```

#### 1.2.3 Code Xá»­ LÃ½ Outliers (IQR Method)

```python
def handle_outliers_iqr(df, column, factor=1.5, method='clip'):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower = Q1 - factor * IQR
    upper = Q3 + factor * IQR
    
    # Clip values ngoÃ i khoáº£ng [lower, upper]
    df[column] = df[column].clip(lower=lower, upper=upper)
    
    return df
```

#### 1.2.4 Segment-Based Data Preparation (QUAN TRá»ŒNG!)

**Váº¥n Ä‘á»:** Dá»¯ liá»‡u cÃ³ 2,588 timestamp gaps (khÃ´ng liÃªn tá»¥c)

```
Timeline thá»±c táº¿:
... 10:00 â†’ 11:00 â†’ 12:00 â†’ [GAP] â†’ 15:00 â†’ 16:00 ...
                              â†‘
                    Thiáº¿u 13:00, 14:00
```

**Giáº£i phÃ¡p:** TÃ¡ch data thÃ nh cÃ¡c segments liÃªn tá»¥c

```python
def split_continuous_segments(df, date_col, target_col, min_length=48, freq_hours=1):
    """
    TÃ¡ch DataFrame thÃ nh cÃ¡c segment THá»°C Sá»° liÃªn tá»¥c:
    1. KhÃ´ng cÃ³ NaN trong target
    2. KhÃ´ng cÃ³ gap trong timestamps (má»—i row cÃ¡ch nhau Ä‘Ãºng freq_hours)
    """
    # TÃ­nh time difference giá»¯a cÃ¡c rows liÃªn tiáº¿p
    time_diff = df[date_col].diff()
    expected_diff = pd.Timedelta(hours=freq_hours)
    
    # PhÃ¡t hiá»‡n gap: time_diff > expected_diff
    is_gap = (time_diff > expected_diff + tolerance) | time_diff.isna()
    
    # Táº¡o segment ID: tÄƒng má»—i khi gáº·p gap
    segment_id = is_gap.cumsum()
    
    # Chá»‰ giá»¯ segments Ä‘á»§ dÃ i (>= min_length)
    # ...
```

**Káº¿t quáº£:**
```
Original rows:     40,575
Gaps detected:      2,588
Segments created:     113
Usable rows:       30,871 (76.1%)
```

#### 1.2.5 Scaling Data

```python
from sklearn.preprocessing import MinMaxScaler

# QUAN TRá»ŒNG: Chá»‰ fit scaler trÃªn TRAINING data
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit + Transform
X_val_scaled = scaler.transform(X_val)          # Chá»‰ Transform
X_test_scaled = scaler.transform(X_test)        # Chá»‰ Transform

# LÃ½ do: TrÃ¡nh data leakage tá»« validation/test vÃ o training
```

#### 1.2.6 Train/Validation/Test Split

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TIMELINE-BASED SPLIT                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â—„â”€â”€â”€â”€â”€â”€â”€ TRAIN (70%) â”€â”€â”€â”€â”€â”€â”€â–ºâ—„â”€ VAL (15%) â”€â–ºâ—„â”€ TEST (15%) â”€â–º  â”‚
â”‚                                                                  â”‚
â”‚  [Past data]                                  [Future data]      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âŒ KHÃ”NG shuffle vÃ¬:
   - Time series pháº£i giá»¯ thá»© tá»± thá»i gian
   - Train trÃªn quÃ¡ khá»©, predict tÆ°Æ¡ng lai
   - Shuffle â†’ data leakage tá»« future
```

---

### 1.3 HIá»‚U THIáº¾T Káº¾ KIáº¾N TRÃšC Cá»¦A CÃC Máº NG ÄÆ¯á»¢C Sá»¬ Dá»¤NG

#### 1.3.1 Tá»•ng quan kiáº¿n trÃºc Encoder-Decoder (Seq2Seq)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ENCODER-DECODER ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  INPUT: (batch_size, 24, 22)                                           â”‚
â”‚  - batch_size: sá»‘ samples                                              â”‚
â”‚  - 24: sequence length (24 giá» lá»‹ch sá»­)                                â”‚
â”‚  - 22: sá»‘ features                                                     â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                      ENCODER                                   â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚  Bidirectional LSTM                                      â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - input_size: 22 (sá»‘ features)                          â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - hidden_size: 64/128/256                               â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - num_layers: 2                                         â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - bidirectional: True                                   â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - dropout: 0.2-0.3                                      â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                           â”‚                                    â”‚     â”‚
â”‚  â”‚                           â–¼                                    â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚  Linear Projection (if bidirectional)                   â”‚  â”‚     â”‚
â”‚  â”‚  â”‚  - hidden_size * 2 â†’ hidden_size                        â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                           â”‚                                    â”‚     â”‚
â”‚  â”‚                           â–¼                                    â”‚     â”‚
â”‚  â”‚  Output: (hidden_state, cell_state)                           â”‚     â”‚
â”‚  â”‚  - hidden: (num_layers, batch, hidden_size)                   â”‚     â”‚
â”‚  â”‚  - cell: (num_layers, batch, hidden_size)                     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â”‚ Context Vector                           â”‚
â”‚                              â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                      DECODER                                   â”‚     â”‚
â”‚  â”‚                                                                â”‚     â”‚
â”‚  â”‚  for t in range(5):  # 5 output steps                         â”‚     â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚      â”‚  LSTM Cell                                           â”‚  â”‚     â”‚
â”‚  â”‚      â”‚  - input: previous_output (or teacher forcing)       â”‚  â”‚     â”‚
â”‚  â”‚      â”‚  - hidden_state, cell_state from encoder/prev step   â”‚  â”‚     â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                           â”‚                                    â”‚     â”‚
â”‚  â”‚                           â–¼                                    â”‚     â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚      â”‚  Fully Connected Layer                               â”‚  â”‚     â”‚
â”‚  â”‚      â”‚  - hidden_size â†’ 1                                   â”‚  â”‚     â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                           â”‚                                    â”‚     â”‚
â”‚  â”‚                           â–¼                                    â”‚     â”‚
â”‚  â”‚      prediction[t] = output                                   â”‚     â”‚
â”‚  â”‚                                                                â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                         â”‚
â”‚  OUTPUT: (batch_size, 5)                                               â”‚
â”‚  - 5 giÃ¡ trá»‹ traffic_volume dá»± bÃ¡o cho t+1, t+2, t+3, t+4, t+5        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.3.2 Chi tiáº¿t Encoder

```python
class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, 
                 dropout=0.2, bidirectional=True):
        super(Encoder, self).__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_size,      # 22 features
            hidden_size=hidden_size,    # 128
            num_layers=num_layers,      # 2 layers stacked
            batch_first=True,           # Input: (batch, seq, features)
            dropout=dropout,            # Dropout between layers
            bidirectional=bidirectional # Äá»c cáº£ 2 chiá»u
        )
        
        # Project bidirectional output back to hidden_size
        if bidirectional:
            self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)
            self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)
    
    def forward(self, x):
        # x: (batch, 24, 22)
        outputs, (hidden, cell) = self.lstm(x)
        # outputs: (batch, 24, hidden_size * 2) if bidirectional
        # hidden: (num_layers * 2, batch, hidden_size)
        
        if self.bidirectional:
            # Combine forward and backward states
            hidden = self.fc_hidden(...)  # â†’ (num_layers, batch, hidden_size)
            cell = self.fc_cell(...)
        
        return outputs, (hidden, cell)
```

#### 1.3.3 Chi tiáº¿t Decoder

```python
class Decoder(nn.Module):
    def __init__(self, output_size=1, hidden_size=128, num_layers=2, dropout=0.2):
        super(Decoder, self).__init__()
        
        self.lstm = nn.LSTM(
            input_size=output_size,     # 1 (previous prediction)
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout
        )
        
        self.fc = nn.Linear(hidden_size, output_size)  # â†’ 1
    
    def forward(self, x, hidden, cell):
        # x: (batch, 1, 1) - previous output
        output, (hidden, cell) = self.lstm(x, (hidden, cell))
        prediction = self.fc(output)  # (batch, 1, 1)
        
        return prediction, (hidden, cell)
```

#### 1.3.4 Teacher Forcing

```python
def forward(self, x, target=None, teacher_forcing_ratio=0.5):
    # Encode
    _, (hidden, cell) = self.encoder(x)
    
    # Initial decoder input: last known traffic_volume
    decoder_input = x[:, -1, 0].unsqueeze(1).unsqueeze(2)  # (batch, 1, 1)
    
    outputs = []
    for t in range(5):  # 5 prediction steps
        prediction, (hidden, cell) = self.decoder(decoder_input, hidden, cell)
        outputs.append(prediction)
        
        # Teacher Forcing: dÃ¹ng ground truth vá»›i xÃ¡c suáº¥t teacher_forcing_ratio
        if target is not None and random.random() < teacher_forcing_ratio:
            decoder_input = target[:, t].unsqueeze(1).unsqueeze(2)  # Use true value
        else:
            decoder_input = prediction  # Use predicted value
    
    return torch.cat(outputs, dim=1)  # (batch, 5)
```

**TÃ¡c dá»¥ng cá»§a Teacher Forcing:**
- Trong training: ÄÃ´i khi dÃ¹ng ground truth thay vÃ¬ prediction lÃ m input cho step tiáº¿p theo
- GiÃºp model há»c nhanh hÆ¡n, á»•n Ä‘á»‹nh hÆ¡n
- Tá»· lá»‡ giáº£m dáº§n theo epoch: `current_tf_ratio = tf_ratio * (1 - epoch/num_epochs)`

#### 1.3.5 Táº¡i sao dÃ¹ng Bidirectional Encoder?

```
Forward LSTM:  t=0 â†’ t=1 â†’ t=2 â†’ ... â†’ t=23
                                         â†“
                              forward_hidden_state
                              
Backward LSTM: t=0 â† t=1 â† t=2 â† ... â† t=23
                â†“
     backward_hidden_state

Combined: concat(forward, backward) â†’ richer representation
```

**Lá»£i Ã­ch:**
- Encoder "nhÃ¬n" Ä‘Æ°á»£c cáº£ quÃ¡ khá»© vÃ  tÆ°Æ¡ng lai trong input sequence
- Capture patterns tá»« cáº£ 2 chiá»u (VD: traffic tÄƒng trÆ°á»›c giá» cao Ä‘iá»ƒm, giáº£m sau)

---

### 1.4 HIá»‚U PHÆ¯Æ NG PHÃP ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG CÃC MODEL

#### 1.4.1 CÃ¡c Metrics Ä‘Æ°á»£c sá»­ dá»¥ng

**1. RÂ² (Coefficient of Determination)**

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum_{i}(y_i - \hat{y}_i)^2}{\sum_{i}(y_i - \bar{y})^2}$$

| GiÃ¡ trá»‹ | Ã nghÄ©a |
|---------|---------|
| RÂ² = 1 | Dá»± bÃ¡o hoÃ n háº£o |
| RÂ² = 0 | Model = dá»± bÃ¡o báº±ng mean |
| RÂ² < 0 | Model tá»‡ hÆ¡n dá»± bÃ¡o báº±ng mean |

**ÄÃ¡nh giÃ¡:**
- RÂ² > 0.9: Excellent
- RÂ² > 0.7: Good
- RÂ² > 0.5: Moderate
- RÂ² < 0.5: Poor

---

**2. NSE (Nash-Sutcliffe Efficiency)**

$$NSE = 1 - \frac{\sum_{i}(y_i - \hat{y}_i)^2}{\sum_{i}(y_i - \bar{y})^2}$$

| GiÃ¡ trá»‹ | Ã nghÄ©a |
|---------|---------|
| NSE > 0.75 | Very Good |
| NSE > 0.65 | Good |
| NSE > 0.50 | Satisfactory |
| NSE < 0.50 | Unsatisfactory |

*Note: NSE vÃ  RÂ² cÃ³ cÃ´ng thá»©c tÆ°Æ¡ng tá»±, nhÆ°ng NSE thÆ°á»ng dÃ¹ng trong hydrology vÃ  time series.*

---

**3. MAE (Mean Absolute Error)**

$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

- ÄÆ¡n vá»‹: Giá»‘ng vá»›i target (vehicles/hour)
- Dá»… hiá»ƒu: "Trung bÃ¬nh, model sai bao nhiÃªu?"
- KhÃ´ng pháº¡t náº·ng outliers

---

**4. RMSE (Root Mean Squared Error)**

$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$$

- ÄÆ¡n vá»‹: Giá»‘ng vá»›i target (vehicles/hour)
- Pháº¡t náº·ng cÃ¡c errors lá»›n (do bÃ¬nh phÆ°Æ¡ng)
- RMSE â‰¥ MAE (báº±ng khi táº¥t cáº£ errors báº±ng nhau)

---

#### 1.4.2 ÄÃ¡nh giÃ¡ Per-Step (Multi-Step Forecasting)

```python
def calculate_metrics_per_step(y_true, y_pred):
    """
    y_true, y_pred: shape (n_samples, 5)
    
    TÃ­nh metrics cho tá»«ng step: t+1, t+2, t+3, t+4, t+5
    """
    results = []
    for i in range(5):  # 5 steps
        metrics = {
            'Step': f't+{i+1}',
            'R2': r2_score(y_true[:, i], y_pred[:, i]),
            'NSE': calculate_nse(y_true[:, i], y_pred[:, i]),
            'MAE': mean_absolute_error(y_true[:, i], y_pred[:, i]),
            'RMSE': calculate_rmse(y_true[:, i], y_pred[:, i])
        }
        results.append(metrics)
    
    # Average across all steps
    avg_metrics = calculate_metrics(y_true.flatten(), y_pred.flatten())
    results.append({'Step': 'Average', **avg_metrics})
    
    return results
```

#### 1.4.3 Káº¿t quáº£ thá»±c táº¿ cá»§a Model

| Step | RÂ² | NSE | MAE | RMSE |
|------|-----|-----|-----|------|
| t+1 | 0.9841 | 0.9841 | 178.9 | 249.6 |
| t+2 | 0.9783 | 0.9783 | 200.4 | 291.7 |
| t+3 | 0.9720 | 0.9720 | 211.8 | 331.0 |
| t+4 | 0.9665 | 0.9665 | 220.7 | 362.3 |
| t+5 | 0.9615 | 0.9615 | 233.7 | 388.8 |
| **Average** | **0.9725** | **0.9725** | **209.1** | **328.4** |

**Nháº­n xÃ©t:**
- RÂ² > 0.96 cho táº¥t cáº£ steps â†’ Excellent
- Error tÄƒng dáº§n theo horizon (t+1 chÃ­nh xÃ¡c nháº¥t, t+5 kÃ©m nháº¥t) â†’ Expected behavior
- Model dá»± bÃ¡o tá»‘t cáº£ 5 steps

#### 1.4.4 Inverse Transform trÆ°á»›c khi Ä‘Ã¡nh giÃ¡

```python
# Predictions Ä‘ang á»Ÿ scaled space [0, 1]
# Cáº§n inverse transform vá» original space Ä‘á»ƒ metrics cÃ³ Ã½ nghÄ©a

def inverse_transform_predictions(y_scaled, scaler, target_idx):
    """
    y_scaled: (n_samples, 5) - scaled predictions
    scaler: fitted MinMaxScaler
    target_idx: index cá»§a traffic_volume trong feature list
    """
    n_samples, n_steps = y_scaled.shape
    n_features = scaler.n_features_in_
    
    y_original = np.zeros_like(y_scaled)
    
    for i in range(n_steps):
        # Táº¡o dummy array vá»›i Ä‘Ãºng sá»‘ features
        dummy = np.zeros((n_samples, n_features))
        dummy[:, target_idx] = y_scaled[:, i]
        
        # Inverse transform vÃ  láº¥y cá»™t target
        y_original[:, i] = scaler.inverse_transform(dummy)[:, target_idx]
    
    return y_original
```

---

## PHáº¦N 2: CÃ‚U Há»I LÃ THUYáº¾T

---

### CÃ¢u 1: Dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘á»ƒ tÃ­nh tráº¡ng thÃ¡i áº©n $h_t$ trong RNN

**Trong máº¡ng nÆ¡-ron há»“i tiáº¿p (RNN), dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘á»ƒ tÃ­nh tráº¡ng thÃ¡i áº©n $h_t$ táº¡i node thá»© $t$ gá»“m:**

1. **Input hiá»‡n táº¡i $x_t$:** Vector Ä‘áº·c trÆ°ng táº¡i thá»i Ä‘iá»ƒm $t$
2. **Tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ $h_{t-1}$:** ThÃ´ng tin tá»« cÃ¡c timesteps trÆ°á»›c

**CÃ´ng thá»©c:**

$$h_t = \tanh(W_{xh} \cdot x_t + W_{hh} \cdot h_{t-1} + b_h)$$

Trong Ä‘Ã³:
- $W_{xh}$: Ma tráº­n trá»ng sá»‘ tá»« input Ä‘áº¿n hidden
- $W_{hh}$: Ma tráº­n trá»ng sá»‘ tá»« hidden Ä‘áº¿n hidden (recurrent)
- $b_h$: Bias
- $\tanh$: HÃ m kÃ­ch hoáº¡t

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
x_t â”€â”€â”€â–ºâ”‚         â”‚
        â”‚  CELL   â”œâ”€â”€â”€â–º h_t
h_{t-1}â–ºâ”‚         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### CÃ¢u 2: CÃ¡c cá»•ng trong máº¡ng GRU

**Máº¡ng GRU (Gated Recurrent Unit) cÃ³ 2 cá»•ng:**

#### 1. Update Gate ($z_t$) - Cá»•ng cáº­p nháº­t

$$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **bao nhiÃªu thÃ´ng tin tá»« quÃ¡ khá»©** ($h_{t-1}$) Ä‘Æ°á»£c giá»¯ láº¡i
- Quyáº¿t Ä‘á»‹nh **bao nhiÃªu thÃ´ng tin má»›i** ($\tilde{h}_t$) Ä‘Æ°á»£c thÃªm vÃ o
- $z_t$ gáº§n 1: Giá»¯ nhiá»u thÃ´ng tin cÅ© (long-term memory)
- $z_t$ gáº§n 0: Cáº­p nháº­t nhiá»u thÃ´ng tin má»›i

#### 2. Reset Gate ($r_t$) - Cá»•ng reset

$$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **bao nhiÃªu thÃ´ng tin quÃ¡ khá»© cáº§n "quÃªn"** khi tÃ­nh candidate hidden state
- $r_t$ gáº§n 0: "QuÃªn" nhiá»u thÃ´ng tin cÅ©
- $r_t$ gáº§n 1: Giá»¯ thÃ´ng tin cÅ© Ä‘á»ƒ tÃ­nh state má»›i

#### CÃ´ng thá»©c tÃ­nh hidden state:

$$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)$$

$$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GRU CELL                             â”‚
â”‚                                                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚    â”‚ Reset   â”‚                    â”‚ Update  â”‚               â”‚
â”‚    â”‚ Gate r_tâ”‚                    â”‚ Gate z_tâ”‚               â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                              â”‚                     â”‚
â”‚         â–¼                              â”‚                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚                     â”‚
â”‚    â”‚Candidateâ”‚                         â”‚                     â”‚
â”‚    â”‚  hÌƒ_t    â”‚                         â”‚                     â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â”‚                     â”‚
â”‚         â”‚                              â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º(Ã—)â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                      â”‚                                       â”‚
â”‚                      â–¼                                       â”‚
â”‚                    h_t                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### CÃ¢u 3: CÃ¡c cá»•ng trong máº¡ng LSTM

**Máº¡ng LSTM (Long Short-Term Memory) cÃ³ 3 cá»•ng:**

#### 1. Forget Gate ($f_t$) - Cá»•ng quÃªn

$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **thÃ´ng tin nÃ o tá»« cell state cÅ© $C_{t-1}$ cáº§n Ä‘Æ°á»£c loáº¡i bá»**
- $f_t$ gáº§n 0: QuÃªn thÃ´ng tin
- $f_t$ gáº§n 1: Giá»¯ thÃ´ng tin

#### 2. Input Gate ($i_t$) - Cá»•ng Ä‘áº§u vÃ o

$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$

$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

**TÃ¡c dá»¥ng:**
- $i_t$: Quyáº¿t Ä‘á»‹nh **thÃ´ng tin má»›i nÃ o sáº½ Ä‘Æ°á»£c lÆ°u vÃ o cell state**
- $\tilde{C}_t$: Candidate values cÃ³ thá»ƒ thÃªm vÃ o cell state
- Káº¿t há»£p: $i_t \odot \tilde{C}_t$ = thÃ´ng tin má»›i thá»±c sá»± Ä‘Æ°á»£c thÃªm

#### 3. Output Gate ($o_t$) - Cá»•ng Ä‘áº§u ra

$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$

**TÃ¡c dá»¥ng:**
- Quyáº¿t Ä‘á»‹nh **pháº§n nÃ o cá»§a cell state Ä‘Æ°á»£c xuáº¥t ra** lÃ m hidden state
- Lá»c thÃ´ng tin tá»« cell state Ä‘á»ƒ táº¡o output

#### CÃ´ng thá»©c cáº­p nháº­t Cell State vÃ  Hidden State:

$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

$$h_t = o_t \odot \tanh(C_t)$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            LSTM CELL                                 â”‚
â”‚                                                                      â”‚
â”‚  C_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º(Ã—)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º(+)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º C_t         â”‚
â”‚                     â–²               â–²                                â”‚
â”‚                     â”‚               â”‚                                â”‚
â”‚                 â”Œâ”€â”€â”€â”´â”€â”€â”€â”       â”Œâ”€â”€â”€â”´â”€â”€â”€â”                           â”‚
â”‚                 â”‚Forget â”‚       â”‚ Input â”‚                            â”‚
â”‚                 â”‚Gate f â”‚       â”‚Gate i â”‚                            â”‚
â”‚                 â”‚  t    â”‚       â”‚  t    â”‚                            â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜                           â”‚
â”‚                                     â”‚                                â”‚
â”‚                                 â”Œâ”€â”€â”€â”´â”€â”€â”€â”                           â”‚
â”‚                                 â”‚ CÌƒ_t   â”‚                           â”‚
â”‚                                 â”‚(tanh) â”‚                           â”‚
â”‚                                 â””â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                      â”‚
â”‚  h_{t-1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–º h_t        â”‚
â”‚                                                    â”‚                 â”‚
â”‚                                                â”Œâ”€â”€â”€â”´â”€â”€â”€â”            â”‚
â”‚                                                â”‚Output â”‚            â”‚
â”‚                                                â”‚Gate o â”‚            â”‚
â”‚                                                â”‚  t    â”‚            â”‚
â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### CÃ¢u 4: TÃ­nh tráº¡ng thÃ¡i áº©n $h_t$ trong Bidirectional RNN

**Trong máº¡ng Bidirectional RNN, tráº¡ng thÃ¡i áº©n $h_t$ Ä‘Æ°á»£c tÃ­nh tá»« 2 hÆ°á»›ng:**

#### Forward Direction (Thuáº­n):
Xá»­ lÃ½ sequence tá»« $t=1$ Ä‘áº¿n $t=T$

$$\overrightarrow{h_t} = f(\overrightarrow{W_{xh}} \cdot x_t + \overrightarrow{W_{hh}} \cdot \overrightarrow{h_{t-1}} + \overrightarrow{b_h})$$

#### Backward Direction (Nghá»‹ch):
Xá»­ lÃ½ sequence tá»« $t=T$ Ä‘áº¿n $t=1$

$$\overleftarrow{h_t} = f(\overleftarrow{W_{xh}} \cdot x_t + \overleftarrow{W_{hh}} \cdot \overleftarrow{h_{t+1}} + \overleftarrow{b_h})$$

#### Káº¿t há»£p (Concatenation):

$$h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]$$

hoáº·c:

$$h_t = \overrightarrow{h_t} + \overleftarrow{h_t} \quad \text{(sum)}$$

$$h_t = (\overrightarrow{h_t} + \overleftarrow{h_t}) / 2 \quad \text{(average)}$$

```
Forward:    x_1 â”€â”€â–º h_1 â”€â”€â–º h_2 â”€â”€â–º h_3 â”€â”€â–º ... â”€â”€â–º h_T
                    â†“       â†“       â†“               â†“
Combine:           [;]     [;]     [;]             [;]
                    â†‘       â†‘       â†‘               â†‘
Backward:   x_1 â—„â”€â”€ h_1 â—„â”€â”€ h_2 â—„â”€â”€ h_3 â—„â”€â”€ ... â—„â”€â”€ h_T
```

**Æ¯u Ä‘iá»ƒm:**
- Hidden state táº¡i $t$ chá»©a thÃ´ng tin tá»« **cáº£ quÃ¡ khá»© vÃ  tÆ°Æ¡ng lai**
- Há»¯u Ã­ch cho cÃ¡c task cáº§n context Ä‘áº§y Ä‘á»§ (NER, POS tagging, machine translation encoder)

---

### CÃ¢u 5: Vai trÃ² cá»§a Encoder vÃ  Decoder trong Seq2Seq

#### ENCODER

**Vai trÃ²:**
1. **Äá»c vÃ  hiá»ƒu** toÃ n bá»™ input sequence
2. **NÃ©n thÃ´ng tin** thÃ nh má»™t vector ngá»¯ cáº£nh cá»‘ Ä‘á»‹nh (context vector)
3. **TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng** quan trá»ng tá»« input

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- Nháº­n input sequence: $X = (x_1, x_2, ..., x_T)$
- Xá»­ lÃ½ tuáº§n tá»± qua cÃ¡c RNN/LSTM cells
- Output: Hidden state cuá»‘i cÃ¹ng (hoáº·c táº¥t cáº£ hidden states náº¿u dÃ¹ng Attention)

```
INPUT: "I love machine learning"

        x_1      x_2       x_3          x_4
         â”‚        â”‚         â”‚            â”‚
         â–¼        â–¼         â–¼            â–¼
      â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
      â”‚ h_1 â”œâ”€â–ºâ”‚ h_2 â”œâ”€â”€â–ºâ”‚ h_3 â”œâ”€â”€â”€â”€â–ºâ”‚ h_4 â”œâ”€â”€â–º Context Vector
      â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜
        "I"    "love"  "machine"   "learning"
```

#### DECODER

**Vai trÃ²:**
1. **Nháº­n context vector** tá»« Encoder
2. **Sinh ra output sequence** tá»«ng pháº§n tá»­ má»™t
3. **Dá»‹ch/Chuyá»ƒn Ä‘á»•i** thÃ´ng tin tá»« context thÃ nh output mong muá»‘n

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
- Khá»Ÿi táº¡o hidden state tá»« context vector cá»§a Encoder
- Sinh output tá»«ng bÆ°á»›c: $y_1, y_2, ..., y_{T'}$
- Má»—i bÆ°á»›c: Input = output cá»§a bÆ°á»›c trÆ°á»›c (hoáº·c ground truth náº¿u teacher forcing)

```
Context Vector
      â”‚
      â–¼
   â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
   â”‚ s_1 â”œâ”€â”€â”€â”€â–ºâ”‚ s_2 â”œâ”€â”€â”€â”€â–ºâ”‚ s_3 â”œâ”€â”€â”€â”€â–ºâ”‚ s_4 â”‚
   â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜     â””â”€â”€â”¬â”€â”€â”˜
      â”‚           â”‚           â”‚           â”‚
      â–¼           â–¼           â–¼           â–¼
    "TÃ´i"      "yÃªu"       "há»c"       "mÃ¡y"

OUTPUT: "TÃ´i yÃªu há»c mÃ¡y"
```

**Tá»•ng káº¿t:**

| Component | Input | Output | Vai trÃ² |
|-----------|-------|--------|---------|
| **Encoder** | Source sequence $(x_1, ..., x_T)$ | Context vector | NÃ©n thÃ´ng tin input |
| **Decoder** | Context + previous outputs | Target sequence $(y_1, ..., y_{T'})$ | Sinh output tuáº§n tá»± |

---

### CÃ¢u 6: Key, Value, Query trong Attention (Encoder-Decoder)

**Trong cÆ¡ cháº¿ Attention giá»¯a Encoder-Decoder:**

#### Query (Q) - Truy váº¥n
- **Nguá»“n:** Hidden state cá»§a **Decoder** táº¡i bÆ°á»›c hiá»‡n táº¡i: $Q = s_t$
- **Ã nghÄ©a:** "TÃ´i (decoder) Ä‘ang á»Ÿ tráº¡ng thÃ¡i nÃ y, cáº§n thÃ´ng tin gÃ¬ tá»« encoder?"
- **Vai trÃ²:** Äáº¡i diá»‡n cho "cÃ¢u há»i" cáº§n tÃ¬m thÃ´ng tin liÃªn quan

#### Key (K) - KhÃ³a
- **Nguá»“n:** Táº¥t cáº£ hidden states cá»§a **Encoder**: $K = (h_1, h_2, ..., h_T)$
- **Ã nghÄ©a:** "ÄÃ¢y lÃ  cÃ¡c keys Ä‘á»ƒ so sÃ¡nh vá»›i query"
- **Vai trÃ²:** DÃ¹ng Ä‘á»ƒ tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá»›i Query

#### Value (V) - GiÃ¡ trá»‹
- **Nguá»“n:** Táº¥t cáº£ hidden states cá»§a **Encoder**: $V = (h_1, h_2, ..., h_T)$
- **Ã nghÄ©a:** "ÄÃ¢y lÃ  thÃ´ng tin thá»±c sá»± sáº½ Ä‘Æ°á»£c láº¥y"
- **Vai trÃ²:** ThÃ´ng tin Ä‘Æ°á»£c trÃ­ch xuáº¥t dá»±a trÃªn attention weights

**LÆ°u Ã½:** Trong Encoder-Decoder Attention cÆ¡ báº£n, $K = V$ (Ä‘á»u lÃ  encoder hidden states)

#### CÃ´ng thá»©c tÃ­nh Attention:

**1. TÃ­nh attention scores (alignment):**

$$e_{t,i} = score(s_t, h_i)$$

CÃ¡c hÃ m score phá»• biáº¿n:
- **Dot product:** $e_{t,i} = s_t^T \cdot h_i$
- **General:** $e_{t,i} = s_t^T \cdot W_a \cdot h_i$
- **Concat (Bahdanau):** $e_{t,i} = v_a^T \cdot \tanh(W_a \cdot [s_t; h_i])$

**2. Softmax Ä‘á»ƒ Ä‘Æ°á»£c attention weights:**

$$\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T}\exp(e_{t,j})}$$

**3. TÃ­nh context vector:**

$$c_t = \sum_{i=1}^{T} \alpha_{t,i} \cdot h_i$$

```
                    Encoder Hidden States (K, V)
                 h_1      h_2      h_3      h_4
                  â”‚        â”‚        â”‚        â”‚
                  â”‚        â”‚        â”‚        â”‚
   Query â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   (s_t)          â”‚        â”‚        â”‚        â”‚
                  â–¼        â–¼        â–¼        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
              â”‚Î±_{t,1}â”‚ â”‚Î±_{t,2}â”‚ â”‚Î±_{t,3}â”‚ â”‚Î±_{t,4}â”‚  Attention Weights
              â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜
                 â”‚        â”‚        â”‚        â”‚
                 â–¼        â–¼        â–¼        â–¼
              h_1Ã—Î±    h_2Ã—Î±    h_3Ã—Î±    h_4Ã—Î±
                 â”‚        â”‚        â”‚        â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    Context Vector (c_t)
```

---

### CÃ¢u 7: Key, Value, Query trong Self-Attention

**Trong Self-Attention, Q, K, V Ä‘á»u Ä‘Æ°á»£c táº¡o tá»« CÃ™NG Má»˜T input:**

Cho input sequence $X = (x_1, x_2, ..., x_n)$

#### Query, Key, Value Ä‘Æ°á»£c tÃ­nh:

$$Q = X \cdot W^Q$$
$$K = X \cdot W^K$$
$$V = X \cdot W^V$$

Trong Ä‘Ã³:
- $W^Q, W^K, W^V$ lÃ  cÃ¡c ma tráº­n trá»ng sá»‘ há»c Ä‘Æ°á»£c
- $X$: Input embeddings, shape $(n, d_{model})$
- $Q, K, V$: shape $(n, d_k)$ hoáº·c $(n, d_v)$

**Äiá»ƒm khÃ¡c biá»‡t vá»›i Encoder-Decoder Attention:**

| | Self-Attention | Encoder-Decoder Attention |
|--|---------------|---------------------------|
| **Q nguá»“n** | Input sequence | Decoder hidden state |
| **K nguá»“n** | Input sequence (same as Q) | Encoder hidden states |
| **V nguá»“n** | Input sequence (same as Q, K) | Encoder hidden states |
| **Má»¥c Ä‘Ã­ch** | Há»c má»‘i quan há»‡ giá»¯a cÃ¡c pháº§n tá»­ trong cÃ¹ng sequence | Há»c alignment giá»¯a input vÃ  output |

#### CÃ´ng thá»©c Self-Attention (Scaled Dot-Product):

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

```
Input: "The cat sat on the mat"
        â†“
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚Embeddingâ”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼            â–¼            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   W^Q  â”‚   â”‚   W^K  â”‚   â”‚   W^V  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
        â”‚            â”‚            â”‚
        â–¼            â–¼            â–¼
        Q            K            V
        â”‚            â”‚            â”‚
        â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”´            â”‚
        â–¼     â–¼                   â”‚
      Q Ã— K^T                     â”‚
        â”‚                         â”‚
        â–¼                         â”‚
   Ã· âˆšd_k                         â”‚
        â”‚                         â”‚
        â–¼                         â”‚
    Softmax                       â”‚
        â”‚                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–º Ã— V
                                  â”‚
                                  â–¼
                             Output (Attention)
```

---

### CÃ¢u 8: Ã tÆ°á»Ÿng cá»§a Multi-Head Attention

**Ã tÆ°á»Ÿng chÃ­nh:**

Thay vÃ¬ thá»±c hiá»‡n **má»™t phÃ©p attention** vá»›i $d_{model}$ dimensions, chia thÃ nh **nhiá»u "heads"** thá»±c hiá»‡n attention song song trÃªn cÃ¡c khÃ´ng gian con khÃ¡c nhau.

#### Táº¡i sao cáº§n Multi-Head?

1. **Capture nhiá»u loáº¡i relationships:**
   - Head 1: CÃ³ thá»ƒ há»c syntactic relationships
   - Head 2: CÃ³ thá»ƒ há»c semantic relationships
   - Head 3: CÃ³ thá»ƒ há»c positional relationships
   - ...

2. **Attention á»Ÿ nhiá»u positions khÃ¡c nhau:**
   - Má»™t head cÃ³ thá»ƒ focus vÃ o tá»« gáº§n
   - Head khÃ¡c cÃ³ thá»ƒ focus vÃ o tá»« xa

3. **Richer representation:**
   - Káº¿t há»£p nhiá»u perspectives

#### CÃ´ng thá»©c Multi-Head Attention:

**1. Táº¡o multiple heads:**

Vá»›i má»—i head $i$ (tá»« 1 Ä‘áº¿n $h$):

$$Q_i = Q \cdot W_i^Q, \quad K_i = K \cdot W_i^K, \quad V_i = V \cdot W_i^V$$

Trong Ä‘Ã³:
- $W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$
- $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$
- $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$
- ThÆ°á»ng: $d_k = d_v = d_{model} / h$

**2. TÃ­nh attention cho má»—i head:**

$$\text{head}_i = \text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i$$

**3. Concatenate táº¥t cáº£ heads:**

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) \cdot W^O$$

Trong Ä‘Ã³ $W^O \in \mathbb{R}^{hd_v \times d_{model}}$

```
                    Input (Q, K, V)
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼               â–¼               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Head 1  â”‚    â”‚ Head 2  â”‚ ... â”‚ Head h  â”‚
     â”‚ W_1^Q   â”‚    â”‚ W_2^Q   â”‚     â”‚ W_h^Q   â”‚
     â”‚ W_1^K   â”‚    â”‚ W_2^K   â”‚     â”‚ W_h^K   â”‚
     â”‚ W_1^V   â”‚    â”‚ W_2^V   â”‚     â”‚ W_h^V   â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚              â”‚               â”‚
          â–¼              â–¼               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚Attentionâ”‚    â”‚Attentionâ”‚     â”‚Attentionâ”‚
     â”‚ head_1  â”‚    â”‚ head_2  â”‚ ... â”‚ head_h  â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚              â”‚               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Concat     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   W^O      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  MultiHead Output
```

#### VÃ­ dá»¥ vá»›i Transformer (h=8 heads):

| Parameter | GiÃ¡ trá»‹ |
|-----------|---------|
| $d_{model}$ | 512 |
| $h$ (sá»‘ heads) | 8 |
| $d_k = d_v$ | 512/8 = 64 |

- Má»—i head cÃ³ dimension 64
- 8 heads cháº¡y song song
- Concat: 8 Ã— 64 = 512
- Project qua $W^O$: 512 â†’ 512

---

## TÃ“M Táº®T

### Pháº§n Code:
1. **Dá»¯ liá»‡u:** 9 thuá»™c tÃ­nh gá»‘c + 22 features sau engineering â†’ X_train (24 timesteps Ã— 22 features), y_train (5 timesteps Ã— 1 target)
2. **Tiá»n xá»­ lÃ½:** Handle duplicates (aggregation), outliers (IQR), segment splitting (2,588 gaps â†’ 113 segments)
3. **Kiáº¿n trÃºc:** Bidirectional LSTM Encoder + LSTM Decoder vá»›i Teacher Forcing
4. **ÄÃ¡nh giÃ¡:** RÂ², NSE, MAE, RMSE per step vÃ  average

### Pháº§n LÃ½ thuyáº¿t:
1. **RNN:** $h_t = f(x_t, h_{t-1})$
2. **GRU:** 2 cá»•ng (Update, Reset)
3. **LSTM:** 3 cá»•ng (Forget, Input, Output) + Cell State
4. **BiRNN:** Forward + Backward hidden states
5. **Encoder-Decoder:** Encoder nÃ©n input â†’ Context â†’ Decoder sinh output
6. **Attention (Enc-Dec):** Q=decoder state, K=V=encoder states
7. **Self-Attention:** Q, K, V tá»« cÃ¹ng input vá»›i learned projections
8. **Multi-Head:** h heads parallel attention â†’ concat â†’ project

---

*Document created for Traffic Volume Forecasting Project - Deep Learning Course*
