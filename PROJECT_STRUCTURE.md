# ğŸ“Š Project Structure: Traffic Volume Forecasting vá»›i Encoder-Decoder

## ğŸ¯ Má»¥c tiÃªu
Dá»± bÃ¡o `traffic_volume` cho **5 bÆ°á»›c thá»i gian tiáº¿p theo** sá»­ dá»¥ng mÃ´ hÃ¬nh **Encoder-Decoder (Seq2Seq)**

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
DeepLearning_final/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ Metro_Interstate_Traffic_Volume.csv    # Dá»¯ liá»‡u gá»‘c
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ cleaned_data.csv                       # Sau preprocessing
â”‚   â”‚   â”œâ”€â”€ featured_data.csv                      # Sau feature engineering
â”‚   â”‚   â””â”€â”€ selected_features.csv                  # Sau feature selection
â”‚   â””â”€â”€ sequences/
â”‚       â”œâ”€â”€ X_train.npy                            # Training sequences
â”‚       â”œâ”€â”€ y_train.npy
â”‚       â”œâ”€â”€ X_val.npy                              # Validation sequences
â”‚       â”œâ”€â”€ y_val.npy
â”‚       â”œâ”€â”€ X_test.npy                             # Test sequences
â”‚       â””â”€â”€ y_test.npy
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb                               # KhÃ¡m phÃ¡ & phÃ¢n tÃ­ch dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 02_Preprocessing.ipynb                     # Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 03_Feature_Engineering.ipynb               # Táº¡o features má»›i
â”‚   â”œâ”€â”€ 04_Feature_Selection.ipynb                 # Chá»n features quan trá»ng
â”‚   â”œâ”€â”€ 05_Data_Preparation.ipynb                  # Chuáº©n bá»‹ data cho model
â”‚   â”œâ”€â”€ 06_Model_Training.ipynb                    # Huáº¥n luyá»‡n Encoder-Decoder
â”‚   â””â”€â”€ 07_Evaluation.ipynb                        # ÄÃ¡nh giÃ¡ & visualization
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                                  # Hyperparameters & paths
â”‚   â”œâ”€â”€ data_preprocessing.py                      # Functions xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ feature_engineering.py                     # Functions táº¡o features
â”‚   â”œâ”€â”€ feature_selection.py                       # Functions chá»n features
â”‚   â”œâ”€â”€ dataset.py                                 # PyTorch Dataset class
â”‚   â”œâ”€â”€ model.py                                   # Encoder-Decoder architecture
â”‚   â”œâ”€â”€ train.py                                   # Training loop & callbacks
â”‚   â”œâ”€â”€ evaluate.py                                # Metrics: RÂ², NSE, MAE, RMSE
â”‚   â””â”€â”€ utils.py                                   # Utility functions
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ checkpoints/                               # Model checkpoints má»—i epoch
â”‚   â”œâ”€â”€ best_model.pth                             # Best model weights
â”‚   â””â”€â”€ scaler.pkl                                 # Saved scaler object
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â”œâ”€â”€ eda/                                   # EDA plots
â”‚   â”‚   â”œâ”€â”€ training/                              # Learning curves
â”‚   â”‚   â””â”€â”€ evaluation/                            # Prediction plots
â”‚   â”œâ”€â”€ metrics.json                               # Káº¿t quáº£ Ä‘Ã¡nh giÃ¡
â”‚   â””â”€â”€ predictions.csv                            # Predicted values
â”‚
â”œâ”€â”€ Deep/                                          # Virtual environment
â”œâ”€â”€ requirements.txt                               # Dependencies
â”œâ”€â”€ PROJECT_STRUCTURE.md                           # File nÃ y
â””â”€â”€ README.md                                      # HÆ°á»›ng dáº«n project
```

---

## ğŸ““ Chi tiáº¿t tá»«ng Notebook

### **01_EDA.ipynb** - Exploratory Data Analysis
```
Input:  data/raw/Metro_Interstate_Traffic_Volume.csv
Output: results/figures/eda/*.png
        Hiá»ƒu biáº¿t vá» dá»¯ liá»‡u

Tasks:
â”œâ”€â”€ Load & inspect data (shape, dtypes, head/tail)
â”œâ”€â”€ Basic statistics (describe, info)
â”œâ”€â”€ Missing values analysis
â”œâ”€â”€ Duplicates check
â”œâ”€â”€ Target distribution (traffic_volume)
â”œâ”€â”€ Time series visualization
â”œâ”€â”€ Correlation heatmap
â”œâ”€â”€ Feature distributions
â””â”€â”€ Insights & findings summary
```

### **02_Preprocessing.ipynb** - Data Preprocessing
```
Input:  data/raw/Metro_Interstate_Traffic_Volume.csv
Output: data/processed/cleaned_data.csv

Tasks:
â”œâ”€â”€ Handle DateTime (convert, sort, set index)
â”œâ”€â”€ Handle missing values
â”œâ”€â”€ Handle duplicates
â”œâ”€â”€ Handle outliers (IQR/Z-score)
â”œâ”€â”€ Resample to ensure hourly continuity
â”œâ”€â”€ Data validation
â””â”€â”€ Save cleaned data
```

### **03_Feature_Engineering.ipynb** - Create New Features
```
Input:  data/processed/cleaned_data.csv
Output: data/processed/featured_data.csv

Tasks:
â”œâ”€â”€ Temporal features (hour, day, month, year, is_weekend, is_rush_hour)
â”œâ”€â”€ Cyclical encoding (sin/cos for hour, day, month)
â”œâ”€â”€ Lag features (t-1, t-24, t-168)
â”œâ”€â”€ Rolling statistics (mean, std, min, max)
â”œâ”€â”€ Difference features (diff, pct_change)
â”œâ”€â”€ Holiday features
â”œâ”€â”€ Weather engineering (temp_celsius, is_rainy, etc.)
â”œâ”€â”€ Interaction features
â””â”€â”€ Save featured data
```

### **04_Feature_Selection.ipynb** - Select Best Features
```
Input:  data/processed/featured_data.csv
Output: data/processed/selected_features.csv
        Danh sÃ¡ch features Ä‘Æ°á»£c chá»n

Tasks:
â”œâ”€â”€ Correlation analysis vá»›i target
â”œâ”€â”€ Remove highly correlated features (>0.95)
â”œâ”€â”€ Feature importance (Random Forest)
â”œâ”€â”€ Mutual Information
â”œâ”€â”€ Recursive Feature Elimination (RFE)
â”œâ”€â”€ Final feature selection
â”œâ”€â”€ Document lÃ½ do chá»n/loáº¡i
â””â”€â”€ Save selected features data
```

### **05_Data_Preparation.ipynb** - Prepare for Seq2Seq
```
Input:  data/processed/selected_features.csv
Output: data/sequences/X_train.npy, y_train.npy, etc.
        models/scaler.pkl

Tasks:
â”œâ”€â”€ Define INPUT_SEQ_LEN, OUTPUT_SEQ_LEN
â”œâ”€â”€ Scaling (fit on train only)
â”œâ”€â”€ Create sequences (sliding window)
â”œâ”€â”€ Train/Val/Test split (time-based, 70/15/15)
â”œâ”€â”€ Save sequences as numpy arrays
â”œâ”€â”€ Save scaler for inverse transform
â””â”€â”€ Verify data shapes
```

### **06_Model_Training.ipynb** - Train Encoder-Decoder
```
Input:  data/sequences/*.npy
        models/scaler.pkl
Output: models/best_model.pth
        models/checkpoints/
        results/figures/training/

Tasks:
â”œâ”€â”€ Load sequences & create DataLoaders
â”œâ”€â”€ Define Encoder-Decoder architecture
â”œâ”€â”€ Setup: loss, optimizer, scheduler
â”œâ”€â”€ Training loop with:
â”‚   â”œâ”€â”€ Early stopping
â”‚   â”œâ”€â”€ Gradient clipping
â”‚   â”œâ”€â”€ Model checkpointing
â”‚   â””â”€â”€ Progress tracking (tqdm)
â”œâ”€â”€ Plot learning curves
â””â”€â”€ Save best model
```

### **07_Evaluation.ipynb** - Evaluate & Visualize Results
```
Input:  models/best_model.pth
        data/sequences/X_test.npy, y_test.npy
        models/scaler.pkl
Output: results/metrics.json
        results/predictions.csv
        results/figures/evaluation/

Tasks:
â”œâ”€â”€ Load model & test data
â”œâ”€â”€ Generate predictions
â”œâ”€â”€ Inverse transform predictions
â”œâ”€â”€ Calculate metrics per step:
â”‚   â”œâ”€â”€ RÂ² (Coefficient of Determination)
â”‚   â”œâ”€â”€ NSE (Nash-Sutcliffe Efficiency)
â”‚   â”œâ”€â”€ MAE (Mean Absolute Error)
â”‚   â””â”€â”€ RMSE (Root Mean Squared Error)
â”œâ”€â”€ Calculate average metrics
â”œâ”€â”€ Visualizations:
â”‚   â”œâ”€â”€ Actual vs Predicted time series
â”‚   â”œâ”€â”€ Scatter plots
â”‚   â”œâ”€â”€ Residual analysis
â”‚   â”œâ”€â”€ Error by forecast horizon
â”‚   â””â”€â”€ Metrics summary table
â”œâ”€â”€ Save results
â””â”€â”€ Final conclusions
```

---

## ğŸ”„ Pipeline hoÃ n chá»‰nh

### **PHASE 1: DATA EXPLORATION & UNDERSTANDING**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1.1 Load Data                                              â”‚
â”‚      - Äá»c CSV, kiá»ƒm tra shape, dtypes                      â”‚
â”‚      - Xem máº«u dá»¯ liá»‡u Ä‘áº§u/cuá»‘i                            â”‚
â”‚                                                             â”‚
â”‚  1.2 Basic Statistics                                       â”‚
â”‚      - describe(), info()                                   â”‚
â”‚      - Kiá»ƒm tra missing values                              â”‚
â”‚      - Kiá»ƒm tra duplicates                                  â”‚
â”‚                                                             â”‚
â”‚  1.3 Visualize                                              â”‚
â”‚      - Distribution cá»§a traffic_volume                      â”‚
â”‚      - Time series plot                                     â”‚
â”‚      - Correlation heatmap                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PHASE 2: DATA PREPROCESSING**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2.1 Handle DateTime                                        â”‚
â”‚      - Convert 'date_time' â†’ datetime object                â”‚
â”‚      - Set as index hoáº·c sort by time                       â”‚
â”‚      - Kiá»ƒm tra time continuity (missing timestamps)        â”‚
â”‚                                                             â”‚
â”‚  2.2 Handle Missing Values                                  â”‚
â”‚      - Numerical: mean/median/interpolation                 â”‚
â”‚      - Categorical: mode/forward fill                       â”‚
â”‚                                                             â”‚
â”‚  2.3 Handle Duplicates                                      â”‚
â”‚      - Remove duplicate timestamps                          â”‚
â”‚      - Keep first/last/mean                                 â”‚
â”‚                                                             â”‚
â”‚  2.4 Handle Outliers                                        â”‚
â”‚      - IQR method / Z-score                                 â”‚
â”‚      - Clip hoáº·c remove                                     â”‚
â”‚                                                             â”‚
â”‚  2.5 Resample (náº¿u cáº§n)                                     â”‚
â”‚      - Äáº£m báº£o dá»¯ liá»‡u Ä‘á»u theo giá»                         â”‚
â”‚      - Fill missing hours                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PHASE 3: FEATURE ENGINEERING**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3.1 Temporal Features (tá»« date_time)                       â”‚
â”‚      â”œâ”€â”€ hour (0-23)                                        â”‚
â”‚      â”œâ”€â”€ day_of_week (0-6, Monday=0)                        â”‚
â”‚      â”œâ”€â”€ day_of_month (1-31)                                â”‚
â”‚      â”œâ”€â”€ month (1-12)                                       â”‚
â”‚      â”œâ”€â”€ year                                               â”‚
â”‚      â”œâ”€â”€ is_weekend (0/1)                                   â”‚
â”‚      â”œâ”€â”€ is_rush_hour (0/1) - giá» cao Ä‘iá»ƒm 7-9, 16-18       â”‚
â”‚      â”œâ”€â”€ quarter (1-4)                                      â”‚
â”‚      â”œâ”€â”€ week_of_year (1-52)                                â”‚
â”‚      â””â”€â”€ season (Spring/Summer/Fall/Winter)                 â”‚
â”‚                                                             â”‚
â”‚  3.2 Cyclical Encoding (cho features tuáº§n hoÃ n)             â”‚
â”‚      â”œâ”€â”€ hour_sin = sin(2Ï€ Ã— hour/24)                       â”‚
â”‚      â”œâ”€â”€ hour_cos = cos(2Ï€ Ã— hour/24)                       â”‚
â”‚      â”œâ”€â”€ day_sin = sin(2Ï€ Ã— day_of_week/7)                  â”‚
â”‚      â”œâ”€â”€ day_cos = cos(2Ï€ Ã— day_of_week/7)                  â”‚
â”‚      â”œâ”€â”€ month_sin = sin(2Ï€ Ã— month/12)                     â”‚
â”‚      â””â”€â”€ month_cos = cos(2Ï€ Ã— month/12)                     â”‚
â”‚                                                             â”‚
â”‚  3.3 Lag Features (Historical values)                       â”‚
â”‚      â”œâ”€â”€ traffic_lag_1h (t-1)                               â”‚
â”‚      â”œâ”€â”€ traffic_lag_2h (t-2)                               â”‚
â”‚      â”œâ”€â”€ traffic_lag_3h (t-3)                               â”‚
â”‚      â”œâ”€â”€ traffic_lag_6h (t-6)                               â”‚
â”‚      â”œâ”€â”€ traffic_lag_12h (t-12)                             â”‚
â”‚      â”œâ”€â”€ traffic_lag_24h (t-24) - cÃ¹ng giá» hÃ´m qua          â”‚
â”‚      â”œâ”€â”€ traffic_lag_168h (t-168) - cÃ¹ng giá» tuáº§n trÆ°á»›c     â”‚
â”‚      â””â”€â”€ traffic_lag_720h (t-720) - cÃ¹ng giá» thÃ¡ng trÆ°á»›c    â”‚
â”‚                                                             â”‚
â”‚  3.4 Rolling Statistics (Window-based)                      â”‚
â”‚      â”œâ”€â”€ rolling_mean_3h                                    â”‚
â”‚      â”œâ”€â”€ rolling_mean_6h                                    â”‚
â”‚      â”œâ”€â”€ rolling_mean_12h                                   â”‚
â”‚      â”œâ”€â”€ rolling_mean_24h                                   â”‚
â”‚      â”œâ”€â”€ rolling_std_3h                                     â”‚
â”‚      â”œâ”€â”€ rolling_std_6h                                     â”‚
â”‚      â”œâ”€â”€ rolling_std_24h                                    â”‚
â”‚      â”œâ”€â”€ rolling_min_24h                                    â”‚
â”‚      â”œâ”€â”€ rolling_max_24h                                    â”‚
â”‚      â””â”€â”€ ewm_mean (Exponential Weighted Mean)               â”‚
â”‚                                                             â”‚
â”‚  3.5 Difference Features                                    â”‚
â”‚      â”œâ”€â”€ diff_1h = traffic(t) - traffic(t-1)                â”‚
â”‚      â”œâ”€â”€ diff_24h = traffic(t) - traffic(t-24)              â”‚
â”‚      â”œâ”€â”€ pct_change_1h = (t - t-1) / t-1                    â”‚
â”‚      â””â”€â”€ pct_change_24h                                     â”‚
â”‚                                                             â”‚
â”‚  3.6 Holiday Features                                       â”‚
â”‚      â”œâ”€â”€ is_holiday (0/1)                                   â”‚
â”‚      â”œâ”€â”€ holiday_type (encoded)                             â”‚
â”‚      â”œâ”€â”€ days_to_holiday                                    â”‚
â”‚      â””â”€â”€ days_after_holiday                                 â”‚
â”‚                                                             â”‚
â”‚  3.7 Weather Feature Engineering                            â”‚
â”‚      â”œâ”€â”€ temp_celsius = temp - 273.15 (convert from Kelvin) â”‚
â”‚      â”œâ”€â”€ temp_category (cold/mild/warm/hot)                 â”‚
â”‚      â”œâ”€â”€ weather_encoded (Label/One-hot encoding)           â”‚
â”‚      â”œâ”€â”€ is_rainy (rain_1h > 0)                             â”‚
â”‚      â”œâ”€â”€ is_snowy (snow_1h > 0)                             â”‚
â”‚      â”œâ”€â”€ cloud_category (clear/partly/cloudy/overcast)      â”‚
â”‚      â””â”€â”€ weather_severity_score                             â”‚
â”‚                                                             â”‚
â”‚  3.8 Interaction Features                                   â”‚
â”‚      â”œâ”€â”€ hour Ã— is_weekend                                  â”‚
â”‚      â”œâ”€â”€ hour Ã— is_holiday                                  â”‚
â”‚      â”œâ”€â”€ temp Ã— is_rush_hour                                â”‚
â”‚      â”œâ”€â”€ rain Ã— is_rush_hour                                â”‚
â”‚      â””â”€â”€ weather Ã— day_of_week                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PHASE 4: FEATURE SELECTION**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4.1 Correlation Analysis                                   â”‚
â”‚      - Pearson correlation vá»›i target                       â”‚
â”‚      - Remove highly correlated features (>0.95)            â”‚
â”‚                                                             â”‚
â”‚  4.2 Feature Importance                                     â”‚
â”‚      - Random Forest importance                             â”‚
â”‚      - XGBoost importance                                   â”‚
â”‚      - Permutation importance                               â”‚
â”‚                                                             â”‚
â”‚  4.3 Statistical Tests                                      â”‚
â”‚      - Mutual Information                                   â”‚
â”‚      - ANOVA F-test                                         â”‚
â”‚                                                             â”‚
â”‚  4.4 Wrapper Methods                                        â”‚
â”‚      - Recursive Feature Elimination (RFE)                  â”‚
â”‚      - Sequential Feature Selection                         â”‚
â”‚                                                             â”‚
â”‚  4.5 Final Feature Set                                      â”‚
â”‚      - Select top K features                                â”‚
â”‚      - Document selected features vÃ  lÃ½ do                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PHASE 5: DATA PREPARATION FOR SEQ2SEQ**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5.1 Scaling/Normalization                                  â”‚
â”‚      - MinMaxScaler (0-1) hoáº·c StandardScaler               â”‚
â”‚      - Fit on TRAIN only, transform on val/test             â”‚
â”‚      - LÆ°u scaler Ä‘á»ƒ inverse transform khi predict          â”‚
â”‚                                                             â”‚
â”‚  5.2 Create Sequences                                       â”‚
â”‚      - Input sequence length: N (e.g., 24, 48, 168)         â”‚
â”‚      - Output sequence length: 5 (predict 5 steps)          â”‚
â”‚      - Sliding window approach                              â”‚
â”‚                                                             â”‚
â”‚      Example:                                               â”‚
â”‚      X: [t-N, t-N+1, ..., t-1, t] â†’ shape: (N, num_features)â”‚
â”‚      Y: [t+1, t+2, t+3, t+4, t+5] â†’ shape: (5,)             â”‚
â”‚                                                             â”‚
â”‚  5.3 Train/Validation/Test Split                            â”‚
â”‚      - Time-based split (KHÃ”NG random shuffle!)             â”‚
â”‚      - Train: 70% (Ä‘áº§u tiÃªn)                                â”‚
â”‚      - Validation: 15% (giá»¯a)                               â”‚
â”‚      - Test: 15% (cuá»‘i cÃ¹ng)                                â”‚
â”‚                                                             â”‚
â”‚  5.4 Create DataLoaders                                     â”‚
â”‚      - Batch size: 32, 64, 128                              â”‚
â”‚      - Shuffle=True cho train, False cho val/test           â”‚
â”‚      - num_workers cho parallel loading                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PHASE 6: MODEL ARCHITECTURE (ENCODER-DECODER)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    ENCODER                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚  Input: (batch, seq_len, input_features)    â”‚   â”‚    â”‚
â”‚  â”‚  â”‚              â†“                              â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  LSTM/GRU Layers (stacked, bidirectional)   â”‚   â”‚    â”‚
â”‚  â”‚  â”‚              â†“                              â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  Hidden State: (num_layers, batch, hidden)  â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  Cell State: (num_layers, batch, hidden)    â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â†“                                  â”‚
â”‚                    Context Vector                           â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    DECODER                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚
â”‚  â”‚  â”‚  Input: Previous prediction + Context       â”‚   â”‚    â”‚
â”‚  â”‚  â”‚              â†“                              â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  LSTM/GRU Layers                            â”‚   â”‚    â”‚
â”‚  â”‚  â”‚              â†“                              â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  Fully Connected Layer                      â”‚   â”‚    â”‚
â”‚  â”‚  â”‚              â†“                              â”‚   â”‚    â”‚
â”‚  â”‚  â”‚  Output: (batch, output_seq_len, 1)         â”‚   â”‚    â”‚
â”‚  â”‚  â”‚         [t+1, t+2, t+3, t+4, t+5]           â”‚   â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  Optional Enhancements:                                     â”‚
â”‚  - Attention Mechanism (Bahdanau/Luong)                     â”‚
â”‚  - Teacher Forcing (training technique)                     â”‚
â”‚  - Dropout for regularization                               â”‚
â”‚  - Batch Normalization                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PHASE 7: TRAINING**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7.1 Loss Function                                          â”‚
â”‚      - MSELoss (Mean Squared Error)                         â”‚
â”‚      - MAELoss (L1Loss)                                     â”‚
â”‚      - HuberLoss (robust to outliers)                       â”‚
â”‚                                                             â”‚
â”‚  7.2 Optimizer                                              â”‚
â”‚      - Adam (lr=0.001)                                      â”‚
â”‚      - AdamW (with weight decay)                            â”‚
â”‚                                                             â”‚
â”‚  7.3 Learning Rate Scheduler                                â”‚
â”‚      - ReduceLROnPlateau                                    â”‚
â”‚      - CosineAnnealingLR                                    â”‚
â”‚      - StepLR                                               â”‚
â”‚                                                             â”‚
â”‚  7.4 Training Loop                                          â”‚
â”‚      - Epochs: 50-200                                       â”‚
â”‚      - Early Stopping (patience=10-20)                      â”‚
â”‚      - Gradient Clipping                                    â”‚
â”‚      - Model Checkpointing (save best model)                â”‚
â”‚                                                             â”‚
â”‚  7.5 Monitoring                                             â”‚
â”‚      - Training loss per epoch                              â”‚
â”‚      - Validation loss per epoch                            â”‚
â”‚      - Learning curves visualization                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PHASE 8: EVALUATION METRICS**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8.1 RÂ² (Coefficient of Determination)                      â”‚
â”‚      RÂ² = 1 - (SS_res / SS_tot)                             â”‚
â”‚      SS_res = Î£(y_true - y_pred)Â²                           â”‚
â”‚      SS_tot = Î£(y_true - y_mean)Â²                           â”‚
â”‚      Range: (-âˆ, 1], Best = 1                               â”‚
â”‚                                                             â”‚
â”‚  8.2 NSE (Nash-Sutcliffe Efficiency)                        â”‚
â”‚      NSE = 1 - [Î£(y_true - y_pred)Â² / Î£(y_true - y_mean)Â²]  â”‚
â”‚      TÆ°Æ¡ng tá»± RÂ² nhÆ°ng dÃ¹ng trong hydrology                 â”‚
â”‚      Range: (-âˆ, 1], Best = 1                               â”‚
â”‚      NSE > 0.5: acceptable, > 0.65: good, > 0.75: very good â”‚
â”‚                                                             â”‚
â”‚  8.3 MAE (Mean Absolute Error)                              â”‚
â”‚      MAE = (1/n) Ã— Î£|y_true - y_pred|                       â”‚
â”‚      Range: [0, âˆ), Best = 0                                â”‚
â”‚      Interpretable in original scale                        â”‚
â”‚                                                             â”‚
â”‚  8.4 RMSE (Root Mean Squared Error)                         â”‚
â”‚      RMSE = âˆš[(1/n) Ã— Î£(y_true - y_pred)Â²]                  â”‚
â”‚      Range: [0, âˆ), Best = 0                                â”‚
â”‚      Penalizes large errors more                            â”‚
â”‚                                                             â”‚
â”‚  8.5 Metrics for Seq2Seq (Multi-step)                       â”‚
â”‚      - Calculate metrics for EACH step (t+1, t+2,...,t+5)   â”‚
â”‚      - Calculate AVERAGE metrics across all steps           â”‚
â”‚      - Analyze error degradation over horizon               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **PHASE 9: RESULTS VISUALIZATION**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9.1 Training Curves                                        â”‚
â”‚      - Loss vs Epochs (train & validation)                  â”‚
â”‚      - Learning rate changes                                â”‚
â”‚                                                             â”‚
â”‚  9.2 Prediction Plots                                       â”‚
â”‚      - Actual vs Predicted time series                      â”‚
â”‚      - Scatter plot (Actual vs Predicted)                   â”‚
â”‚      - Residual plots                                       â”‚
â”‚                                                             â”‚
â”‚  9.3 Error Analysis                                         â”‚
â”‚      - Error distribution histogram                         â”‚
â”‚      - Error by time step (t+1 to t+5)                      â”‚
â”‚      - Error by hour/day/month                              â”‚
â”‚                                                             â”‚
â”‚  9.4 Metrics Summary Table                                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚      â”‚ Step     â”‚ t+1   â”‚ t+2   â”‚ t+3   â”‚ t+4   â”‚ t+5   â”‚   â”‚
â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚      â”‚ RÂ²       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚   â”‚
â”‚      â”‚ NSE      â”‚       â”‚       â”‚       â”‚       â”‚       â”‚   â”‚
â”‚      â”‚ MAE      â”‚       â”‚       â”‚       â”‚       â”‚       â”‚   â”‚
â”‚      â”‚ RMSE     â”‚       â”‚       â”‚       â”‚       â”‚       â”‚   â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Features tá»« Dataset gá»‘c

| Column | Type | Description |
|--------|------|-------------|
| `holiday` | Categorical | TÃªn ngÃ y lá»… hoáº·c None |
| `temp` | Numerical | Nhiá»‡t Ä‘á»™ (Kelvin) |
| `rain_1h` | Numerical | LÆ°á»£ng mÆ°a trong 1 giá» (mm) |
| `snow_1h` | Numerical | LÆ°á»£ng tuyáº¿t trong 1 giá» (mm) |
| `clouds_all` | Numerical | % mÃ¢y che phá»§ |
| `weather_main` | Categorical | Thá»i tiáº¿t chÃ­nh (Clear, Clouds, Rain...) |
| `weather_description` | Categorical | MÃ´ táº£ chi tiáº¿t thá»i tiáº¿t |
| `date_time` | DateTime | Timestamp |
| **`traffic_volume`** | **Numerical** | **TARGET - LÆ°u lÆ°á»£ng giao thÃ´ng** |

---

## ğŸš€ Quick Start Code Template

```python
# 1. Import libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# 2. Load data
df = pd.read_csv('data/Metro_Interstate_Traffic_Volume.csv')

# 3. Preprocess
# ... (xem chi tiáº¿t trong notebook)

# 4. Feature Engineering
# ... (xem chi tiáº¿t trong notebook)

# 5. Create sequences
# INPUT_SEQ_LEN = 24  # Use 24 hours of history
# OUTPUT_SEQ_LEN = 5  # Predict next 5 hours

# 6. Build model
# class Encoder(nn.Module): ...
# class Decoder(nn.Module): ...
# class Seq2Seq(nn.Module): ...

# 7. Train
# ... 

# 8. Evaluate
# RÂ², NSE, MAE, RMSE for each prediction step
```

---

## ğŸ“š References

1. **Seq2Seq Papers:**
   - Sutskever et al. (2014) - Sequence to Sequence Learning
   - Bahdanau et al. (2015) - Attention Mechanism

2. **Time Series Forecasting:**
   - Multi-step forecasting strategies
   - Feature engineering for time series

3. **Metrics:**
   - NSE: Nash & Sutcliffe (1970)
   - Standard regression metrics

---

## âœ… Checklist

- [ ] EDA completed
- [ ] Missing values handled
- [ ] Duplicates removed
- [ ] Outliers handled
- [ ] Temporal features created
- [ ] Lag features created
- [ ] Rolling statistics created
- [ ] Features scaled
- [ ] Sequences created
- [ ] Data split (time-based)
- [ ] Model architecture defined
- [ ] Training completed
- [ ] Metrics calculated (RÂ², NSE, MAE, RMSE)
- [ ] Results visualized
- [ ] Model saved
