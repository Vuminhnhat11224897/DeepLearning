# ğŸ“Š Project Structure: Traffic Volume Forecasting vá»›i LSTM Encoder-Decoder

## ğŸ¯ Má»¥c tiÃªu
Dá»± bÃ¡o `traffic_volume` cho **5 bÆ°á»›c thá»i gian tiáº¿p theo** sá»­ dá»¥ng mÃ´ hÃ¬nh **LSTM Encoder-Decoder (Seq2Seq)**

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
DeepLearning_final/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ Metro_Interstate_Traffic_Volume.csv    # Dá»¯ liá»‡u gá»‘c (~48,204 rows)
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ cleaned_data.csv                       # Sau preprocessing (40,575 rows)
â”‚   â”‚   â”œâ”€â”€ featured_data.csv                      # Sau feature engineering
â”‚   â”‚   â”œâ”€â”€ selected_features.csv                  # Sau feature selection (22 features)
â”‚   â”‚   â””â”€â”€ selected_features_info.json            # ThÃ´ng tin features Ä‘Ã£ chá»n
â”‚   â””â”€â”€ sequences/
â”‚       â”œâ”€â”€ X_train.npy, y_train.npy               # Training sequences
â”‚       â”œâ”€â”€ X_val.npy, y_val.npy                   # Validation sequences
â”‚       â”œâ”€â”€ X_test.npy, y_test.npy                 # Test sequences
â”‚       â””â”€â”€ metadata.json                          # Sequence metadata
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb                               # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_Preprocessing.ipynb                     # Data cleaning & preprocessing
â”‚   â”œâ”€â”€ 03_Feature_Engineering.ipynb               # Feature creation (LSTM-safe)
â”‚   â”œâ”€â”€ 04_Feature_Selection.ipynb                 # Feature selection (no leakage)
â”‚   â”œâ”€â”€ 05_Data_Preparation.ipynb                  # Segment splitting & sequences
â”‚   â”œâ”€â”€ 06_Model_Training.ipynb                    # LSTM Encoder-Decoder training
â”‚   â”œâ”€â”€ 06a_Optuna_Optimization.ipynb              # Hyperparameter tuning
â”‚   â””â”€â”€ 07_Evaluation.ipynb                        # Evaluation & visualization
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                                  # Hyperparameters & paths
â”‚   â”œâ”€â”€ data_preprocessing.py                      # Preprocessing + segment splitting
â”‚   â”œâ”€â”€ feature_engineering.py                     # LSTM-safe feature functions
â”‚   â”œâ”€â”€ feature_selection.py                       # Feature selection (no leakage)
â”‚   â”œâ”€â”€ dataset.py                                 # PyTorch Dataset & sequences
â”‚   â”œâ”€â”€ model.py                                   # LSTM Encoder-Decoder architecture
â”‚   â”œâ”€â”€ train.py                                   # Training loop
â”‚   â”œâ”€â”€ evaluate.py                                # Metrics: RÂ², NSE, MAE, RMSE
â”‚   â”œâ”€â”€ optuna_optimization.py                     # Hyperparameter tuning
â”‚   â”œâ”€â”€ logger.py                                  # Logging utilities
â”‚   â””â”€â”€ utils.py                                   # Helper functions
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pth                             # Best model weights
â”‚   â”œâ”€â”€ scaler.pkl                                 # Saved MinMaxScaler
â”‚   â””â”€â”€ checkpoints/                               # Epoch checkpoints
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ metrics.json                               # Evaluation metrics
â”‚   â”œâ”€â”€ predictions.csv                            # Test predictions
â”‚   â””â”€â”€ figures/
â”‚       â”œâ”€â”€ eda/                                   # EDA plots
â”‚       â”œâ”€â”€ training/                              # Learning curves
â”‚       â””â”€â”€ evaluation/                            # Prediction plots
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ best_params.json                           # Best hyperparameters
â”‚   â””â”€â”€ optuna_study.db                            # Optuna database
â”‚
â”œâ”€â”€ Deep/                                          # Virtual environment
â”œâ”€â”€ requirements.txt                               # Dependencies
â”œâ”€â”€ PROJECT_STRUCTURE.md                           # This file
â””â”€â”€ README.md                                      # Project overview
```

---

## ğŸ““ Chi tiáº¿t tá»«ng Notebook

### **01_EDA.ipynb** - Exploratory Data Analysis
```
Input:  data/raw/Metro_Interstate_Traffic_Volume.csv
Output: Insights vá» dá»¯ liá»‡u

Tasks:
â”œâ”€â”€ Load & inspect data (48,204 rows, 9 columns)
â”œâ”€â”€ Check duplicates (7,629 duplicates found)
â”œâ”€â”€ Check missing timestamps (11,976 gaps)
â”œâ”€â”€ Target distribution analysis
â”œâ”€â”€ Time series visualization
â”œâ”€â”€ Correlation analysis
â””â”€â”€ Initial findings
```

### **02_Preprocessing.ipynb** - Data Preprocessing
```
Input:  data/raw/Metro_Interstate_Traffic_Volume.csv
Output: data/processed/cleaned_data.csv (40,575 rows)

Tasks:
â”œâ”€â”€ Convert datetime & sort
â”œâ”€â”€ Remove duplicates â†’ 40,575 rows
â”œâ”€â”€ Handle missing values
â”œâ”€â”€ Handle outliers (IQR clipping)
â”œâ”€â”€ Verify data quality
â””â”€â”€ Save cleaned data
```

### **03_Feature_Engineering.ipynb** - LSTM-Safe Features
```
Input:  data/processed/cleaned_data.csv
Output: data/processed/featured_data.csv

âš ï¸ IMPORTANT: No lag/rolling/diff features for LSTM!

Tasks:
â”œâ”€â”€ Temporal features (hour, day_of_week, month, season, etc.)
â”œâ”€â”€ Cyclical encoding (sin/cos for hour, day, month)
â”œâ”€â”€ Binary features (is_weekend, is_rush_hour)
â”œâ”€â”€ Weather features (temp_celsius, is_rainy, is_snowy)
â”œâ”€â”€ Interaction features (rush_rain)
â””â”€â”€ Save featured data

âŒ NOT included (by design):
â”œâ”€â”€ Lag features (traffic_lag_1h, etc.)
â”œâ”€â”€ Rolling statistics (rolling_mean, etc.)
â””â”€â”€ Difference features (diff_1h, etc.)
â†’ LSTM learns these patterns from sequence input!
```

### **04_Feature_Selection.ipynb** - No-Leakage Selection
```
Input:  data/processed/featured_data.csv
Output: data/processed/selected_features.csv (22 features)
        data/processed/selected_features_info.json

Tasks:
â”œâ”€â”€ Define LSTM-safe feature categories
â”œâ”€â”€ Check for leakage features
â”œâ”€â”€ Remove highly correlated features (>0.95)
â”œâ”€â”€ Verify correlation with target
â””â”€â”€ Save selected features

Selected Categories:
â”œâ”€â”€ cyclical: hour_sin/cos, day_sin/cos, month_sin/cos
â”œâ”€â”€ temporal: hour, day_of_week, month, season, etc.
â”œâ”€â”€ context: is_weekend, is_rush_hour
â”œâ”€â”€ weather: temp, temp_celsius, clouds_all, rain_1h, etc.
â””â”€â”€ interaction: rush_rain
```

### **05_Data_Preparation.ipynb** - Segment-Based Workflow
```
Input:  data/processed/selected_features.csv
Output: data/sequences/*.npy
        models/scaler.pkl

âš ï¸ KEY INNOVATION: Segment-based splitting!

Tasks:
â”œâ”€â”€ Load data (40,575 rows)
â”œâ”€â”€ Split into continuous segments:
â”‚   â”œâ”€â”€ Detect 2,588 timestamp gaps
â”‚   â”œâ”€â”€ Create 113 valid segments
â”‚   â”œâ”€â”€ Skip 2,476 short segments (<48 rows)
â”‚   â””â”€â”€ Usable: 30,871 rows (76.1%)
â”œâ”€â”€ Scale data (fit on training portion only)
â”œâ”€â”€ Create sequences from each segment independently
â”œâ”€â”€ Train/Val/Test split (70/15/15, time-based)
â””â”€â”€ Save sequences & metadata

Sequence Format:
â”œâ”€â”€ X: (n_samples, 24, 22) - 24 hours Ã— 22 features
â””â”€â”€ y: (n_samples, 5) - next 5 hours traffic volume
```

### **06_Model_Training.ipynb** - LSTM Encoder-Decoder
```
Input:  data/sequences/*.npy
Output: models/best_model.pth
        results/figures/training/

Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input (batch, 24, 22)                                  â”‚
â”‚         â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ENCODER (Bidirectional LSTM)                   â”‚    â”‚
â”‚  â”‚  - hidden_size: 128                             â”‚    â”‚
â”‚  â”‚  - num_layers: 2                                â”‚    â”‚
â”‚  â”‚  - dropout: 0.2                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â†“                                               â”‚
â”‚  Context Vector (hidden_state, cell_state)              â”‚
â”‚         â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  DECODER (LSTM)                                 â”‚    â”‚
â”‚  â”‚  - Teacher Forcing during training              â”‚    â”‚
â”‚  â”‚  - Autoregressive during inference              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â†“                                               â”‚
â”‚  Output (batch, 5) - traffic volume for t+1...t+5       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Training:
â”œâ”€â”€ Loss: MSELoss
â”œâ”€â”€ Optimizer: Adam (lr=0.001)
â”œâ”€â”€ Scheduler: ReduceLROnPlateau
â”œâ”€â”€ Early Stopping (patience=15)
â”œâ”€â”€ Gradient Clipping (max_norm=1.0)
â””â”€â”€ Teacher Forcing Ratio: 0.5
```

### **07_Evaluation.ipynb** - Metrics & Visualization
```
Input:  models/best_model.pth
        data/sequences/X_test.npy, y_test.npy
Output: results/metrics.json
        results/predictions.csv
        results/figures/evaluation/

Metrics (per step t+1...t+5 and average):
â”œâ”€â”€ RÂ² (Coefficient of Determination)
â”œâ”€â”€ NSE (Nash-Sutcliffe Efficiency)
â”œâ”€â”€ MAE (Mean Absolute Error)
â””â”€â”€ RMSE (Root Mean Squared Error)

Visualizations:
â”œâ”€â”€ Actual vs Predicted time series
â”œâ”€â”€ Scatter plots
â”œâ”€â”€ Residual analysis
â”œâ”€â”€ Error by forecast horizon
â””â”€â”€ Metrics summary table
```

---

## ğŸ”„ Data Flow Pipeline

```
Raw Data (48,204 rows)
    â†“
[02_Preprocessing] Remove duplicates, handle outliers
    â†“
Cleaned Data (40,575 rows, 2,588 gaps)
    â†“
[03_Feature_Engineering] Add temporal, cyclical, weather features
    â†“
Featured Data (40,575 rows, ~30 features)
    â†“
[04_Feature_Selection] Select LSTM-safe features (no leakage)
    â†“
Selected Data (40,575 rows, 22 features)
    â†“
[05_Data_Preparation] Split into 113 continuous segments
    â†“
Usable Data (30,871 rows from 113 segments)
    â†“
Create Sequences (sliding window within each segment)
    â†“
Sequences: X(n, 24, 22), y(n, 5)
    â†“
Train/Val/Test Split (70/15/15, time-based)
    â†“
[06_Model_Training] LSTM Encoder-Decoder
    â†“
[07_Evaluation] Metrics & Predictions
```

---

## ğŸ“ Features tá»« Dataset gá»‘c

| Column | Type | Description |
|--------|------|-------------|
| `holiday` | Categorical | TÃªn ngÃ y lá»… hoáº·c None |
| `temp` | Numerical | Nhiá»‡t Ä‘á»™ (Kelvin) |
| `rain_1h` | Numerical | LÆ°á»£ng mÆ°a trong 1 giá» (mm) |
| `snow_1h` | Numerical | LÆ°á»£ng tuyáº¿t trong 1 giá» (mm) |
| `clouds_all` | Numerical | % mÃ¢y che phá»§ |
| `weather_main` | Categorical | Thá»i tiáº¿t chÃ­nh |
| `weather_description` | Categorical | MÃ´ táº£ chi tiáº¿t |
| `date_time` | DateTime | Timestamp |
| **`traffic_volume`** | **Numerical** | **TARGET** |

---

## ğŸ”‘ Key Design Decisions

### 1. No Lag/Rolling/Diff Features
```
âŒ KhÃ´ng dÃ¹ng: traffic_lag_1h, rolling_mean_24h, diff_1h, etc.
âœ… LÃ½ do: 
   - LSTM há»c temporal patterns tá»« sequence input
   - ThÃªm lag features thá»§ cÃ´ng gÃ¢y data leakage
   - Model nháº­n 24 giá» history â†’ tá»± "tháº¥y" lag information
```

### 2. Segment-Based Workflow
```
âŒ KhÃ´ng dÃ¹ng: Táº¡o sequences tá»« toÃ n bá»™ data (cÃ³ gaps)
âœ… LÃ½ do:
   - Data cÃ³ 2,588 timestamp gaps
   - Sequence chá»“ng qua gap â†’ LSTM há»c sai pattern
   - TÃ¡ch segments â†’ má»—i sequence Ä‘áº£m báº£o liÃªn tá»¥c
```

### 3. Time-Based Split
```
âŒ KhÃ´ng dÃ¹ng: Random shuffle
âœ… LÃ½ do:
   - Time series cáº§n giá»¯ thá»© tá»± thá»i gian
   - Train trÃªn past â†’ predict future
   - Shuffle gÃ¢y data leakage tá»« future
```

---

## âœ… Checklist

- [x] EDA completed
- [x] Duplicates removed (7,629)
- [x] Missing values handled
- [x] Outliers clipped
- [x] Temporal features created
- [x] Cyclical encoding added
- [x] Weather features engineered
- [x] Features selected (no leakage)
- [x] Segments split (113 continuous)
- [x] Sequences created (24â†’5)
- [x] Data scaled (fit on train only)
- [x] Train/Val/Test split (time-based)
- [ ] Model trained
- [ ] Metrics calculated
- [ ] Results visualized
- [ ] Model saved

---

## ğŸ“š References

1. **Seq2Seq**: Sutskever et al. (2014) - Sequence to Sequence Learning
2. **LSTM**: Hochreiter & Schmidhuber (1997) - Long Short-Term Memory
3. **Dataset**: Metro Interstate Traffic Volume - UCI ML Repository
4. **Metrics**: Nash & Sutcliffe (1970) - NSE for model evaluation
