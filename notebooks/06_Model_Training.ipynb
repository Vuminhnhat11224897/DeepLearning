{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8cbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import (\n",
    "    SEQUENCES_DIR, BEST_MODEL_PATH, CHECKPOINTS_DIR, TRAINING_FIGURES_DIR,\n",
    "    LOGS_DIR, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN,\n",
    "    ENCODER_HIDDEN_SIZE, ENCODER_NUM_LAYERS, ENCODER_DROPOUT, ENCODER_BIDIRECTIONAL,\n",
    "    DECODER_HIDDEN_SIZE, DECODER_NUM_LAYERS, DECODER_DROPOUT,\n",
    "    BATCH_SIZE, LEARNING_RATE, WEIGHT_DECAY, NUM_EPOCHS,\n",
    "    EARLY_STOPPING_PATIENCE, GRADIENT_CLIP, TEACHER_FORCING_RATIO,\n",
    "    LR_SCHEDULER, DEVICE, RANDOM_SEED, set_seed\n",
    ")\n",
    "from src.dataset import create_dataloaders\n",
    "from src.model import build_model\n",
    "from src.train import train\n",
    "from src.utils import print_gpu_info, save_figure, load_json\n",
    "\n",
    "# Set random seed\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print_gpu_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b3240",
   "metadata": {},
   "source": [
    "## 6.1 Load Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca34538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences\n",
    "X_train = np.load(os.path.join(SEQUENCES_DIR, 'X_train.npy'))\n",
    "y_train = np.load(os.path.join(SEQUENCES_DIR, 'y_train.npy'))\n",
    "X_val = np.load(os.path.join(SEQUENCES_DIR, 'X_val.npy'))\n",
    "y_val = np.load(os.path.join(SEQUENCES_DIR, 'y_val.npy'))\n",
    "X_test = np.load(os.path.join(SEQUENCES_DIR, 'X_test.npy'))\n",
    "y_test = np.load(os.path.join(SEQUENCES_DIR, 'y_test.npy'))\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_json(os.path.join(SEQUENCES_DIR, 'metadata.json'))\n",
    "\n",
    "print(\"Data loaded:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af442ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions\n",
    "n_features = X_train.shape[2]\n",
    "print(f\"\\nNumber of input features: {n_features}\")\n",
    "print(f\"Input sequence length: {INPUT_SEQ_LEN}\")\n",
    "print(f\"Output sequence length: {OUTPUT_SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65013b99",
   "metadata": {},
   "source": [
    "## 6.2 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f887ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ca249",
   "metadata": {},
   "source": [
    "## 6.3 Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Encoder-Decoder model\n",
    "model = build_model(\n",
    "    input_size=n_features,\n",
    "    hidden_size=ENCODER_HIDDEN_SIZE,\n",
    "    num_layers=ENCODER_NUM_LAYERS,\n",
    "    dropout=ENCODER_DROPOUT,\n",
    "    bidirectional=ENCODER_BIDIRECTIONAL,\n",
    "    output_seq_len=OUTPUT_SEQ_LEN,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c8b88",
   "metadata": {},
   "source": [
    "## 6.4 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8710a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training configuration\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"Gradient clipping: {GRADIENT_CLIP}\")\n",
    "print(f\"Teacher forcing ratio: {TEACHER_FORCING_RATIO}\")\n",
    "print(f\"LR scheduler: {LR_SCHEDULER}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc91ee",
   "metadata": {},
   "source": [
    "## 6.4a Load Optuna Best Params (Optional)\n",
    "\n",
    "Run this cell if you have already run Optuna optimization and want to use the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best parameters from Optuna (if available)\n",
    "best_params_path = os.path.join(LOGS_DIR, 'best_params.json')\n",
    "\n",
    "USE_OPTUNA_PARAMS = False  # Set to True to use Optuna parameters\n",
    "\n",
    "if USE_OPTUNA_PARAMS and os.path.exists(best_params_path):\n",
    "    optuna_params = load_json(best_params_path)\n",
    "    print(\"Loaded Optuna best parameters:\")\n",
    "    for k, v in optuna_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Override config values\n",
    "    ENCODER_HIDDEN_SIZE = optuna_params.get('hidden_size', ENCODER_HIDDEN_SIZE)\n",
    "    ENCODER_NUM_LAYERS = optuna_params.get('num_layers', ENCODER_NUM_LAYERS)\n",
    "    ENCODER_DROPOUT = optuna_params.get('dropout', ENCODER_DROPOUT)\n",
    "    LEARNING_RATE = optuna_params.get('learning_rate', LEARNING_RATE)\n",
    "    BATCH_SIZE = optuna_params.get('batch_size', BATCH_SIZE)\n",
    "    WEIGHT_DECAY = optuna_params.get('weight_decay', WEIGHT_DECAY)\n",
    "    TEACHER_FORCING_RATIO = optuna_params.get('teacher_forcing_ratio', TEACHER_FORCING_RATIO)\n",
    "    \n",
    "    print(\"\\nConfig values updated!\")\n",
    "else:\n",
    "    print(\"Using default config parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb6493",
   "metadata": {},
   "source": [
    "## 6.5 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36717be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with logging\n",
    "history = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    teacher_forcing_ratio=TEACHER_FORCING_RATIO,\n",
    "    gradient_clip=GRADIENT_CLIP,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    checkpoint_dir=CHECKPOINTS_DIR,\n",
    "    best_model_path=BEST_MODEL_PATH,\n",
    "    device=DEVICE,\n",
    "    log_dir=LOGS_DIR  # Enable logging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29eb65",
   "metadata": {},
   "source": [
    "## 6.6 Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6883f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
    "axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "axes[0].axvline(x=history['best_epoch'] + 1, color='green', linestyle='--', label=f'Best Epoch ({history[\"best_epoch\"]+1})')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(epochs, history['learning_rate'], 'g-')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(TRAINING_FIGURES_DIR, exist_ok=True)\n",
    "save_figure(fig, os.path.join(TRAINING_FIGURES_DIR, 'learning_curves.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a52fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total epochs trained: {len(history['train_loss'])}\")\n",
    "print(f\"Best epoch: {history['best_epoch'] + 1}\")\n",
    "print(f\"Best validation loss: {history['best_val_loss']:.6f}\")\n",
    "print(f\"Final training loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final validation loss: {history['val_loss'][-1]:.6f}\")\n",
    "print(f\"Model saved to: {BEST_MODEL_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35d70c",
   "metadata": {},
   "source": [
    "## 6.7 Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "from src.utils import save_json\n",
    "\n",
    "history_to_save = {\n",
    "    'train_loss': [float(x) for x in history['train_loss']],\n",
    "    'val_loss': [float(x) for x in history['val_loss']],\n",
    "    'learning_rate': [float(x) for x in history['learning_rate']],\n",
    "    'best_epoch': int(history['best_epoch']),\n",
    "    'best_val_loss': float(history['best_val_loss'])\n",
    "}\n",
    "\n",
    "save_json(history_to_save, os.path.join(TRAINING_FIGURES_DIR, 'training_history.json'))\n",
    "print(\"Training history saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c89ea",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model Training completed:**\n",
    "1. ✅ Loaded sequence data\n",
    "2. ✅ Created DataLoaders\n",
    "3. ✅ Built Encoder-Decoder model\n",
    "4. ✅ Trained with GPU\n",
    "5. ✅ Early stopping applied\n",
    "6. ✅ Saved best model\n",
    "7. ✅ Visualized learning curves\n",
    "\n",
    "**Next step:** Evaluation (07_Evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
