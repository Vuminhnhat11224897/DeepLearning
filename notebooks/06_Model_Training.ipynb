{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f8cbfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "GPU: NVIDIA GeForce RTX 3060\n",
      "CUDA Version: 11.8\n",
      "Memory Allocated: 24.25 MB\n",
      "Memory Cached: 172.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import (\n",
    "    SEQUENCES_DIR, BEST_MODEL_PATH, CHECKPOINTS_DIR, TRAINING_FIGURES_DIR,\n",
    "    LOGS_DIR, INPUT_SEQ_LEN, OUTPUT_SEQ_LEN,\n",
    "    ENCODER_HIDDEN_SIZE, ENCODER_NUM_LAYERS, ENCODER_DROPOUT, ENCODER_BIDIRECTIONAL,\n",
    "    DECODER_HIDDEN_SIZE, DECODER_NUM_LAYERS, DECODER_DROPOUT,\n",
    "    BATCH_SIZE, LEARNING_RATE, WEIGHT_DECAY, NUM_EPOCHS,\n",
    "    EARLY_STOPPING_PATIENCE, GRADIENT_CLIP, TEACHER_FORCING_RATIO,\n",
    "    LR_SCHEDULER, DEVICE, RANDOM_SEED, set_seed\n",
    ")\n",
    "from src.dataset import create_dataloaders\n",
    "from src.model import build_model\n",
    "from src.train import train\n",
    "from src.utils import print_gpu_info, save_figure, load_json\n",
    "\n",
    "# Set random seed\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print_gpu_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b3240",
   "metadata": {},
   "source": [
    "## 6.1 Load Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca34538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded:\n",
      "X_train: (19394, 24, 22)\n",
      "y_train: (19394, 5)\n",
      "X_val: (4156, 24, 22)\n",
      "y_val: (4156, 5)\n",
      "X_test: (4157, 24, 22)\n",
      "y_test: (4157, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load sequences\n",
    "X_train = np.load(os.path.join(SEQUENCES_DIR, 'X_train.npy'))\n",
    "y_train = np.load(os.path.join(SEQUENCES_DIR, 'y_train.npy'))\n",
    "X_val = np.load(os.path.join(SEQUENCES_DIR, 'X_val.npy'))\n",
    "y_val = np.load(os.path.join(SEQUENCES_DIR, 'y_val.npy'))\n",
    "X_test = np.load(os.path.join(SEQUENCES_DIR, 'X_test.npy'))\n",
    "y_test = np.load(os.path.join(SEQUENCES_DIR, 'y_test.npy'))\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_json(os.path.join(SEQUENCES_DIR, 'metadata.json'))\n",
    "\n",
    "print(\"Data loaded:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af442ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of input features: 22\n",
      "Input sequence length: 24\n",
      "Output sequence length: 5\n"
     ]
    }
   ],
   "source": [
    "# Get dimensions\n",
    "n_features = X_train.shape[2]\n",
    "print(f\"\\nNumber of input features: {n_features}\")\n",
    "print(f\"Input sequence length: {INPUT_SEQ_LEN}\")\n",
    "print(f\"Output sequence length: {OUTPUT_SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65013b99",
   "metadata": {},
   "source": [
    "## 6.2 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f887ebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataLoaders with batch_size=64\n",
      "\n",
      "Train batches: 304\n",
      "Val batches: 65\n",
      "Test batches: 65\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ca249",
   "metadata": {},
   "source": [
    "## 6.3 Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4313e0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built on cuda\n",
      "Total parameters: 816,001\n",
      "Trainable parameters: 816,001\n"
     ]
    }
   ],
   "source": [
    "# Build Encoder-Decoder model\n",
    "model = build_model(\n",
    "    input_size=n_features,\n",
    "    hidden_size=ENCODER_HIDDEN_SIZE,\n",
    "    num_layers=ENCODER_NUM_LAYERS,\n",
    "    dropout=ENCODER_DROPOUT,\n",
    "    bidirectional=ENCODER_BIDIRECTIONAL,\n",
    "    output_seq_len=OUTPUT_SEQ_LEN,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dff8916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "============================================================\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(22, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (fc_hidden): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc_cell): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (lstm): LSTM(1, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
      "    (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model architecture summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c8b88",
   "metadata": {},
   "source": [
    "## 6.4 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8710a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "============================================================\n",
      "Device: cuda\n",
      "Batch size: 64\n",
      "Learning rate: 0.001\n",
      "Weight decay: 1e-05\n",
      "Epochs: 100\n",
      "Early stopping patience: 15\n",
      "Gradient clipping: 1.0\n",
      "Teacher forcing ratio: 0.5\n",
      "LR scheduler: ReduceLROnPlateau\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Print training configuration\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"Gradient clipping: {GRADIENT_CLIP}\")\n",
    "print(f\"Teacher forcing ratio: {TEACHER_FORCING_RATIO}\")\n",
    "print(f\"LR scheduler: {LR_SCHEDULER}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc91ee",
   "metadata": {},
   "source": [
    "## 6.4a Load Optuna Best Params (Optional)\n",
    "\n",
    "Run this cell if you have already run Optuna optimization and want to use the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba21772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Optuna best parameters:\n",
      "  hidden_size: 64\n",
      "  num_layers: 2\n",
      "  dropout: 0.24822872742761123\n",
      "  learning_rate: 0.0015517954134162649\n",
      "  batch_size: 64\n",
      "  weight_decay: 0.00037944994831423065\n",
      "  teacher_forcing_ratio: 0.648343601345059\n",
      "\n",
      "Config values updated!\n"
     ]
    }
   ],
   "source": [
    "# Load best parameters from Optuna (if available)\n",
    "best_params_path = os.path.join(LOGS_DIR, 'best_params.json')\n",
    "\n",
    "USE_OPTUNA_PARAMS = True  # Set to True to use Optuna parameters\n",
    "\n",
    "if USE_OPTUNA_PARAMS and os.path.exists(best_params_path):\n",
    "    optuna_params = load_json(best_params_path)\n",
    "    print(\"Loaded Optuna best parameters:\")\n",
    "    for k, v in optuna_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Override config values\n",
    "    ENCODER_HIDDEN_SIZE = optuna_params.get('hidden_size', ENCODER_HIDDEN_SIZE)\n",
    "    ENCODER_NUM_LAYERS = optuna_params.get('num_layers', ENCODER_NUM_LAYERS)\n",
    "    ENCODER_DROPOUT = optuna_params.get('dropout', ENCODER_DROPOUT)\n",
    "    LEARNING_RATE = optuna_params.get('learning_rate', LEARNING_RATE)\n",
    "    BATCH_SIZE = optuna_params.get('batch_size', BATCH_SIZE)\n",
    "    WEIGHT_DECAY = optuna_params.get('weight_decay', WEIGHT_DECAY)\n",
    "    TEACHER_FORCING_RATIO = optuna_params.get('teacher_forcing_ratio', TEACHER_FORCING_RATIO)\n",
    "    \n",
    "    print(\"\\nConfig values updated!\")\n",
    "else:\n",
    "    print(\"Using default config parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb6493",
   "metadata": {},
   "source": [
    "## 6.5 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b36717be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36msetup_logger\u001b[0m:\u001b[36m67\u001b[0m | \u001b[1mLog file: d:\\DeepLearning_final\\logs\\training.log\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m134\u001b[0m | \u001b[1mTraining on cuda\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m140\u001b[0m | \u001b[1m============================================================\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m141\u001b[0m | \u001b[1mHyperparameters:\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m143\u001b[0m | \u001b[1m  num_epochs: 100\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m143\u001b[0m | \u001b[1m  learning_rate: 0.0015517954134162649\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m143\u001b[0m | \u001b[1m  weight_decay: 0.00037944994831423065\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m143\u001b[0m | \u001b[1m  teacher_forcing_ratio: 0.648343601345059\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m143\u001b[0m | \u001b[1m  gradient_clip: 1.0\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m143\u001b[0m | \u001b[1m  early_stopping_patience: 15\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m143\u001b[0m | \u001b[1m  lr_scheduler: ReduceLROnPlateau\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_hyperparams\u001b[0m:\u001b[36m144\u001b[0m | \u001b[1m============================================================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:11\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 1 with val_loss=0.006403\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   1 | Train Loss: 0.014411 | Val Loss: 0.006403 | LR: 1.55e-03 | Time: 2.7s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   2 | Train Loss: 0.003758 | Val Loss: 0.007568 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:16\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 3 with val_loss=0.003764\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   3 | Train Loss: 0.003030 | Val Loss: 0.003764 | LR: 1.55e-03 | Time: 2.5s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   4 | Train Loss: 0.002648 | Val Loss: 0.004449 | LR: 1.55e-03 | Time: 2.5s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   5 | Train Loss: 0.002356 | Val Loss: 0.009058 | LR: 1.55e-03 | Time: 2.5s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   6 | Train Loss: 0.002390 | Val Loss: 0.005607 | LR: 1.55e-03 | Time: 2.5s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   7 | Train Loss: 0.002161 | Val Loss: 0.004045 | LR: 1.55e-03 | Time: 2.5s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:29\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 8 with val_loss=0.003663\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   8 | Train Loss: 0.001943 | Val Loss: 0.003663 | LR: 1.55e-03 | Time: 2.6s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:32\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 9 with val_loss=0.003288\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch   9 | Train Loss: 0.002008 | Val Loss: 0.003288 | LR: 1.55e-03 | Time: 2.7s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:34\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 10 with val_loss=0.003255\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  10 | Train Loss: 0.001897 | Val Loss: 0.003255 | LR: 1.55e-03 | Time: 2.5s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  11 | Train Loss: 0.001874 | Val Loss: 0.003786 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:39\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 12 with val_loss=0.003198\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  12 | Train Loss: 0.001822 | Val Loss: 0.003198 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  13 | Train Loss: 0.001639 | Val Loss: 0.003330 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:44\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 14 with val_loss=0.003189\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  14 | Train Loss: 0.001689 | Val Loss: 0.003189 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  15 | Train Loss: 0.001799 | Val Loss: 0.003521 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:48\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 16 with val_loss=0.003173\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  16 | Train Loss: 0.001729 | Val Loss: 0.003173 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  17 | Train Loss: 0.001740 | Val Loss: 0.003206 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  18 | Train Loss: 0.001624 | Val Loss: 0.003912 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  19 | Train Loss: 0.001557 | Val Loss: 0.003383 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:03:58\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 20 with val_loss=0.003107\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:03:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  20 | Train Loss: 0.001431 | Val Loss: 0.003107 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:00\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m123\u001b[0m | \u001b[32m\u001b[1mNew best model at epoch 21 with val_loss=0.002864\u001b[0m\n",
      "\u001b[32m2025-12-05 04:04:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_best_model\u001b[0m:\u001b[36m124\u001b[0m | \u001b[1mModel saved to: d:\\DeepLearning_final\\models\\best_model.pth\u001b[0m\n",
      "\u001b[32m2025-12-05 04:04:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  21 | Train Loss: 0.001600 | Val Loss: 0.002864 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  22 | Train Loss: 0.001412 | Val Loss: 0.003077 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  23 | Train Loss: 0.001450 | Val Loss: 0.003092 | LR: 1.55e-03 | Time: 2.5s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  24 | Train Loss: 0.001375 | Val Loss: 0.003116 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  25 | Train Loss: 0.001430 | Val Loss: 0.003791 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  26 | Train Loss: 0.001557 | Val Loss: 0.002958 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  27 | Train Loss: 0.001494 | Val Loss: 0.003417 | LR: 1.55e-03 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  28 | Train Loss: 0.001179 | Val Loss: 0.002970 | LR: 7.76e-04 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  29 | Train Loss: 0.001119 | Val Loss: 0.003213 | LR: 7.76e-04 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  30 | Train Loss: 0.001107 | Val Loss: 0.003056 | LR: 7.76e-04 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  31 | Train Loss: 0.001094 | Val Loss: 0.003201 | LR: 7.76e-04 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  32 | Train Loss: 0.001092 | Val Loss: 0.003220 | LR: 7.76e-04 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  33 | Train Loss: 0.001046 | Val Loss: 0.003149 | LR: 7.76e-04 | Time: 3.0s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  34 | Train Loss: 0.000952 | Val Loss: 0.003244 | LR: 3.88e-04 | Time: 2.6s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  35 | Train Loss: 0.000892 | Val Loss: 0.003276 | LR: 3.88e-04 | Time: 2.4s\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-05 04:04:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_epoch\u001b[0m:\u001b[36m113\u001b[0m | \u001b[1mEpoch  36 | Train Loss: 0.000917 | Val Loss: 0.003115 | LR: 3.88e-04 | Time: 2.4s\u001b[0m\n",
      "\u001b[32m2025-12-05 04:04:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_early_stopping\u001b[0m:\u001b[36m128\u001b[0m | \u001b[33m\u001b[1mEarly stopping triggered at epoch 36 (patience=15)\u001b[0m\n",
      "\u001b[32m2025-12-05 04:04:37\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.logger\u001b[0m:\u001b[36mlog_training_complete\u001b[0m:\u001b[36m132\u001b[0m | \u001b[32m\u001b[1mTraining completed in 1.5 minutes | Best epoch: 21 | Best val_loss: 0.002864\u001b[0m\n",
      "\u001b[32m2025-12-05 04:04:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m250\u001b[0m | \u001b[1mLoaded best model from epoch 21\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Train model with logging\n",
    "history = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    teacher_forcing_ratio=TEACHER_FORCING_RATIO,\n",
    "    gradient_clip=GRADIENT_CLIP,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    checkpoint_dir=CHECKPOINTS_DIR,\n",
    "    best_model_path=BEST_MODEL_PATH,\n",
    "    device=DEVICE,\n",
    "    log_dir=LOGS_DIR  # Enable logging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d29eb65",
   "metadata": {},
   "source": [
    "## 6.6 Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6883f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: d:\\DeepLearning_final\\results\\figures\\training\\learning_curves.png\n"
     ]
    }
   ],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
    "axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "axes[0].axvline(x=history['best_epoch'] + 1, color='green', linestyle='--', label=f'Best Epoch ({history[\"best_epoch\"]+1})')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(epochs, history['learning_rate'], 'g-')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(TRAINING_FIGURES_DIR, exist_ok=True)\n",
    "save_figure(fig, os.path.join(TRAINING_FIGURES_DIR, 'learning_curves.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9a52fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "Total epochs trained: 36\n",
      "Best epoch: 21\n",
      "Best validation loss: 0.002864\n",
      "Final training loss: 0.000917\n",
      "Final validation loss: 0.003115\n",
      "Model saved to: d:\\DeepLearning_final\\models\\best_model.pth\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total epochs trained: {len(history['train_loss'])}\")\n",
    "print(f\"Best epoch: {history['best_epoch'] + 1}\")\n",
    "print(f\"Best validation loss: {history['best_val_loss']:.6f}\")\n",
    "print(f\"Final training loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final validation loss: {history['val_loss'][-1]:.6f}\")\n",
    "print(f\"Model saved to: {BEST_MODEL_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35d70c",
   "metadata": {},
   "source": [
    "## 6.7 Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5582423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: d:\\DeepLearning_final\\results\\figures\\training\\training_history.json\n",
      "Training history saved!\n"
     ]
    }
   ],
   "source": [
    "# Save training history\n",
    "from src.utils import save_json\n",
    "\n",
    "history_to_save = {\n",
    "    'train_loss': [float(x) for x in history['train_loss']],\n",
    "    'val_loss': [float(x) for x in history['val_loss']],\n",
    "    'learning_rate': [float(x) for x in history['learning_rate']],\n",
    "    'best_epoch': int(history['best_epoch']),\n",
    "    'best_val_loss': float(history['best_val_loss'])\n",
    "}\n",
    "\n",
    "save_json(history_to_save, os.path.join(TRAINING_FIGURES_DIR, 'training_history.json'))\n",
    "print(\"Training history saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c89ea",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model Training completed:**\n",
    "1. ✅ Loaded sequence data\n",
    "2. ✅ Created DataLoaders\n",
    "3. ✅ Built Encoder-Decoder model\n",
    "4. ✅ Trained with GPU\n",
    "5. ✅ Early stopping applied\n",
    "6. ✅ Saved best model\n",
    "7. ✅ Visualized learning curves\n",
    "\n",
    "**Next step:** Evaluation (07_Evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
