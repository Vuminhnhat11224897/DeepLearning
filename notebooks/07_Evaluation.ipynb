{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import (\n",
    "    SEQUENCES_DIR, BEST_MODEL_PATH, SCALER_PATH,\n",
    "    RESULTS_DIR, EVAL_FIGURES_DIR, METRICS_PATH, PREDICTIONS_PATH,\n",
    "    INPUT_SEQ_LEN, OUTPUT_SEQ_LEN,\n",
    "    ENCODER_HIDDEN_SIZE, ENCODER_NUM_LAYERS, ENCODER_DROPOUT, ENCODER_BIDIRECTIONAL,\n",
    "    BATCH_SIZE, DEVICE\n",
    ")\n",
    "from src.dataset import create_dataloaders\n",
    "from src.model import build_model\n",
    "from src.evaluate import (\n",
    "    predict, calculate_metrics, calculate_metrics_per_step,\n",
    "    evaluate_model, print_evaluation_report\n",
    ")\n",
    "from src.utils import load_pickle, load_json, save_json, save_figure, save_csv\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0fe097",
   "metadata": {},
   "source": [
    "## 7.1 Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_test = np.load(os.path.join(SEQUENCES_DIR, 'X_test.npy'))\n",
    "y_test = np.load(os.path.join(SEQUENCES_DIR, 'y_test.npy'))\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_json(os.path.join(SEQUENCES_DIR, 'metadata.json'))\n",
    "n_features = metadata['n_features']\n",
    "target_idx = metadata['target_idx']\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"Number of features: {n_features}\")\n",
    "print(f\"Target index: {target_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42378c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaler\n",
    "scaler = load_pickle(SCALER_PATH)\n",
    "print(\"Scaler loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model architecture (same as training)\n",
    "model = build_model(\n",
    "    input_size=n_features,\n",
    "    hidden_size=ENCODER_HIDDEN_SIZE,\n",
    "    num_layers=ENCODER_NUM_LAYERS,\n",
    "    dropout=ENCODER_DROPOUT,\n",
    "    bidirectional=ENCODER_BIDIRECTIONAL,\n",
    "    output_seq_len=OUTPUT_SEQ_LEN,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "checkpoint = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Model loaded from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Validation loss at checkpoint: {checkpoint['val_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test DataLoader\n",
    "from src.dataset import TimeSeriesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee8fe0",
   "metadata": {},
   "source": [
    "## 7.2 Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_true_scaled, y_pred_scaled = predict(model, test_loader, DEVICE)\n",
    "\n",
    "print(f\"y_true shape: {y_true_scaled.shape}\")\n",
    "print(f\"y_pred shape: {y_pred_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84adc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform to original scale\n",
    "n_samples, n_steps = y_true_scaled.shape\n",
    "\n",
    "y_true_original = np.zeros_like(y_true_scaled)\n",
    "y_pred_original = np.zeros_like(y_pred_scaled)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    # Create dummy array with zeros\n",
    "    dummy = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    # Put true values in target column and inverse transform\n",
    "    dummy[:, target_idx] = y_true_scaled[:, i]\n",
    "    y_true_original[:, i] = scaler.inverse_transform(dummy)[:, target_idx]\n",
    "    \n",
    "    # Put predicted values in target column and inverse transform\n",
    "    dummy[:, target_idx] = y_pred_scaled[:, i]\n",
    "    y_pred_original[:, i] = scaler.inverse_transform(dummy)[:, target_idx]\n",
    "\n",
    "print(\"Inverse transform completed!\")\n",
    "print(f\"Traffic volume range: {y_true_original.min():.0f} - {y_true_original.max():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907b29fb",
   "metadata": {},
   "source": [
    "## 7.3 Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab898dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics per step\n",
    "step_names = [f't+{i+1}' for i in range(OUTPUT_SEQ_LEN)]\n",
    "metrics_df = calculate_metrics_per_step(y_true_original, y_pred_original, step_names)\n",
    "\n",
    "print(\"\\nMetrics per Prediction Step:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac33c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed evaluation report\n",
    "print_evaluation_report(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf11e2",
   "metadata": {},
   "source": [
    "## 7.4 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12671b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figures directory\n",
    "os.makedirs(EVAL_FIGURES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f96b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Metrics by forecast horizon\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics_plot = metrics_df[metrics_df['Step'] != 'Average']\n",
    "x = range(len(metrics_plot))\n",
    "\n",
    "# R²\n",
    "axes[0, 0].bar(x, metrics_plot['R2'], color='steelblue')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(metrics_plot['Step'])\n",
    "axes[0, 0].set_ylabel('R²')\n",
    "axes[0, 0].set_title('R² by Forecast Horizon')\n",
    "axes[0, 0].axhline(y=metrics_df[metrics_df['Step']=='Average']['R2'].values[0], color='red', linestyle='--', label='Average')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# NSE\n",
    "axes[0, 1].bar(x, metrics_plot['NSE'], color='coral')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(metrics_plot['Step'])\n",
    "axes[0, 1].set_ylabel('NSE')\n",
    "axes[0, 1].set_title('NSE by Forecast Horizon')\n",
    "axes[0, 1].axhline(y=metrics_df[metrics_df['Step']=='Average']['NSE'].values[0], color='red', linestyle='--', label='Average')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# MAE\n",
    "axes[1, 0].bar(x, metrics_plot['MAE'], color='seagreen')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(metrics_plot['Step'])\n",
    "axes[1, 0].set_ylabel('MAE')\n",
    "axes[1, 0].set_title('MAE by Forecast Horizon')\n",
    "axes[1, 0].axhline(y=metrics_df[metrics_df['Step']=='Average']['MAE'].values[0], color='red', linestyle='--', label='Average')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# RMSE\n",
    "axes[1, 1].bar(x, metrics_plot['RMSE'], color='purple')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metrics_plot['Step'])\n",
    "axes[1, 1].set_ylabel('RMSE')\n",
    "axes[1, 1].set_title('RMSE by Forecast Horizon')\n",
    "axes[1, 1].axhline(y=metrics_df[metrics_df['Step']=='Average']['RMSE'].values[0], color='red', linestyle='--', label='Average')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, os.path.join(EVAL_FIGURES_DIR, 'metrics_by_horizon.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Actual vs Predicted scatter plots\n",
    "fig, axes = plt.subplots(1, OUTPUT_SEQ_LEN, figsize=(4*OUTPUT_SEQ_LEN, 4))\n",
    "\n",
    "for i in range(OUTPUT_SEQ_LEN):\n",
    "    ax = axes[i] if OUTPUT_SEQ_LEN > 1 else axes\n",
    "    ax.scatter(y_true_original[:, i], y_pred_original[:, i], alpha=0.3, s=10)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_true_original[:, i].min(), y_pred_original[:, i].min())\n",
    "    max_val = max(y_true_original[:, i].max(), y_pred_original[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect')\n",
    "    \n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(f't+{i+1}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Actual vs Predicted Traffic Volume', y=1.02)\n",
    "plt.tight_layout()\n",
    "save_figure(fig, os.path.join(EVAL_FIGURES_DIR, 'scatter_plots.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea514f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Time series comparison (sample)\n",
    "n_samples_plot = 200\n",
    "\n",
    "fig, axes = plt.subplots(OUTPUT_SEQ_LEN, 1, figsize=(16, 3*OUTPUT_SEQ_LEN))\n",
    "\n",
    "for i in range(OUTPUT_SEQ_LEN):\n",
    "    ax = axes[i] if OUTPUT_SEQ_LEN > 1 else axes\n",
    "    ax.plot(y_true_original[:n_samples_plot, i], 'b-', label='Actual', alpha=0.7)\n",
    "    ax.plot(y_pred_original[:n_samples_plot, i], 'r-', label='Predicted', alpha=0.7)\n",
    "    ax.set_xlabel('Sample')\n",
    "    ax.set_ylabel('Traffic Volume')\n",
    "    ax.set_title(f'Prediction Step t+{i+1}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, os.path.join(EVAL_FIGURES_DIR, 'time_series_comparison.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf342d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Error distribution\n",
    "errors = y_pred_original - y_true_original\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Error histogram\n",
    "axes[0].hist(errors.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Prediction Error')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Error Distribution (All Steps)')\n",
    "\n",
    "# Error by step (boxplot)\n",
    "axes[1].boxplot([errors[:, i] for i in range(OUTPUT_SEQ_LEN)], labels=step_names)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Prediction Step')\n",
    "axes[1].set_ylabel('Prediction Error')\n",
    "axes[1].set_title('Error Distribution by Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, os.path.join(EVAL_FIGURES_DIR, 'error_distribution.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f2527",
   "metadata": {},
   "source": [
    "## 7.5 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to JSON\n",
    "metrics_dict = {\n",
    "    'per_step': metrics_df.to_dict('records'),\n",
    "    'summary': {\n",
    "        'avg_R2': float(metrics_df[metrics_df['Step']=='Average']['R2'].values[0]),\n",
    "        'avg_NSE': float(metrics_df[metrics_df['Step']=='Average']['NSE'].values[0]),\n",
    "        'avg_MAE': float(metrics_df[metrics_df['Step']=='Average']['MAE'].values[0]),\n",
    "        'avg_RMSE': float(metrics_df[metrics_df['Step']=='Average']['RMSE'].values[0])\n",
    "    }\n",
    "}\n",
    "\n",
    "save_json(metrics_dict, METRICS_PATH)\n",
    "print(f\"Metrics saved to: {METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9842a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "predictions_df = pd.DataFrame()\n",
    "\n",
    "for i in range(OUTPUT_SEQ_LEN):\n",
    "    predictions_df[f'actual_t+{i+1}'] = y_true_original[:, i]\n",
    "    predictions_df[f'predicted_t+{i+1}'] = y_pred_original[:, i]\n",
    "\n",
    "save_csv(predictions_df, PREDICTIONS_PATH, index=False)\n",
    "print(f\"Predictions saved to: {PREDICTIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf45bf",
   "metadata": {},
   "source": [
    "## 7.6 Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nMetrics Summary Table:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Step':<10} {'R²':>10} {'NSE':>10} {'MAE':>12} {'RMSE':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for _, row in metrics_df.iterrows():\n",
    "    print(f\"{row['Step']:<10} {row['R2']:>10.4f} {row['NSE']:>10.4f} {row['MAE']:>12.2f} {row['RMSE']:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  • R² (Coefficient of Determination): How well the model explains variance\")\n",
    "print(f\"  • NSE (Nash-Sutcliffe Efficiency): Model performance relative to mean\")\n",
    "print(f\"  • MAE (Mean Absolute Error): Average prediction error in traffic units\")\n",
    "print(f\"  • RMSE (Root Mean Squared Error): Penalizes large errors more\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8683f6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Evaluation completed:**\n",
    "1. ✅ Loaded trained model\n",
    "2. ✅ Generated predictions on test set\n",
    "3. ✅ Inverse transformed to original scale\n",
    "4. ✅ Calculated metrics (R², NSE, MAE, RMSE)\n",
    "5. ✅ Visualized results\n",
    "6. ✅ Saved metrics and predictions\n",
    "\n",
    "**Model Performance:**\n",
    "- Metrics calculated for each forecast step (t+1 to t+5)\n",
    "- Overall average metrics provided\n",
    "- Error degradation analyzed over forecast horizon"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
