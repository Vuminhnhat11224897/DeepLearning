# Traffic Volume Forecasting â€” LSTM Encoder-Decoder (Seq2Seq)

Dá»± Ä‘oÃ¡n lÆ°u lÆ°á»£ng giao thÃ´ng (traffic_volume) sá»­ dá»¥ng mÃ´ hÃ¬nh **LSTM Encoder-Decoder**.

## ğŸ“Š Tá»•ng quan

| ThÃ´ng tin | Chi tiáº¿t |
|-----------|----------|
| **Dataset** | Metro Interstate Traffic Volume (~48,204 báº£n ghi, 2012-2018) |
| **Input** | 24 giá» lá»‹ch sá»­ (24 timesteps Ã— 22 features) |
| **Output** | 5 giá» tÆ°Æ¡ng lai (traffic volume) |
| **Model** | LSTM Encoder-Decoder vá»›i Teacher Forcing |
| **Metrics** | RMSE, MAE, RÂ², NSE cho tá»«ng horizon (t+1...t+5) |

## ğŸ”‘ Key Features

### Feature Engineering (LSTM-optimized)
- âœ… **Temporal features**: hour, day_of_week, month, season, is_weekend, is_rush_hour
- âœ… **Cyclical encoding**: sin/cos cho hour, day, month
- âœ… **Weather features**: temp, temp_celsius, clouds_all, rain_1h, snow_1h, is_rainy, is_snowy
- âœ… **Interaction**: rush_rain

### Segment-based Data Preparation
- Dá»¯ liá»‡u cÃ³ ~2,588 gaps (timestamps bá»‹ thiáº¿u)
- TÃ¡ch thÃ nh 113 segments liÃªn tá»¥c
- Má»—i sequence Ä‘áº£m báº£o timestamps liÃªn tá»¥c
- **30,871 rows usable** (76.1% cá»§a 40,575)
- Táº¥t cáº£ labels lÃ  dá»¯ liá»‡u tháº­t (khÃ´ng interpolate)

## ğŸš€ Quick Start

```bash
# 1. CÃ i PyTorch vá»›i CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 2. CÃ i dependencies
pip install -r requirements.txt

# 3. Cháº¡y notebooks theo thá»© tá»±
```

## ğŸ““ Notebooks

| # | Notebook | MÃ´ táº£ |
|---|----------|-------|
| 01 | `01_EDA.ipynb` | Exploratory Data Analysis |
| 02 | `02_Preprocessing.ipynb` | Xá»­ lÃ½ missing, duplicates, outliers |
| 03 | `03_Feature_Engineering.ipynb` | Táº¡o temporal, cyclical, weather features |
| 04 | `04_Feature_Selection.ipynb` | Chá»n features cho LSTM (no leakage) |
| 05 | `05_Data_Preparation.ipynb` | Segment splitting, scaling, create sequences |
| 06 | `06_Model_Training.ipynb` | Train LSTM Encoder-Decoder |
| 06a | `06a_Optuna_Optimization.ipynb` | Hyperparameter tuning (optional) |
| 07 | `07_Evaluation.ipynb` | ÄÃ¡nh giÃ¡ & visualization |

## ğŸ“ Project Structure

```
DeepLearning_final/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw dataset
â”‚   â”œâ”€â”€ processed/              # Cleaned, featured, selected data
â”‚   â””â”€â”€ sequences/              # Train/val/test sequences (.npy)
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”œâ”€â”€ src/                        # Source code modules
â”œâ”€â”€ models/                     # Saved models & checkpoints
â”œâ”€â”€ results/                    # Metrics, predictions, figures
â””â”€â”€ logs/                       # Training & Optuna logs
```

## ğŸ“ˆ Model Architecture

```
Input (24, 22) â†’ Encoder (Bidirectional LSTM) â†’ Context â†’ Decoder (LSTM) â†’ Output (5,)
                     â†“                                        â†‘
              Hidden State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Initial State
```

## ğŸ“ Notes

- **Scaling**: Fit scaler trÃªn training data only â†’ apply cho val/test
- **Split**: Time-based (70/15/15), khÃ´ng shuffle
- **Segment workflow**: Äáº£m báº£o má»—i sequence cÃ³ timestamps liÃªn tá»¥c
- **Teacher Forcing**: Sá»­ dá»¥ng trong training Ä‘á»ƒ tÄƒng tá»‘c há»c

## ğŸ“š References

- Sutskever et al. (2014) - Sequence to Sequence Learning
- Metro Interstate Traffic Volume Dataset - UCI ML Repository

